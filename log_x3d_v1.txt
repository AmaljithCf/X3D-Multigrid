./data/charades_traininglabeldata.npy exists
dataset size:7974
./data/charades_testinglabeldata.npy exists
dataset size:1861
datasets created
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv3d-1     [-1, 24, 16, 224, 224]             648
            Conv3d-2     [-1, 24, 16, 224, 224]              72
       BatchNorm3d-3     [-1, 24, 16, 224, 224]              48
         Hardswish-4     [-1, 24, 16, 224, 224]               0
         MaxPool3d-5      [-1, 24, 8, 112, 112]               0
            Conv3d-6      [-1, 54, 8, 112, 112]           1,296
       BatchNorm3d-7      [-1, 54, 8, 112, 112]             108
         Hardswish-8      [-1, 54, 8, 112, 112]               0
            Conv3d-9        [-1, 54, 4, 56, 56]           1,458
      BatchNorm3d-10        [-1, 54, 4, 56, 56]             108
        Hardswish-11        [-1, 54, 4, 56, 56]               0
           Conv3d-12        [-1, 24, 4, 56, 56]           1,296
      BatchNorm3d-13        [-1, 24, 4, 56, 56]              48
AdaptiveAvgPool3d-14          [-1, 24, 1, 1, 1]               0
           Linear-15                    [-1, 6]             150
        Hardswish-16                    [-1, 6]               0
           Linear-17                   [-1, 24]             168
          Sigmoid-18                   [-1, 24]               0
           Conv3d-19        [-1, 24, 4, 56, 56]             576
      BatchNorm3d-20        [-1, 24, 4, 56, 56]              48
        Hardswish-21        [-1, 24, 4, 56, 56]               0
       Bottleneck-22        [-1, 24, 4, 56, 56]               0
           Conv3d-23        [-1, 54, 4, 56, 56]           1,296
      BatchNorm3d-24        [-1, 54, 4, 56, 56]             108
        Hardswish-25        [-1, 54, 4, 56, 56]               0
           Conv3d-26        [-1, 54, 4, 56, 56]           1,458
      BatchNorm3d-27        [-1, 54, 4, 56, 56]             108
        Hardswish-28        [-1, 54, 4, 56, 56]               0
           Conv3d-29        [-1, 24, 4, 56, 56]           1,296
      BatchNorm3d-30        [-1, 24, 4, 56, 56]              48
AdaptiveAvgPool3d-31          [-1, 24, 1, 1, 1]               0
           Linear-32                    [-1, 6]             150
        Hardswish-33                    [-1, 6]               0
           Linear-34                   [-1, 24]             168
          Sigmoid-35                   [-1, 24]               0
        Hardswish-36        [-1, 24, 4, 56, 56]               0
       Bottleneck-37        [-1, 24, 4, 56, 56]               0
           Conv3d-38        [-1, 54, 4, 56, 56]           1,296
      BatchNorm3d-39        [-1, 54, 4, 56, 56]             108
        Hardswish-40        [-1, 54, 4, 56, 56]               0
           Conv3d-41        [-1, 54, 4, 56, 56]           1,458
      BatchNorm3d-42        [-1, 54, 4, 56, 56]             108
        Hardswish-43        [-1, 54, 4, 56, 56]               0
           Conv3d-44        [-1, 24, 4, 56, 56]           1,296
      BatchNorm3d-45        [-1, 24, 4, 56, 56]              48
AdaptiveAvgPool3d-46          [-1, 24, 1, 1, 1]               0
           Linear-47                    [-1, 6]             150
        Hardswish-48                    [-1, 6]               0
           Linear-49                   [-1, 24]             168
          Sigmoid-50                   [-1, 24]               0
        Hardswish-51        [-1, 24, 4, 56, 56]               0
       Bottleneck-52        [-1, 24, 4, 56, 56]               0
           Conv3d-53       [-1, 108, 4, 56, 56]           2,592
      BatchNorm3d-54       [-1, 108, 4, 56, 56]             216
        Hardswish-55       [-1, 108, 4, 56, 56]               0
           Conv3d-56       [-1, 108, 2, 28, 28]           2,916
      BatchNorm3d-57       [-1, 108, 2, 28, 28]             216
        Hardswish-58       [-1, 108, 2, 28, 28]               0
           Conv3d-59        [-1, 48, 2, 28, 28]           5,184
      BatchNorm3d-60        [-1, 48, 2, 28, 28]              96
AdaptiveAvgPool3d-61          [-1, 48, 1, 1, 1]               0
           Linear-62                   [-1, 12]             588
        Hardswish-63                   [-1, 12]               0
           Linear-64                   [-1, 48]             624
          Sigmoid-65                   [-1, 48]               0
           Conv3d-66        [-1, 48, 2, 28, 28]           1,152
      BatchNorm3d-67        [-1, 48, 2, 28, 28]              96
        Hardswish-68        [-1, 48, 2, 28, 28]               0
       Bottleneck-69        [-1, 48, 2, 28, 28]               0
           Conv3d-70       [-1, 108, 2, 28, 28]           5,184
      BatchNorm3d-71       [-1, 108, 2, 28, 28]             216
        Hardswish-72       [-1, 108, 2, 28, 28]               0
           Conv3d-73       [-1, 108, 2, 28, 28]           2,916
      BatchNorm3d-74       [-1, 108, 2, 28, 28]             216
        Hardswish-75       [-1, 108, 2, 28, 28]               0
           Conv3d-76        [-1, 48, 2, 28, 28]           5,184
      BatchNorm3d-77        [-1, 48, 2, 28, 28]              96
AdaptiveAvgPool3d-78          [-1, 48, 1, 1, 1]               0
           Linear-79                   [-1, 12]             588
        Hardswish-80                   [-1, 12]               0
           Linear-81                   [-1, 48]             624
          Sigmoid-82                   [-1, 48]               0
        Hardswish-83        [-1, 48, 2, 28, 28]               0
       Bottleneck-84        [-1, 48, 2, 28, 28]               0
           Conv3d-85       [-1, 108, 2, 28, 28]           5,184
      BatchNorm3d-86       [-1, 108, 2, 28, 28]             216
        Hardswish-87       [-1, 108, 2, 28, 28]               0
           Conv3d-88       [-1, 108, 2, 28, 28]           2,916
      BatchNorm3d-89       [-1, 108, 2, 28, 28]             216
        Hardswish-90       [-1, 108, 2, 28, 28]               0
           Conv3d-91        [-1, 48, 2, 28, 28]           5,184
      BatchNorm3d-92        [-1, 48, 2, 28, 28]              96
AdaptiveAvgPool3d-93          [-1, 48, 1, 1, 1]               0
           Linear-94                   [-1, 12]             588
        Hardswish-95                   [-1, 12]               0
           Linear-96                   [-1, 48]             624
          Sigmoid-97                   [-1, 48]               0
        Hardswish-98        [-1, 48, 2, 28, 28]               0
       Bottleneck-99        [-1, 48, 2, 28, 28]               0
          Conv3d-100       [-1, 108, 2, 28, 28]           5,184
     BatchNorm3d-101       [-1, 108, 2, 28, 28]             216
       Hardswish-102       [-1, 108, 2, 28, 28]               0
          Conv3d-103       [-1, 108, 2, 28, 28]           2,916
     BatchNorm3d-104       [-1, 108, 2, 28, 28]             216
       Hardswish-105       [-1, 108, 2, 28, 28]               0
          Conv3d-106        [-1, 48, 2, 28, 28]           5,184
     BatchNorm3d-107        [-1, 48, 2, 28, 28]              96
AdaptiveAvgPool3d-108          [-1, 48, 1, 1, 1]               0
          Linear-109                   [-1, 12]             588
       Hardswish-110                   [-1, 12]               0
          Linear-111                   [-1, 48]             624
         Sigmoid-112                   [-1, 48]               0
       Hardswish-113        [-1, 48, 2, 28, 28]               0
      Bottleneck-114        [-1, 48, 2, 28, 28]               0
          Conv3d-115       [-1, 108, 2, 28, 28]           5,184
     BatchNorm3d-116       [-1, 108, 2, 28, 28]             216
       Hardswish-117       [-1, 108, 2, 28, 28]               0
          Conv3d-118       [-1, 108, 2, 28, 28]           2,916
     BatchNorm3d-119       [-1, 108, 2, 28, 28]             216
       Hardswish-120       [-1, 108, 2, 28, 28]               0
          Conv3d-121        [-1, 48, 2, 28, 28]           5,184
     BatchNorm3d-122        [-1, 48, 2, 28, 28]              96
AdaptiveAvgPool3d-123          [-1, 48, 1, 1, 1]               0
          Linear-124                   [-1, 12]             588
       Hardswish-125                   [-1, 12]               0
          Linear-126                   [-1, 48]             624
         Sigmoid-127                   [-1, 48]               0
       Hardswish-128        [-1, 48, 2, 28, 28]               0
      Bottleneck-129        [-1, 48, 2, 28, 28]               0
          Conv3d-130       [-1, 216, 2, 28, 28]          10,368
     BatchNorm3d-131       [-1, 216, 2, 28, 28]             432
       Hardswish-132       [-1, 216, 2, 28, 28]               0
          Conv3d-133       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-134       [-1, 216, 1, 14, 14]             432
       Hardswish-135       [-1, 216, 1, 14, 14]               0
          Conv3d-136        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-137        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-138          [-1, 96, 1, 1, 1]               0
          Linear-139                   [-1, 24]           2,328
       Hardswish-140                   [-1, 24]               0
          Linear-141                   [-1, 96]           2,400
         Sigmoid-142                   [-1, 96]               0
          Conv3d-143        [-1, 96, 1, 14, 14]           4,608
     BatchNorm3d-144        [-1, 96, 1, 14, 14]             192
       Hardswish-145        [-1, 96, 1, 14, 14]               0
      Bottleneck-146        [-1, 96, 1, 14, 14]               0
          Conv3d-147       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-148       [-1, 216, 1, 14, 14]             432
       Hardswish-149       [-1, 216, 1, 14, 14]               0
          Conv3d-150       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-151       [-1, 216, 1, 14, 14]             432
       Hardswish-152       [-1, 216, 1, 14, 14]               0
          Conv3d-153        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-154        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-155          [-1, 96, 1, 1, 1]               0
          Linear-156                   [-1, 24]           2,328
       Hardswish-157                   [-1, 24]               0
          Linear-158                   [-1, 96]           2,400
         Sigmoid-159                   [-1, 96]               0
       Hardswish-160        [-1, 96, 1, 14, 14]               0
      Bottleneck-161        [-1, 96, 1, 14, 14]               0
          Conv3d-162       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-163       [-1, 216, 1, 14, 14]             432
       Hardswish-164       [-1, 216, 1, 14, 14]               0
          Conv3d-165       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-166       [-1, 216, 1, 14, 14]             432
       Hardswish-167       [-1, 216, 1, 14, 14]               0
          Conv3d-168        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-169        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-170          [-1, 96, 1, 1, 1]               0
          Linear-171                   [-1, 24]           2,328
       Hardswish-172                   [-1, 24]               0
          Linear-173                   [-1, 96]           2,400
         Sigmoid-174                   [-1, 96]               0
       Hardswish-175        [-1, 96, 1, 14, 14]               0
      Bottleneck-176        [-1, 96, 1, 14, 14]               0
          Conv3d-177       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-178       [-1, 216, 1, 14, 14]             432
       Hardswish-179       [-1, 216, 1, 14, 14]               0
          Conv3d-180       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-181       [-1, 216, 1, 14, 14]             432
       Hardswish-182       [-1, 216, 1, 14, 14]               0
          Conv3d-183        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-184        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-185          [-1, 96, 1, 1, 1]               0
          Linear-186                   [-1, 24]           2,328
       Hardswish-187                   [-1, 24]               0
          Linear-188                   [-1, 96]           2,400
         Sigmoid-189                   [-1, 96]               0
       Hardswish-190        [-1, 96, 1, 14, 14]               0
      Bottleneck-191        [-1, 96, 1, 14, 14]               0
          Conv3d-192       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-193       [-1, 216, 1, 14, 14]             432
       Hardswish-194       [-1, 216, 1, 14, 14]               0
          Conv3d-195       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-196       [-1, 216, 1, 14, 14]             432
       Hardswish-197       [-1, 216, 1, 14, 14]               0
          Conv3d-198        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-199        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-200          [-1, 96, 1, 1, 1]               0
          Linear-201                   [-1, 24]           2,328
       Hardswish-202                   [-1, 24]               0
          Linear-203                   [-1, 96]           2,400
         Sigmoid-204                   [-1, 96]               0
       Hardswish-205        [-1, 96, 1, 14, 14]               0
      Bottleneck-206        [-1, 96, 1, 14, 14]               0
          Conv3d-207       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-208       [-1, 216, 1, 14, 14]             432
       Hardswish-209       [-1, 216, 1, 14, 14]               0
          Conv3d-210       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-211       [-1, 216, 1, 14, 14]             432
       Hardswish-212       [-1, 216, 1, 14, 14]               0
          Conv3d-213        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-214        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-215          [-1, 96, 1, 1, 1]               0
          Linear-216                   [-1, 24]           2,328
       Hardswish-217                   [-1, 24]               0
          Linear-218                   [-1, 96]           2,400
         Sigmoid-219                   [-1, 96]               0
       Hardswish-220        [-1, 96, 1, 14, 14]               0
      Bottleneck-221        [-1, 96, 1, 14, 14]               0
          Conv3d-222       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-223       [-1, 216, 1, 14, 14]             432
       Hardswish-224       [-1, 216, 1, 14, 14]               0
          Conv3d-225       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-226       [-1, 216, 1, 14, 14]             432
       Hardswish-227       [-1, 216, 1, 14, 14]               0
          Conv3d-228        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-229        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-230          [-1, 96, 1, 1, 1]               0
          Linear-231                   [-1, 24]           2,328
       Hardswish-232                   [-1, 24]               0
          Linear-233                   [-1, 96]           2,400
         Sigmoid-234                   [-1, 96]               0
       Hardswish-235        [-1, 96, 1, 14, 14]               0
      Bottleneck-236        [-1, 96, 1, 14, 14]               0
          Conv3d-237       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-238       [-1, 216, 1, 14, 14]             432
       Hardswish-239       [-1, 216, 1, 14, 14]               0
          Conv3d-240       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-241       [-1, 216, 1, 14, 14]             432
       Hardswish-242       [-1, 216, 1, 14, 14]               0
          Conv3d-243        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-244        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-245          [-1, 96, 1, 1, 1]               0
          Linear-246                   [-1, 24]           2,328
       Hardswish-247                   [-1, 24]               0
          Linear-248                   [-1, 96]           2,400
         Sigmoid-249                   [-1, 96]               0
       Hardswish-250        [-1, 96, 1, 14, 14]               0
      Bottleneck-251        [-1, 96, 1, 14, 14]               0
          Conv3d-252       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-253       [-1, 216, 1, 14, 14]             432
       Hardswish-254       [-1, 216, 1, 14, 14]               0
          Conv3d-255       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-256       [-1, 216, 1, 14, 14]             432
       Hardswish-257       [-1, 216, 1, 14, 14]               0
          Conv3d-258        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-259        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-260          [-1, 96, 1, 1, 1]               0
          Linear-261                   [-1, 24]           2,328
       Hardswish-262                   [-1, 24]               0
          Linear-263                   [-1, 96]           2,400
         Sigmoid-264                   [-1, 96]               0
       Hardswish-265        [-1, 96, 1, 14, 14]               0
      Bottleneck-266        [-1, 96, 1, 14, 14]               0
          Conv3d-267       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-268       [-1, 216, 1, 14, 14]             432
       Hardswish-269       [-1, 216, 1, 14, 14]               0
          Conv3d-270       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-271       [-1, 216, 1, 14, 14]             432
       Hardswish-272       [-1, 216, 1, 14, 14]               0
          Conv3d-273        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-274        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-275          [-1, 96, 1, 1, 1]               0
          Linear-276                   [-1, 24]           2,328
       Hardswish-277                   [-1, 24]               0
          Linear-278                   [-1, 96]           2,400
         Sigmoid-279                   [-1, 96]               0
       Hardswish-280        [-1, 96, 1, 14, 14]               0
      Bottleneck-281        [-1, 96, 1, 14, 14]               0
          Conv3d-282       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-283       [-1, 216, 1, 14, 14]             432
       Hardswish-284       [-1, 216, 1, 14, 14]               0
          Conv3d-285       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-286       [-1, 216, 1, 14, 14]             432
       Hardswish-287       [-1, 216, 1, 14, 14]               0
          Conv3d-288        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-289        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-290          [-1, 96, 1, 1, 1]               0
          Linear-291                   [-1, 24]           2,328
       Hardswish-292                   [-1, 24]               0
          Linear-293                   [-1, 96]           2,400
         Sigmoid-294                   [-1, 96]               0
       Hardswish-295        [-1, 96, 1, 14, 14]               0
      Bottleneck-296        [-1, 96, 1, 14, 14]               0
          Conv3d-297       [-1, 432, 1, 14, 14]          41,472
     BatchNorm3d-298       [-1, 432, 1, 14, 14]             864
       Hardswish-299       [-1, 432, 1, 14, 14]               0
          Conv3d-300         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-301         [-1, 432, 1, 7, 7]             864
       Hardswish-302         [-1, 432, 1, 7, 7]               0
          Conv3d-303         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-304         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-305         [-1, 192, 1, 1, 1]               0
          Linear-306                   [-1, 48]           9,264
       Hardswish-307                   [-1, 48]               0
          Linear-308                  [-1, 192]           9,408
         Sigmoid-309                  [-1, 192]               0
          Conv3d-310         [-1, 192, 1, 7, 7]          18,432
     BatchNorm3d-311         [-1, 192, 1, 7, 7]             384
       Hardswish-312         [-1, 192, 1, 7, 7]               0
      Bottleneck-313         [-1, 192, 1, 7, 7]               0
          Conv3d-314         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-315         [-1, 432, 1, 7, 7]             864
       Hardswish-316         [-1, 432, 1, 7, 7]               0
          Conv3d-317         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-318         [-1, 432, 1, 7, 7]             864
       Hardswish-319         [-1, 432, 1, 7, 7]               0
          Conv3d-320         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-321         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-322         [-1, 192, 1, 1, 1]               0
          Linear-323                   [-1, 48]           9,264
       Hardswish-324                   [-1, 48]               0
          Linear-325                  [-1, 192]           9,408
         Sigmoid-326                  [-1, 192]               0
       Hardswish-327         [-1, 192, 1, 7, 7]               0
      Bottleneck-328         [-1, 192, 1, 7, 7]               0
          Conv3d-329         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-330         [-1, 432, 1, 7, 7]             864
       Hardswish-331         [-1, 432, 1, 7, 7]               0
          Conv3d-332         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-333         [-1, 432, 1, 7, 7]             864
       Hardswish-334         [-1, 432, 1, 7, 7]               0
          Conv3d-335         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-336         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-337         [-1, 192, 1, 1, 1]               0
          Linear-338                   [-1, 48]           9,264
       Hardswish-339                   [-1, 48]               0
          Linear-340                  [-1, 192]           9,408
         Sigmoid-341                  [-1, 192]               0
       Hardswish-342         [-1, 192, 1, 7, 7]               0
      Bottleneck-343         [-1, 192, 1, 7, 7]               0
          Conv3d-344         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-345         [-1, 432, 1, 7, 7]             864
       Hardswish-346         [-1, 432, 1, 7, 7]               0
          Conv3d-347         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-348         [-1, 432, 1, 7, 7]             864
       Hardswish-349         [-1, 432, 1, 7, 7]               0
          Conv3d-350         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-351         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-352         [-1, 192, 1, 1, 1]               0
          Linear-353                   [-1, 48]           9,264
       Hardswish-354                   [-1, 48]               0
          Linear-355                  [-1, 192]           9,408
         Sigmoid-356                  [-1, 192]               0
       Hardswish-357         [-1, 192, 1, 7, 7]               0
      Bottleneck-358         [-1, 192, 1, 7, 7]               0
          Conv3d-359         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-360         [-1, 432, 1, 7, 7]             864
       Hardswish-361         [-1, 432, 1, 7, 7]               0
          Conv3d-362         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-363         [-1, 432, 1, 7, 7]             864
       Hardswish-364         [-1, 432, 1, 7, 7]               0
          Conv3d-365         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-366         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-367         [-1, 192, 1, 1, 1]               0
          Linear-368                   [-1, 48]           9,264
       Hardswish-369                   [-1, 48]               0
          Linear-370                  [-1, 192]           9,408
         Sigmoid-371                  [-1, 192]               0
       Hardswish-372         [-1, 192, 1, 7, 7]               0
      Bottleneck-373         [-1, 192, 1, 7, 7]               0
          Conv3d-374         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-375         [-1, 432, 1, 7, 7]             864
       Hardswish-376         [-1, 432, 1, 7, 7]               0
          Conv3d-377         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-378         [-1, 432, 1, 7, 7]             864
       Hardswish-379         [-1, 432, 1, 7, 7]               0
          Conv3d-380         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-381         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-382         [-1, 192, 1, 1, 1]               0
          Linear-383                   [-1, 48]           9,264
       Hardswish-384                   [-1, 48]               0
          Linear-385                  [-1, 192]           9,408
         Sigmoid-386                  [-1, 192]               0
       Hardswish-387         [-1, 192, 1, 7, 7]               0
      Bottleneck-388         [-1, 192, 1, 7, 7]               0
          Conv3d-389         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-390         [-1, 432, 1, 7, 7]             864
       Hardswish-391         [-1, 432, 1, 7, 7]               0
          Conv3d-392         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-393         [-1, 432, 1, 7, 7]             864
       Hardswish-394         [-1, 432, 1, 7, 7]               0
          Conv3d-395         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-396         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-397         [-1, 192, 1, 1, 1]               0
          Linear-398                   [-1, 48]           9,264
       Hardswish-399                   [-1, 48]               0
          Linear-400                  [-1, 192]           9,408
         Sigmoid-401                  [-1, 192]               0
       Hardswish-402         [-1, 192, 1, 7, 7]               0
      Bottleneck-403         [-1, 192, 1, 7, 7]               0
          Conv3d-404         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-405         [-1, 432, 1, 7, 7]             864
       Hardswish-406         [-1, 432, 1, 7, 7]               0
AdaptiveAvgPool3d-407        [-1, 432, 16, 1, 1]               0
          Linear-408             [-1, 16, 2048]         886,784
          Linear-409              [-1, 16, 157]         321,693
================================================================
Total params: 3,325,445
Trainable params: 3,325,445
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 9.19
Forward/backward pass size (MB): 964.58
Params size (MB): 12.69
Estimated Total Size (MB): 986.45
----------------------------------------------------------------
model loaded
Step 0/64000.0
----------
train
 Epoch:1 train steps: 1 Loc Loss: 0.0694 Cls Loss: 0.0694 Tot Loss: 0.0694 mAP: 0.0297
 Epoch:1 train steps: 10 Loc Loss: 0.5963 Cls Loss: 0.5965 Tot Loss: 0.5964 mAP: 0.0195
 Epoch:1 train steps: 20 Loc Loss: 0.5336 Cls Loss: 0.5347 Tot Loss: 0.5342 mAP: 0.0210
 Epoch:1 train steps: 30 Loc Loss: 0.3029 Cls Loss: 0.3064 Tot Loss: 0.3047 mAP: 0.0227
 Epoch:1 train steps: 40 Loc Loss: 0.1224 Cls Loss: 0.1306 Tot Loss: 0.1265 mAP: 0.0209
 Epoch:1 train steps: 50 Loc Loss: 0.0847 Cls Loss: 0.0963 Tot Loss: 0.0905 mAP: 0.0192
train
 Epoch:2 train steps: 60 Loc Loss: 0.0825 Cls Loss: 0.0940 Tot Loss: 0.0883 mAP: 0.0203
 Epoch:2 train steps: 70 Loc Loss: 0.0842 Cls Loss: 0.0972 Tot Loss: 0.0907 mAP: 0.0211
 Epoch:2 train steps: 80 Loc Loss: 0.0824 Cls Loss: 0.0944 Tot Loss: 0.0884 mAP: 0.0194
 Epoch:2 train steps: 90 Loc Loss: 0.0815 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0195
 Epoch:2 train steps: 100 Loc Loss: 0.0822 Cls Loss: 0.0935 Tot Loss: 0.0878 mAP: 0.0203
train
 Epoch:3 train steps: 110 Loc Loss: 0.0821 Cls Loss: 0.0939 Tot Loss: 0.0880 mAP: 0.0201
 Epoch:3 train steps: 120 Loc Loss: 0.0817 Cls Loss: 0.0926 Tot Loss: 0.0871 mAP: 0.0192
 Epoch:3 train steps: 130 Loc Loss: 0.0813 Cls Loss: 0.0923 Tot Loss: 0.0868 mAP: 0.0206
 Epoch:3 train steps: 140 Loc Loss: 0.0794 Cls Loss: 0.0901 Tot Loss: 0.0847 mAP: 0.0188
 Epoch:3 train steps: 150 Loc Loss: 0.0817 Cls Loss: 0.0924 Tot Loss: 0.0870 mAP: 0.0208
train
 Epoch:4 train steps: 160 Loc Loss: 0.0823 Cls Loss: 0.0933 Tot Loss: 0.0878 mAP: 0.0190
 Epoch:4 train steps: 170 Loc Loss: 0.0794 Cls Loss: 0.0904 Tot Loss: 0.0849 mAP: 0.0204
 Epoch:4 train steps: 180 Loc Loss: 0.0821 Cls Loss: 0.0936 Tot Loss: 0.0878 mAP: 0.0198
 Epoch:4 train steps: 190 Loc Loss: 0.0806 Cls Loss: 0.0919 Tot Loss: 0.0863 mAP: 0.0194
 Epoch:4 train steps: 200 Loc Loss: 0.0809 Cls Loss: 0.0923 Tot Loss: 0.0866 mAP: 0.0193
train
 Epoch:5 train steps: 210 Loc Loss: 0.0808 Cls Loss: 0.0922 Tot Loss: 0.0865 mAP: 0.0202
 Epoch:5 train steps: 220 Loc Loss: 0.0820 Cls Loss: 0.0936 Tot Loss: 0.0878 mAP: 0.0193
 Epoch:5 train steps: 230 Loc Loss: 0.0811 Cls Loss: 0.0927 Tot Loss: 0.0869 mAP: 0.0194
 Epoch:5 train steps: 240 Loc Loss: 0.0807 Cls Loss: 0.0916 Tot Loss: 0.0861 mAP: 0.0199
 Epoch:5 train steps: 250 Loc Loss: 0.0779 Cls Loss: 0.0892 Tot Loss: 0.0836 mAP: 0.0187
train
 Epoch:6 train steps: 260 Loc Loss: 0.0808 Cls Loss: 0.0925 Tot Loss: 0.0867 mAP: 0.0201
 Epoch:6 train steps: 270 Loc Loss: 0.0815 Cls Loss: 0.0933 Tot Loss: 0.0874 mAP: 0.0194
 Epoch:6 train steps: 280 Loc Loss: 0.0799 Cls Loss: 0.0914 Tot Loss: 0.0857 mAP: 0.0200
 Epoch:6 train steps: 290 Loc Loss: 0.0806 Cls Loss: 0.0919 Tot Loss: 0.0863 mAP: 0.0197
 Epoch:6 train steps: 300 Loc Loss: 0.0810 Cls Loss: 0.0927 Tot Loss: 0.0868 mAP: 0.0196
train
 Epoch:7 train steps: 310 Loc Loss: 0.0800 Cls Loss: 0.0911 Tot Loss: 0.0855 mAP: 0.0193
 Epoch:7 train steps: 320 Loc Loss: 0.0816 Cls Loss: 0.0932 Tot Loss: 0.0874 mAP: 0.0205
 Epoch:7 train steps: 330 Loc Loss: 0.0819 Cls Loss: 0.0935 Tot Loss: 0.0877 mAP: 0.0202
 Epoch:7 train steps: 340 Loc Loss: 0.0813 Cls Loss: 0.0926 Tot Loss: 0.0869 mAP: 0.0195
 Epoch:7 train steps: 350 Loc Loss: 0.0804 Cls Loss: 0.0918 Tot Loss: 0.0861 mAP: 0.0190
train
 Epoch:8 train steps: 360 Loc Loss: 0.0815 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0208
 Epoch:8 train steps: 370 Loc Loss: 0.0794 Cls Loss: 0.0901 Tot Loss: 0.0847 mAP: 0.0188
 Epoch:8 train steps: 380 Loc Loss: 0.0808 Cls Loss: 0.0922 Tot Loss: 0.0865 mAP: 0.0202
 Epoch:8 train steps: 390 Loc Loss: 0.0811 Cls Loss: 0.0924 Tot Loss: 0.0868 mAP: 0.0198
 Epoch:8 train steps: 400 Loc Loss: 0.0810 Cls Loss: 0.0924 Tot Loss: 0.0867 mAP: 0.0191
train
 Epoch:9 train steps: 410 Loc Loss: 0.0789 Cls Loss: 0.0903 Tot Loss: 0.0846 mAP: 0.0203
 Epoch:9 train steps: 420 Loc Loss: 0.0796 Cls Loss: 0.0908 Tot Loss: 0.0852 mAP: 0.0192
 Epoch:9 train steps: 430 Loc Loss: 0.0811 Cls Loss: 0.0926 Tot Loss: 0.0868 mAP: 0.0208
 Epoch:9 train steps: 440 Loc Loss: 0.0800 Cls Loss: 0.0899 Tot Loss: 0.0849 mAP: 0.0193
 Epoch:9 train steps: 450 Loc Loss: 0.0807 Cls Loss: 0.0915 Tot Loss: 0.0861 mAP: 0.0199
train
 Epoch:10 train steps: 460 Loc Loss: 0.0801 Cls Loss: 0.0913 Tot Loss: 0.0857 mAP: 0.0199
 Epoch:10 train steps: 470 Loc Loss: 0.0807 Cls Loss: 0.0924 Tot Loss: 0.0865 mAP: 0.0210
 Epoch:10 train steps: 480 Loc Loss: 0.0794 Cls Loss: 0.0905 Tot Loss: 0.0849 mAP: 0.0202
 Epoch:10 train steps: 490 Loc Loss: 0.0818 Cls Loss: 0.0937 Tot Loss: 0.0877 mAP: 0.0201
 Epoch:10 train steps: 500 Loc Loss: 0.0797 Cls Loss: 0.0908 Tot Loss: 0.0852 mAP: 0.0197
train
 Epoch:11 train steps: 510 Loc Loss: 0.0808 Cls Loss: 0.0925 Tot Loss: 0.0867 mAP: 0.0186
 Epoch:11 train steps: 520 Loc Loss: 0.0799 Cls Loss: 0.0914 Tot Loss: 0.0857 mAP: 0.0200
 Epoch:11 train steps: 530 Loc Loss: 0.0812 Cls Loss: 0.0926 Tot Loss: 0.0869 mAP: 0.0201
 Epoch:11 train steps: 540 Loc Loss: 0.0800 Cls Loss: 0.0913 Tot Loss: 0.0857 mAP: 0.0196
 Epoch:11 train steps: 550 Loc Loss: 0.0810 Cls Loss: 0.0925 Tot Loss: 0.0868 mAP: 0.0204
train
 Epoch:12 train steps: 560 Loc Loss: 0.0796 Cls Loss: 0.0913 Tot Loss: 0.0854 mAP: 0.0198
 Epoch:12 train steps: 570 Loc Loss: 0.0805 Cls Loss: 0.0911 Tot Loss: 0.0858 mAP: 0.0195
 Epoch:12 train steps: 580 Loc Loss: 0.0819 Cls Loss: 0.0934 Tot Loss: 0.0877 mAP: 0.0196
 Epoch:12 train steps: 590 Loc Loss: 0.0815 Cls Loss: 0.0923 Tot Loss: 0.0869 mAP: 0.0218
 Epoch:12 train steps: 600 Loc Loss: 0.0812 Cls Loss: 0.0934 Tot Loss: 0.0873 mAP: 0.0197
train
 Epoch:13 train steps: 610 Loc Loss: 0.0813 Cls Loss: 0.0934 Tot Loss: 0.0874 mAP: 0.0209
 Epoch:13 train steps: 620 Loc Loss: 0.0813 Cls Loss: 0.0925 Tot Loss: 0.0869 mAP: 0.0214
 Epoch:13 train steps: 630 Loc Loss: 0.0798 Cls Loss: 0.0918 Tot Loss: 0.0858 mAP: 0.0201
 Epoch:13 train steps: 640 Loc Loss: 0.0824 Cls Loss: 0.0929 Tot Loss: 0.0876 mAP: 0.0212
 Epoch:13 train steps: 650 Loc Loss: 0.0800 Cls Loss: 0.0919 Tot Loss: 0.0860 mAP: 0.0200
train
 Epoch:14 train steps: 660 Loc Loss: 0.0812 Cls Loss: 0.0925 Tot Loss: 0.0868 mAP: 0.0201
 Epoch:14 train steps: 670 Loc Loss: 0.0814 Cls Loss: 0.0926 Tot Loss: 0.0870 mAP: 0.0199
 Epoch:14 train steps: 680 Loc Loss: 0.0802 Cls Loss: 0.0918 Tot Loss: 0.0860 mAP: 0.0204
 Epoch:14 train steps: 690 Loc Loss: 0.0809 Cls Loss: 0.0917 Tot Loss: 0.0863 mAP: 0.0194
 Epoch:14 train steps: 700 Loc Loss: 0.0815 Cls Loss: 0.0928 Tot Loss: 0.0872 mAP: 0.0201
train
 Epoch:15 train steps: 710 Loc Loss: 0.0802 Cls Loss: 0.0926 Tot Loss: 0.0864 mAP: 0.0205
 Epoch:15 train steps: 720 Loc Loss: 0.0797 Cls Loss: 0.0913 Tot Loss: 0.0855 mAP: 0.0199
 Epoch:15 train steps: 730 Loc Loss: 0.0827 Cls Loss: 0.0946 Tot Loss: 0.0886 mAP: 0.0209
 Epoch:15 train steps: 740 Loc Loss: 0.0787 Cls Loss: 0.0901 Tot Loss: 0.0844 mAP: 0.0201
 Epoch:15 train steps: 750 Loc Loss: 0.0808 Cls Loss: 0.0920 Tot Loss: 0.0864 mAP: 0.0197
train
 Epoch:16 train steps: 760 Loc Loss: 0.0809 Cls Loss: 0.0916 Tot Loss: 0.0863 mAP: 0.0199
 Epoch:16 train steps: 770 Loc Loss: 0.0811 Cls Loss: 0.0930 Tot Loss: 0.0871 mAP: 0.0205
 Epoch:16 train steps: 780 Loc Loss: 0.0809 Cls Loss: 0.0919 Tot Loss: 0.0864 mAP: 0.0202
 Epoch:16 train steps: 790 Loc Loss: 0.0802 Cls Loss: 0.0912 Tot Loss: 0.0857 mAP: 0.0191
 Epoch:16 train steps: 800 Loc Loss: 0.0789 Cls Loss: 0.0903 Tot Loss: 0.0846 mAP: 0.0192
train
 Epoch:17 train steps: 810 Loc Loss: 0.0810 Cls Loss: 0.0921 Tot Loss: 0.0866 mAP: 0.0201
 Epoch:17 train steps: 820 Loc Loss: 0.0794 Cls Loss: 0.0901 Tot Loss: 0.0847 mAP: 0.0206
 Epoch:17 train steps: 830 Loc Loss: 0.0817 Cls Loss: 0.0928 Tot Loss: 0.0872 mAP: 0.0242
 Epoch:17 train steps: 840 Loc Loss: 0.0816 Cls Loss: 0.0935 Tot Loss: 0.0875 mAP: 0.0193
 Epoch:17 train steps: 850 Loc Loss: 0.0798 Cls Loss: 0.0905 Tot Loss: 0.0852 mAP: 0.0207
train
 Epoch:18 train steps: 860 Loc Loss: 0.0809 Cls Loss: 0.0921 Tot Loss: 0.0865 mAP: 0.0218
 Epoch:18 train steps: 870 Loc Loss: 0.0809 Cls Loss: 0.0919 Tot Loss: 0.0864 mAP: 0.0197
 Epoch:18 train steps: 880 Loc Loss: 0.0820 Cls Loss: 0.0937 Tot Loss: 0.0878 mAP: 0.0199
 Epoch:18 train steps: 890 Loc Loss: 0.0803 Cls Loss: 0.0925 Tot Loss: 0.0864 mAP: 0.0201
 Epoch:18 train steps: 900 Loc Loss: 0.0789 Cls Loss: 0.0899 Tot Loss: 0.0844 mAP: 0.0190
train
 Epoch:19 train steps: 910 Loc Loss: 0.0804 Cls Loss: 0.0916 Tot Loss: 0.0860 mAP: 0.0195
 Epoch:19 train steps: 920 Loc Loss: 0.0814 Cls Loss: 0.0931 Tot Loss: 0.0873 mAP: 0.0207
 Epoch:19 train steps: 930 Loc Loss: 0.0819 Cls Loss: 0.0930 Tot Loss: 0.0875 mAP: 0.0219
 Epoch:19 train steps: 940 Loc Loss: 0.0810 Cls Loss: 0.0926 Tot Loss: 0.0868 mAP: 0.0217
 Epoch:19 train steps: 950 Loc Loss: 0.0806 Cls Loss: 0.0926 Tot Loss: 0.0866 mAP: 0.0209
train
 Epoch:20 train steps: 960 Loc Loss: 0.0809 Cls Loss: 0.0925 Tot Loss: 0.0867 mAP: 0.0193
 Epoch:20 train steps: 970 Loc Loss: 0.0817 Cls Loss: 0.0925 Tot Loss: 0.0871 mAP: 0.0209
 Epoch:20 train steps: 980 Loc Loss: 0.0808 Cls Loss: 0.0921 Tot Loss: 0.0865 mAP: 0.0196
 Epoch:20 train steps: 990 Loc Loss: 0.0798 Cls Loss: 0.0904 Tot Loss: 0.0851 mAP: 0.0192
 Epoch:20 train steps: 1000 Loc Loss: 0.0804 Cls Loss: 0.0918 Tot Loss: 0.0861 mAP: 0.0193
val
 Epoch:20 val Loc Loss: 0.1089 Cls Loss: 0.1266 Tot Loss: 0.1178 mAP: 0.0270
Step 1000/64000.0
----------
train
 Epoch:21 train steps: 1010 Loc Loss: 0.0805 Cls Loss: 0.0921 Tot Loss: 0.0863 mAP: 0.0203
 Epoch:21 train steps: 1020 Loc Loss: 0.0789 Cls Loss: 0.0906 Tot Loss: 0.0847 mAP: 0.0204
 Epoch:21 train steps: 1030 Loc Loss: 0.0814 Cls Loss: 0.0932 Tot Loss: 0.0873 mAP: 0.0196
 Epoch:21 train steps: 1040 Loc Loss: 0.0809 Cls Loss: 0.0924 Tot Loss: 0.0867 mAP: 0.0198
 Epoch:21 train steps: 1050 Loc Loss: 0.0801 Cls Loss: 0.0919 Tot Loss: 0.0860 mAP: 0.0208
train
 Epoch:22 train steps: 1060 Loc Loss: 0.0799 Cls Loss: 0.0918 Tot Loss: 0.0859 mAP: 0.0192
 Epoch:22 train steps: 1070 Loc Loss: 0.0811 Cls Loss: 0.0921 Tot Loss: 0.0866 mAP: 0.0194
 Epoch:22 train steps: 1080 Loc Loss: 0.0814 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0199
 Epoch:22 train steps: 1090 Loc Loss: 0.0797 Cls Loss: 0.0908 Tot Loss: 0.0853 mAP: 0.0201
 Epoch:22 train steps: 1100 Loc Loss: 0.0810 Cls Loss: 0.0930 Tot Loss: 0.0870 mAP: 0.0205
train
 Epoch:23 train steps: 1110 Loc Loss: 0.0795 Cls Loss: 0.0908 Tot Loss: 0.0851 mAP: 0.0197
 Epoch:23 train steps: 1120 Loc Loss: 0.0805 Cls Loss: 0.0916 Tot Loss: 0.0860 mAP: 0.0205
 Epoch:23 train steps: 1130 Loc Loss: 0.0808 Cls Loss: 0.0924 Tot Loss: 0.0866 mAP: 0.0199
 Epoch:23 train steps: 1140 Loc Loss: 0.0820 Cls Loss: 0.0929 Tot Loss: 0.0875 mAP: 0.0204
 Epoch:23 train steps: 1150 Loc Loss: 0.0821 Cls Loss: 0.0939 Tot Loss: 0.0880 mAP: 0.0203
train
 Epoch:24 train steps: 1160 Loc Loss: 0.0819 Cls Loss: 0.0928 Tot Loss: 0.0874 mAP: 0.0198
 Epoch:24 train steps: 1170 Loc Loss: 0.0803 Cls Loss: 0.0913 Tot Loss: 0.0858 mAP: 0.0197
 Epoch:24 train steps: 1180 Loc Loss: 0.0808 Cls Loss: 0.0923 Tot Loss: 0.0865 mAP: 0.0206
 Epoch:24 train steps: 1190 Loc Loss: 0.0798 Cls Loss: 0.0901 Tot Loss: 0.0849 mAP: 0.0195
 Epoch:24 train steps: 1200 Loc Loss: 0.0830 Cls Loss: 0.0949 Tot Loss: 0.0890 mAP: 0.0214
train
 Epoch:25 train steps: 1210 Loc Loss: 0.0810 Cls Loss: 0.0923 Tot Loss: 0.0866 mAP: 0.0201
 Epoch:25 train steps: 1220 Loc Loss: 0.0807 Cls Loss: 0.0922 Tot Loss: 0.0864 mAP: 0.0200
 Epoch:25 train steps: 1230 Loc Loss: 0.0803 Cls Loss: 0.0912 Tot Loss: 0.0857 mAP: 0.0210
 Epoch:25 train steps: 1240 Loc Loss: 0.0809 Cls Loss: 0.0927 Tot Loss: 0.0868 mAP: 0.0201
 Epoch:25 train steps: 1250 Loc Loss: 0.0805 Cls Loss: 0.0918 Tot Loss: 0.0862 mAP: 0.0204
train
 Epoch:26 train steps: 1260 Loc Loss: 0.0804 Cls Loss: 0.0923 Tot Loss: 0.0864 mAP: 0.0205
 Epoch:26 train steps: 1270 Loc Loss: 0.0810 Cls Loss: 0.0920 Tot Loss: 0.0865 mAP: 0.0208
 Epoch:26 train steps: 1280 Loc Loss: 0.0804 Cls Loss: 0.0913 Tot Loss: 0.0858 mAP: 0.0196
 Epoch:26 train steps: 1290 Loc Loss: 0.0811 Cls Loss: 0.0928 Tot Loss: 0.0870 mAP: 0.0202
 Epoch:26 train steps: 1300 Loc Loss: 0.0815 Cls Loss: 0.0925 Tot Loss: 0.0870 mAP: 0.0199
train
 Epoch:27 train steps: 1310 Loc Loss: 0.0806 Cls Loss: 0.0918 Tot Loss: 0.0862 mAP: 0.0208
 Epoch:27 train steps: 1320 Loc Loss: 0.0819 Cls Loss: 0.0930 Tot Loss: 0.0875 mAP: 0.0202
 Epoch:27 train steps: 1330 Loc Loss: 0.0795 Cls Loss: 0.0903 Tot Loss: 0.0849 mAP: 0.0204
 Epoch:27 train steps: 1340 Loc Loss: 0.0794 Cls Loss: 0.0907 Tot Loss: 0.0850 mAP: 0.0197
 Epoch:27 train steps: 1350 Loc Loss: 0.0825 Cls Loss: 0.0939 Tot Loss: 0.0882 mAP: 0.0196
train
 Epoch:28 train steps: 1360 Loc Loss: 0.0807 Cls Loss: 0.0925 Tot Loss: 0.0866 mAP: 0.0201
 Epoch:28 train steps: 1370 Loc Loss: 0.0812 Cls Loss: 0.0927 Tot Loss: 0.0870 mAP: 0.0200
 Epoch:28 train steps: 1380 Loc Loss: 0.0808 Cls Loss: 0.0928 Tot Loss: 0.0868 mAP: 0.0211
 Epoch:28 train steps: 1390 Loc Loss: 0.0796 Cls Loss: 0.0909 Tot Loss: 0.0852 mAP: 0.0245
 Epoch:28 train steps: 1400 Loc Loss: 0.0812 Cls Loss: 0.0929 Tot Loss: 0.0870 mAP: 0.0213
train
 Epoch:29 train steps: 1410 Loc Loss: 0.0816 Cls Loss: 0.0920 Tot Loss: 0.0868 mAP: 0.0192
 Epoch:29 train steps: 1420 Loc Loss: 0.0806 Cls Loss: 0.0924 Tot Loss: 0.0865 mAP: 0.0208
 Epoch:29 train steps: 1430 Loc Loss: 0.0800 Cls Loss: 0.0919 Tot Loss: 0.0860 mAP: 0.0197
 Epoch:29 train steps: 1440 Loc Loss: 0.0819 Cls Loss: 0.0937 Tot Loss: 0.0878 mAP: 0.0212
 Epoch:29 train steps: 1450 Loc Loss: 0.0799 Cls Loss: 0.0906 Tot Loss: 0.0852 mAP: 0.0201
train
 Epoch:30 train steps: 1460 Loc Loss: 0.0817 Cls Loss: 0.0927 Tot Loss: 0.0872 mAP: 0.0209
 Epoch:30 train steps: 1470 Loc Loss: 0.0811 Cls Loss: 0.0918 Tot Loss: 0.0864 mAP: 0.0204
 Epoch:30 train steps: 1480 Loc Loss: 0.0799 Cls Loss: 0.0911 Tot Loss: 0.0855 mAP: 0.0200
 Epoch:30 train steps: 1490 Loc Loss: 0.0805 Cls Loss: 0.0927 Tot Loss: 0.0866 mAP: 0.0209
 Epoch:30 train steps: 1500 Loc Loss: 0.0799 Cls Loss: 0.0912 Tot Loss: 0.0855 mAP: 0.0194
train
 Epoch:31 train steps: 1510 Loc Loss: 0.0807 Cls Loss: 0.0920 Tot Loss: 0.0864 mAP: 0.0205
 Epoch:31 train steps: 1520 Loc Loss: 0.0796 Cls Loss: 0.0913 Tot Loss: 0.0855 mAP: 0.0190
 Epoch:31 train steps: 1530 Loc Loss: 0.0815 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0195
 Epoch:31 train steps: 1540 Loc Loss: 0.0808 Cls Loss: 0.0925 Tot Loss: 0.0866 mAP: 0.0201
 Epoch:31 train steps: 1550 Loc Loss: 0.0817 Cls Loss: 0.0933 Tot Loss: 0.0875 mAP: 0.0203
train
 Epoch:32 train steps: 1560 Loc Loss: 0.0808 Cls Loss: 0.0920 Tot Loss: 0.0864 mAP: 0.0203
 Epoch:32 train steps: 1570 Loc Loss: 0.0809 Cls Loss: 0.0922 Tot Loss: 0.0865 mAP: 0.0210
 Epoch:32 train steps: 1580 Loc Loss: 0.0796 Cls Loss: 0.0906 Tot Loss: 0.0851 mAP: 0.0197
 Epoch:32 train steps: 1590 Loc Loss: 0.0805 Cls Loss: 0.0927 Tot Loss: 0.0866 mAP: 0.0213
 Epoch:32 train steps: 1600 Loc Loss: 0.0819 Cls Loss: 0.0937 Tot Loss: 0.0878 mAP: 0.0206
train
 Epoch:33 train steps: 1610 Loc Loss: 0.0821 Cls Loss: 0.0937 Tot Loss: 0.0879 mAP: 0.0202
 Epoch:33 train steps: 1620 Loc Loss: 0.0822 Cls Loss: 0.0940 Tot Loss: 0.0881 mAP: 0.0203
 Epoch:33 train steps: 1630 Loc Loss: 0.0814 Cls Loss: 0.0925 Tot Loss: 0.0869 mAP: 0.0208
 Epoch:33 train steps: 1640 Loc Loss: 0.0802 Cls Loss: 0.0908 Tot Loss: 0.0855 mAP: 0.0198
 Epoch:33 train steps: 1650 Loc Loss: 0.0809 Cls Loss: 0.0918 Tot Loss: 0.0864 mAP: 0.0216
train
 Epoch:34 train steps: 1660 Loc Loss: 0.0797 Cls Loss: 0.0910 Tot Loss: 0.0854 mAP: 0.0195
 Epoch:34 train steps: 1670 Loc Loss: 0.0814 Cls Loss: 0.0933 Tot Loss: 0.0873 mAP: 0.0207
 Epoch:34 train steps: 1680 Loc Loss: 0.0810 Cls Loss: 0.0925 Tot Loss: 0.0867 mAP: 0.0199
 Epoch:34 train steps: 1690 Loc Loss: 0.0800 Cls Loss: 0.0921 Tot Loss: 0.0860 mAP: 0.0215
 Epoch:34 train steps: 1700 Loc Loss: 0.0812 Cls Loss: 0.0927 Tot Loss: 0.0869 mAP: 0.0203
train
 Epoch:35 train steps: 1710 Loc Loss: 0.0797 Cls Loss: 0.0907 Tot Loss: 0.0852 mAP: 0.0228
 Epoch:35 train steps: 1720 Loc Loss: 0.0811 Cls Loss: 0.0925 Tot Loss: 0.0868 mAP: 0.0198
 Epoch:35 train steps: 1730 Loc Loss: 0.0810 Cls Loss: 0.0925 Tot Loss: 0.0867 mAP: 0.0208
 Epoch:35 train steps: 1740 Loc Loss: 0.0825 Cls Loss: 0.0942 Tot Loss: 0.0884 mAP: 0.0211
 Epoch:35 train steps: 1750 Loc Loss: 0.0811 Cls Loss: 0.0927 Tot Loss: 0.0869 mAP: 0.0193
train
 Epoch:36 train steps: 1760 Loc Loss: 0.0824 Cls Loss: 0.0943 Tot Loss: 0.0884 mAP: 0.0208
 Epoch:36 train steps: 1770 Loc Loss: 0.0818 Cls Loss: 0.0936 Tot Loss: 0.0877 mAP: 0.0205
 Epoch:36 train steps: 1780 Loc Loss: 0.0804 Cls Loss: 0.0917 Tot Loss: 0.0861 mAP: 0.0201
 Epoch:36 train steps: 1790 Loc Loss: 0.0794 Cls Loss: 0.0900 Tot Loss: 0.0847 mAP: 0.0189
 Epoch:36 train steps: 1800 Loc Loss: 0.0816 Cls Loss: 0.0928 Tot Loss: 0.0872 mAP: 0.0199
train
 Epoch:37 train steps: 1810 Loc Loss: 0.0806 Cls Loss: 0.0918 Tot Loss: 0.0862 mAP: 0.0196
 Epoch:37 train steps: 1820 Loc Loss: 0.0806 Cls Loss: 0.0918 Tot Loss: 0.0862 mAP: 0.0207
 Epoch:37 train steps: 1830 Loc Loss: 0.0804 Cls Loss: 0.0919 Tot Loss: 0.0862 mAP: 0.0210
 Epoch:37 train steps: 1840 Loc Loss: 0.0807 Cls Loss: 0.0918 Tot Loss: 0.0863 mAP: 0.0200
 Epoch:37 train steps: 1850 Loc Loss: 0.0811 Cls Loss: 0.0929 Tot Loss: 0.0870 mAP: 0.0206
train
 Epoch:38 train steps: 1860 Loc Loss: 0.0807 Cls Loss: 0.0923 Tot Loss: 0.0865 mAP: 0.0205
 Epoch:38 train steps: 1870 Loc Loss: 0.0801 Cls Loss: 0.0916 Tot Loss: 0.0858 mAP: 0.0211
 Epoch:38 train steps: 1880 Loc Loss: 0.0816 Cls Loss: 0.0935 Tot Loss: 0.0876 mAP: 0.0194
 Epoch:38 train steps: 1890 Loc Loss: 0.0831 Cls Loss: 0.0942 Tot Loss: 0.0886 mAP: 0.0213
 Epoch:38 train steps: 1900 Loc Loss: 0.0801 Cls Loss: 0.0915 Tot Loss: 0.0858 mAP: 0.0211
train
 Epoch:39 train steps: 1910 Loc Loss: 0.0805 Cls Loss: 0.0920 Tot Loss: 0.0863 mAP: 0.0195
 Epoch:39 train steps: 1920 Loc Loss: 0.0809 Cls Loss: 0.0919 Tot Loss: 0.0864 mAP: 0.0204
 Epoch:39 train steps: 1930 Loc Loss: 0.0793 Cls Loss: 0.0908 Tot Loss: 0.0850 mAP: 0.0197
 Epoch:39 train steps: 1940 Loc Loss: 0.0807 Cls Loss: 0.0917 Tot Loss: 0.0862 mAP: 0.0187
 Epoch:39 train steps: 1950 Loc Loss: 0.0804 Cls Loss: 0.0915 Tot Loss: 0.0860 mAP: 0.0190
train
 Epoch:40 train steps: 1960 Loc Loss: 0.0810 Cls Loss: 0.0924 Tot Loss: 0.0867 mAP: 0.0199
 Epoch:40 train steps: 1970 Loc Loss: 0.0809 Cls Loss: 0.0922 Tot Loss: 0.0866 mAP: 0.0195
 Epoch:40 train steps: 1980 Loc Loss: 0.0809 Cls Loss: 0.0931 Tot Loss: 0.0870 mAP: 0.0204
 Epoch:40 train steps: 1990 Loc Loss: 0.0802 Cls Loss: 0.0907 Tot Loss: 0.0855 mAP: 0.0202
 Epoch:40 train steps: 2000 Loc Loss: 0.0806 Cls Loss: 0.0914 Tot Loss: 0.0860 mAP: 0.0203
val
 Epoch:40 val Loc Loss: 0.1081 Cls Loss: 0.1257 Tot Loss: 0.1169 mAP: 0.0270
Step 2000/64000.0
----------
train
 Epoch:41 train steps: 2010 Loc Loss: 0.0801 Cls Loss: 0.0910 Tot Loss: 0.0855 mAP: 0.0216
 Epoch:41 train steps: 2020 Loc Loss: 0.0802 Cls Loss: 0.0918 Tot Loss: 0.0860 mAP: 0.0210
 Epoch:41 train steps: 2030 Loc Loss: 0.0809 Cls Loss: 0.0919 Tot Loss: 0.0864 mAP: 0.0209
 Epoch:41 train steps: 2040 Loc Loss: 0.0806 Cls Loss: 0.0925 Tot Loss: 0.0865 mAP: 0.0194
 Epoch:41 train steps: 2050 Loc Loss: 0.0838 Cls Loss: 0.0956 Tot Loss: 0.0897 mAP: 0.0207
train
 Epoch:42 train steps: 2060 Loc Loss: 0.0827 Cls Loss: 0.0944 Tot Loss: 0.0885 mAP: 0.0191
 Epoch:42 train steps: 2070 Loc Loss: 0.0788 Cls Loss: 0.0905 Tot Loss: 0.0847 mAP: 0.0197
 Epoch:42 train steps: 2080 Loc Loss: 0.0804 Cls Loss: 0.0914 Tot Loss: 0.0859 mAP: 0.0192
 Epoch:42 train steps: 2090 Loc Loss: 0.0814 Cls Loss: 0.0924 Tot Loss: 0.0869 mAP: 0.0207
 Epoch:42 train steps: 2100 Loc Loss: 0.0819 Cls Loss: 0.0934 Tot Loss: 0.0876 mAP: 0.0202
train
 Epoch:43 train steps: 2110 Loc Loss: 0.0796 Cls Loss: 0.0907 Tot Loss: 0.0851 mAP: 0.0202
 Epoch:43 train steps: 2120 Loc Loss: 0.0810 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0204
 Epoch:43 train steps: 2130 Loc Loss: 0.0821 Cls Loss: 0.0937 Tot Loss: 0.0879 mAP: 0.0194
 Epoch:43 train steps: 2140 Loc Loss: 0.0809 Cls Loss: 0.0915 Tot Loss: 0.0862 mAP: 0.0216
 Epoch:43 train steps: 2150 Loc Loss: 0.0807 Cls Loss: 0.0920 Tot Loss: 0.0864 mAP: 0.0200
train
 Epoch:44 train steps: 2160 Loc Loss: 0.0795 Cls Loss: 0.0917 Tot Loss: 0.0856 mAP: 0.0197
 Epoch:44 train steps: 2170 Loc Loss: 0.0820 Cls Loss: 0.0939 Tot Loss: 0.0880 mAP: 0.0203
 Epoch:44 train steps: 2180 Loc Loss: 0.0804 Cls Loss: 0.0915 Tot Loss: 0.0859 mAP: 0.0200
 Epoch:44 train steps: 2190 Loc Loss: 0.0801 Cls Loss: 0.0913 Tot Loss: 0.0857 mAP: 0.0205
 Epoch:44 train steps: 2200 Loc Loss: 0.0798 Cls Loss: 0.0915 Tot Loss: 0.0856 mAP: 0.0204
train
 Epoch:45 train steps: 2210 Loc Loss: 0.0801 Cls Loss: 0.0918 Tot Loss: 0.0859 mAP: 0.0201
 Epoch:45 train steps: 2220 Loc Loss: 0.0808 Cls Loss: 0.0917 Tot Loss: 0.0863 mAP: 0.0189
 Epoch:45 train steps: 2230 Loc Loss: 0.0788 Cls Loss: 0.0902 Tot Loss: 0.0845 mAP: 0.0198
 Epoch:45 train steps: 2240 Loc Loss: 0.0810 Cls Loss: 0.0922 Tot Loss: 0.0866 mAP: 0.0203
 Epoch:45 train steps: 2250 Loc Loss: 0.0818 Cls Loss: 0.0934 Tot Loss: 0.0876 mAP: 0.0223
train
 Epoch:46 train steps: 2260 Loc Loss: 0.0795 Cls Loss: 0.0905 Tot Loss: 0.0850 mAP: 0.0192
 Epoch:46 train steps: 2270 Loc Loss: 0.0825 Cls Loss: 0.0935 Tot Loss: 0.0880 mAP: 0.0208
 Epoch:46 train steps: 2280 Loc Loss: 0.0796 Cls Loss: 0.0911 Tot Loss: 0.0854 mAP: 0.0190
 Epoch:46 train steps: 2290 Loc Loss: 0.0792 Cls Loss: 0.0903 Tot Loss: 0.0848 mAP: 0.0205
 Epoch:46 train steps: 2300 Loc Loss: 0.0814 Cls Loss: 0.0933 Tot Loss: 0.0873 mAP: 0.0198
train
 Epoch:47 train steps: 2310 Loc Loss: 0.0798 Cls Loss: 0.0907 Tot Loss: 0.0852 mAP: 0.0196
 Epoch:47 train steps: 2320 Loc Loss: 0.0802 Cls Loss: 0.0915 Tot Loss: 0.0858 mAP: 0.0217
 Epoch:47 train steps: 2330 Loc Loss: 0.0803 Cls Loss: 0.0916 Tot Loss: 0.0859 mAP: 0.0198
 Epoch:47 train steps: 2340 Loc Loss: 0.0811 Cls Loss: 0.0927 Tot Loss: 0.0869 mAP: 0.0204
 Epoch:47 train steps: 2350 Loc Loss: 0.0804 Cls Loss: 0.0913 Tot Loss: 0.0858 mAP: 0.0199
train
 Epoch:48 train steps: 2360 Loc Loss: 0.0797 Cls Loss: 0.0903 Tot Loss: 0.0850 mAP: 0.0202
 Epoch:48 train steps: 2370 Loc Loss: 0.0808 Cls Loss: 0.0924 Tot Loss: 0.0866 mAP: 0.0207
 Epoch:48 train steps: 2380 Loc Loss: 0.0787 Cls Loss: 0.0898 Tot Loss: 0.0842 mAP: 0.0195
 Epoch:48 train steps: 2390 Loc Loss: 0.0823 Cls Loss: 0.0935 Tot Loss: 0.0879 mAP: 0.0196
 Epoch:48 train steps: 2400 Loc Loss: 0.0811 Cls Loss: 0.0925 Tot Loss: 0.0868 mAP: 0.0209
train
 Epoch:49 train steps: 2410 Loc Loss: 0.0807 Cls Loss: 0.0923 Tot Loss: 0.0865 mAP: 0.0205
 Epoch:49 train steps: 2420 Loc Loss: 0.0817 Cls Loss: 0.0925 Tot Loss: 0.0871 mAP: 0.0197
 Epoch:49 train steps: 2430 Loc Loss: 0.0812 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0205
 Epoch:49 train steps: 2440 Loc Loss: 0.0816 Cls Loss: 0.0928 Tot Loss: 0.0872 mAP: 0.0210
 Epoch:49 train steps: 2450 Loc Loss: 0.0802 Cls Loss: 0.0917 Tot Loss: 0.0860 mAP: 0.0214
train
 Epoch:50 train steps: 2460 Loc Loss: 0.0797 Cls Loss: 0.0908 Tot Loss: 0.0852 mAP: 0.0202
 Epoch:50 train steps: 2470 Loc Loss: 0.0797 Cls Loss: 0.0901 Tot Loss: 0.0849 mAP: 0.0217
 Epoch:50 train steps: 2480 Loc Loss: 0.0819 Cls Loss: 0.0934 Tot Loss: 0.0876 mAP: 0.0206
 Epoch:50 train steps: 2490 Loc Loss: 0.0799 Cls Loss: 0.0911 Tot Loss: 0.0855 mAP: 0.0196
 Epoch:50 train steps: 2500 Loc Loss: 0.0809 Cls Loss: 0.0926 Tot Loss: 0.0867 mAP: 0.0216
train
 Epoch:51 train steps: 2510 Loc Loss: 0.0790 Cls Loss: 0.0904 Tot Loss: 0.0847 mAP: 0.0193
 Epoch:51 train steps: 2520 Loc Loss: 0.0795 Cls Loss: 0.0913 Tot Loss: 0.0854 mAP: 0.0204
 Epoch:51 train steps: 2530 Loc Loss: 0.0810 Cls Loss: 0.0928 Tot Loss: 0.0869 mAP: 0.0201
 Epoch:51 train steps: 2540 Loc Loss: 0.0822 Cls Loss: 0.0934 Tot Loss: 0.0878 mAP: 0.0205
 Epoch:51 train steps: 2550 Loc Loss: 0.0802 Cls Loss: 0.0909 Tot Loss: 0.0856 mAP: 0.0199
train
 Epoch:52 train steps: 2560 Loc Loss: 0.0826 Cls Loss: 0.0940 Tot Loss: 0.0883 mAP: 0.0213
 Epoch:52 train steps: 2570 Loc Loss: 0.0791 Cls Loss: 0.0907 Tot Loss: 0.0849 mAP: 0.0195
 Epoch:52 train steps: 2580 Loc Loss: 0.0807 Cls Loss: 0.0928 Tot Loss: 0.0868 mAP: 0.0202
 Epoch:52 train steps: 2590 Loc Loss: 0.0805 Cls Loss: 0.0917 Tot Loss: 0.0861 mAP: 0.0213
 Epoch:52 train steps: 2600 Loc Loss: 0.0813 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0198
train
 Epoch:53 train steps: 2610 Loc Loss: 0.0821 Cls Loss: 0.0932 Tot Loss: 0.0877 mAP: 0.0202
 Epoch:53 train steps: 2620 Loc Loss: 0.0807 Cls Loss: 0.0923 Tot Loss: 0.0865 mAP: 0.0209
 Epoch:53 train steps: 2630 Loc Loss: 0.0796 Cls Loss: 0.0905 Tot Loss: 0.0851 mAP: 0.0190
 Epoch:53 train steps: 2640 Loc Loss: 0.0803 Cls Loss: 0.0915 Tot Loss: 0.0859 mAP: 0.0216
 Epoch:53 train steps: 2650 Loc Loss: 0.0801 Cls Loss: 0.0913 Tot Loss: 0.0857 mAP: 0.0200
train
 Epoch:54 train steps: 2660 Loc Loss: 0.0800 Cls Loss: 0.0913 Tot Loss: 0.0857 mAP: 0.0190
 Epoch:54 train steps: 2670 Loc Loss: 0.0806 Cls Loss: 0.0921 Tot Loss: 0.0864 mAP: 0.0190
 Epoch:54 train steps: 2680 Loc Loss: 0.0806 Cls Loss: 0.0918 Tot Loss: 0.0862 mAP: 0.0207
 Epoch:54 train steps: 2690 Loc Loss: 0.0808 Cls Loss: 0.0927 Tot Loss: 0.0868 mAP: 0.0206
 Epoch:54 train steps: 2700 Loc Loss: 0.0810 Cls Loss: 0.0922 Tot Loss: 0.0866 mAP: 0.0201
train
 Epoch:55 train steps: 2710 Loc Loss: 0.0812 Cls Loss: 0.0928 Tot Loss: 0.0870 mAP: 0.0206
 Epoch:55 train steps: 2720 Loc Loss: 0.0804 Cls Loss: 0.0920 Tot Loss: 0.0862 mAP: 0.0197
 Epoch:55 train steps: 2730 Loc Loss: 0.0808 Cls Loss: 0.0923 Tot Loss: 0.0866 mAP: 0.0204
 Epoch:55 train steps: 2740 Loc Loss: 0.0807 Cls Loss: 0.0921 Tot Loss: 0.0864 mAP: 0.0213
 Epoch:55 train steps: 2750 Loc Loss: 0.0808 Cls Loss: 0.0916 Tot Loss: 0.0862 mAP: 0.0211
train
 Epoch:56 train steps: 2760 Loc Loss: 0.0798 Cls Loss: 0.0912 Tot Loss: 0.0855 mAP: 0.0199
 Epoch:56 train steps: 2770 Loc Loss: 0.0796 Cls Loss: 0.0910 Tot Loss: 0.0853 mAP: 0.0191
 Epoch:56 train steps: 2780 Loc Loss: 0.0794 Cls Loss: 0.0909 Tot Loss: 0.0852 mAP: 0.0195
 Epoch:56 train steps: 2790 Loc Loss: 0.0801 Cls Loss: 0.0912 Tot Loss: 0.0857 mAP: 0.0197
 Epoch:56 train steps: 2800 Loc Loss: 0.0809 Cls Loss: 0.0921 Tot Loss: 0.0865 mAP: 0.0192
train
 Epoch:57 train steps: 2810 Loc Loss: 0.0805 Cls Loss: 0.0918 Tot Loss: 0.0862 mAP: 0.0201
 Epoch:57 train steps: 2820 Loc Loss: 0.0824 Cls Loss: 0.0942 Tot Loss: 0.0883 mAP: 0.0207
 Epoch:57 train steps: 2830 Loc Loss: 0.0808 Cls Loss: 0.0915 Tot Loss: 0.0861 mAP: 0.0200
 Epoch:57 train steps: 2840 Loc Loss: 0.0815 Cls Loss: 0.0926 Tot Loss: 0.0871 mAP: 0.0208
 Epoch:57 train steps: 2850 Loc Loss: 0.0798 Cls Loss: 0.0914 Tot Loss: 0.0856 mAP: 0.0194
train
 Epoch:58 train steps: 2860 Loc Loss: 0.0814 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0208
 Epoch:58 train steps: 2870 Loc Loss: 0.0829 Cls Loss: 0.0944 Tot Loss: 0.0886 mAP: 0.0196
 Epoch:58 train steps: 2880 Loc Loss: 0.0805 Cls Loss: 0.0923 Tot Loss: 0.0864 mAP: 0.0197
 Epoch:58 train steps: 2890 Loc Loss: 0.0815 Cls Loss: 0.0926 Tot Loss: 0.0871 mAP: 0.0198
 Epoch:58 train steps: 2900 Loc Loss: 0.0791 Cls Loss: 0.0901 Tot Loss: 0.0846 mAP: 0.0209
train
 Epoch:59 train steps: 2910 Loc Loss: 0.0788 Cls Loss: 0.0897 Tot Loss: 0.0842 mAP: 0.0191
 Epoch:59 train steps: 2920 Loc Loss: 0.0814 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0204
 Epoch:59 train steps: 2930 Loc Loss: 0.0817 Cls Loss: 0.0939 Tot Loss: 0.0878 mAP: 0.0195
 Epoch:59 train steps: 2940 Loc Loss: 0.0812 Cls Loss: 0.0921 Tot Loss: 0.0866 mAP: 0.0195
 Epoch:59 train steps: 2950 Loc Loss: 0.0804 Cls Loss: 0.0925 Tot Loss: 0.0864 mAP: 0.0206
train
 Epoch:60 train steps: 2960 Loc Loss: 0.0811 Cls Loss: 0.0921 Tot Loss: 0.0866 mAP: 0.0202
 Epoch:60 train steps: 2970 Loc Loss: 0.0811 Cls Loss: 0.0930 Tot Loss: 0.0870 mAP: 0.0253
 Epoch:60 train steps: 2980 Loc Loss: 0.0798 Cls Loss: 0.0909 Tot Loss: 0.0854 mAP: 0.0201
 Epoch:60 train steps: 2990 Loc Loss: 0.0811 Cls Loss: 0.0925 Tot Loss: 0.0868 mAP: 0.0198
 Epoch:60 train steps: 3000 Loc Loss: 0.0807 Cls Loss: 0.0921 Tot Loss: 0.0864 mAP: 0.0196
val
 Epoch:60 val Loc Loss: 0.1065 Cls Loss: 0.1228 Tot Loss: 0.1147 mAP: 0.0265
Step 3000/64000.0
----------
train
 Epoch:61 train steps: 3010 Loc Loss: 0.0819 Cls Loss: 0.0929 Tot Loss: 0.0874 mAP: 0.0204
 Epoch:61 train steps: 3020 Loc Loss: 0.0817 Cls Loss: 0.0931 Tot Loss: 0.0874 mAP: 0.0218
 Epoch:61 train steps: 3030 Loc Loss: 0.0801 Cls Loss: 0.0917 Tot Loss: 0.0859 mAP: 0.0206
 Epoch:61 train steps: 3040 Loc Loss: 0.0795 Cls Loss: 0.0901 Tot Loss: 0.0848 mAP: 0.0186
 Epoch:61 train steps: 3050 Loc Loss: 0.0809 Cls Loss: 0.0928 Tot Loss: 0.0869 mAP: 0.0202
train
 Epoch:62 train steps: 3060 Loc Loss: 0.0804 Cls Loss: 0.0918 Tot Loss: 0.0861 mAP: 0.0190
 Epoch:62 train steps: 3070 Loc Loss: 0.0810 Cls Loss: 0.0927 Tot Loss: 0.0868 mAP: 0.0201
 Epoch:62 train steps: 3080 Loc Loss: 0.0806 Cls Loss: 0.0910 Tot Loss: 0.0858 mAP: 0.0198
 Epoch:62 train steps: 3090 Loc Loss: 0.0810 Cls Loss: 0.0922 Tot Loss: 0.0866 mAP: 0.0213
 Epoch:62 train steps: 3100 Loc Loss: 0.0813 Cls Loss: 0.0935 Tot Loss: 0.0874 mAP: 0.0212
train
 Epoch:63 train steps: 3110 Loc Loss: 0.0804 Cls Loss: 0.0918 Tot Loss: 0.0861 mAP: 0.0201
 Epoch:63 train steps: 3120 Loc Loss: 0.0799 Cls Loss: 0.0910 Tot Loss: 0.0855 mAP: 0.0201
 Epoch:63 train steps: 3130 Loc Loss: 0.0805 Cls Loss: 0.0919 Tot Loss: 0.0862 mAP: 0.0199
 Epoch:63 train steps: 3140 Loc Loss: 0.0811 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0199
 Epoch:63 train steps: 3150 Loc Loss: 0.0810 Cls Loss: 0.0929 Tot Loss: 0.0869 mAP: 0.0197
train
 Epoch:64 train steps: 3160 Loc Loss: 0.0796 Cls Loss: 0.0916 Tot Loss: 0.0856 mAP: 0.0203
 Epoch:64 train steps: 3170 Loc Loss: 0.0800 Cls Loss: 0.0912 Tot Loss: 0.0856 mAP: 0.0200
 Epoch:64 train steps: 3180 Loc Loss: 0.0823 Cls Loss: 0.0952 Tot Loss: 0.0887 mAP: 0.0208
 Epoch:64 train steps: 3190 Loc Loss: 0.0820 Cls Loss: 0.0937 Tot Loss: 0.0879 mAP: 0.0204
 Epoch:64 train steps: 3200 Loc Loss: 0.0806 Cls Loss: 0.0916 Tot Loss: 0.0861 mAP: 0.0197
train
 Epoch:65 train steps: 3210 Loc Loss: 0.0800 Cls Loss: 0.0912 Tot Loss: 0.0856 mAP: 0.0196
 Epoch:65 train steps: 3220 Loc Loss: 0.0793 Cls Loss: 0.0895 Tot Loss: 0.0844 mAP: 0.0196
 Epoch:65 train steps: 3230 Loc Loss: 0.0804 Cls Loss: 0.0915 Tot Loss: 0.0860 mAP: 0.0209
 Epoch:65 train steps: 3240 Loc Loss: 0.0804 Cls Loss: 0.0916 Tot Loss: 0.0860 mAP: 0.0199
 Epoch:65 train steps: 3250 Loc Loss: 0.0814 Cls Loss: 0.0923 Tot Loss: 0.0868 mAP: 0.0232
train
 Epoch:66 train steps: 3260 Loc Loss: 0.0821 Cls Loss: 0.0930 Tot Loss: 0.0876 mAP: 0.0206
 Epoch:66 train steps: 3270 Loc Loss: 0.0811 Cls Loss: 0.0926 Tot Loss: 0.0869 mAP: 0.0195
 Epoch:66 train steps: 3280 Loc Loss: 0.0820 Cls Loss: 0.0934 Tot Loss: 0.0877 mAP: 0.0217
 Epoch:66 train steps: 3290 Loc Loss: 0.0799 Cls Loss: 0.0917 Tot Loss: 0.0858 mAP: 0.0195
 Epoch:66 train steps: 3300 Loc Loss: 0.0784 Cls Loss: 0.0893 Tot Loss: 0.0838 mAP: 0.0186
train
 Epoch:67 train steps: 3310 Loc Loss: 0.0820 Cls Loss: 0.0937 Tot Loss: 0.0878 mAP: 0.0205
 Epoch:67 train steps: 3320 Loc Loss: 0.0804 Cls Loss: 0.0920 Tot Loss: 0.0862 mAP: 0.0194
 Epoch:67 train steps: 3330 Loc Loss: 0.0807 Cls Loss: 0.0920 Tot Loss: 0.0864 mAP: 0.0208
 Epoch:67 train steps: 3340 Loc Loss: 0.0819 Cls Loss: 0.0934 Tot Loss: 0.0876 mAP: 0.0207
 Epoch:67 train steps: 3350 Loc Loss: 0.0809 Cls Loss: 0.0933 Tot Loss: 0.0871 mAP: 0.0201
train
 Epoch:68 train steps: 3360 Loc Loss: 0.0806 Cls Loss: 0.0916 Tot Loss: 0.0861 mAP: 0.0193
 Epoch:68 train steps: 3370 Loc Loss: 0.0801 Cls Loss: 0.0914 Tot Loss: 0.0857 mAP: 0.0203
 Epoch:68 train steps: 3380 Loc Loss: 0.0782 Cls Loss: 0.0887 Tot Loss: 0.0834 mAP: 0.0213
 Epoch:68 train steps: 3390 Loc Loss: 0.0804 Cls Loss: 0.0915 Tot Loss: 0.0859 mAP: 0.0202
 Epoch:68 train steps: 3400 Loc Loss: 0.0808 Cls Loss: 0.0920 Tot Loss: 0.0864 mAP: 0.0208
train
 Epoch:69 train steps: 3410 Loc Loss: 0.0805 Cls Loss: 0.0921 Tot Loss: 0.0863 mAP: 0.0196
 Epoch:69 train steps: 3420 Loc Loss: 0.0799 Cls Loss: 0.0912 Tot Loss: 0.0856 mAP: 0.0197
 Epoch:69 train steps: 3430 Loc Loss: 0.0810 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0211
 Epoch:69 train steps: 3440 Loc Loss: 0.0810 Cls Loss: 0.0922 Tot Loss: 0.0866 mAP: 0.0202
 Epoch:69 train steps: 3450 Loc Loss: 0.0803 Cls Loss: 0.0915 Tot Loss: 0.0859 mAP: 0.0210
train
 Epoch:70 train steps: 3460 Loc Loss: 0.0805 Cls Loss: 0.0913 Tot Loss: 0.0859 mAP: 0.0200
 Epoch:70 train steps: 3470 Loc Loss: 0.0800 Cls Loss: 0.0914 Tot Loss: 0.0857 mAP: 0.0191
 Epoch:70 train steps: 3480 Loc Loss: 0.0788 Cls Loss: 0.0902 Tot Loss: 0.0845 mAP: 0.0190
 Epoch:70 train steps: 3490 Loc Loss: 0.0820 Cls Loss: 0.0935 Tot Loss: 0.0877 mAP: 0.0211
 Epoch:70 train steps: 3500 Loc Loss: 0.0805 Cls Loss: 0.0925 Tot Loss: 0.0865 mAP: 0.0213
train
 Epoch:71 train steps: 3510 Loc Loss: 0.0825 Cls Loss: 0.0938 Tot Loss: 0.0882 mAP: 0.0214
 Epoch:71 train steps: 3520 Loc Loss: 0.0818 Cls Loss: 0.0934 Tot Loss: 0.0876 mAP: 0.0215
 Epoch:71 train steps: 3530 Loc Loss: 0.0811 Cls Loss: 0.0931 Tot Loss: 0.0871 mAP: 0.0194
 Epoch:71 train steps: 3540 Loc Loss: 0.0802 Cls Loss: 0.0911 Tot Loss: 0.0857 mAP: 0.0199
 Epoch:71 train steps: 3550 Loc Loss: 0.0803 Cls Loss: 0.0913 Tot Loss: 0.0858 mAP: 0.0205
train
 Epoch:72 train steps: 3560 Loc Loss: 0.0811 Cls Loss: 0.0927 Tot Loss: 0.0869 mAP: 0.0205
 Epoch:72 train steps: 3570 Loc Loss: 0.0820 Cls Loss: 0.0941 Tot Loss: 0.0880 mAP: 0.0196
 Epoch:72 train steps: 3580 Loc Loss: 0.0802 Cls Loss: 0.0911 Tot Loss: 0.0856 mAP: 0.0212
 Epoch:72 train steps: 3590 Loc Loss: 0.0795 Cls Loss: 0.0915 Tot Loss: 0.0855 mAP: 0.0190
 Epoch:72 train steps: 3600 Loc Loss: 0.0827 Cls Loss: 0.0948 Tot Loss: 0.0888 mAP: 0.0217
train
 Epoch:73 train steps: 3610 Loc Loss: 0.0800 Cls Loss: 0.0909 Tot Loss: 0.0854 mAP: 0.0195
 Epoch:73 train steps: 3620 Loc Loss: 0.0799 Cls Loss: 0.0910 Tot Loss: 0.0855 mAP: 0.0207
 Epoch:73 train steps: 3630 Loc Loss: 0.0812 Cls Loss: 0.0930 Tot Loss: 0.0871 mAP: 0.0200
 Epoch:73 train steps: 3640 Loc Loss: 0.0803 Cls Loss: 0.0913 Tot Loss: 0.0858 mAP: 0.0198
 Epoch:73 train steps: 3650 Loc Loss: 0.0808 Cls Loss: 0.0925 Tot Loss: 0.0866 mAP: 0.0214
train
 Epoch:74 train steps: 3660 Loc Loss: 0.0810 Cls Loss: 0.0924 Tot Loss: 0.0867 mAP: 0.0241
 Epoch:74 train steps: 3670 Loc Loss: 0.0805 Cls Loss: 0.0918 Tot Loss: 0.0862 mAP: 0.0202
 Epoch:74 train steps: 3680 Loc Loss: 0.0824 Cls Loss: 0.0942 Tot Loss: 0.0883 mAP: 0.0209
 Epoch:74 train steps: 3690 Loc Loss: 0.0801 Cls Loss: 0.0919 Tot Loss: 0.0860 mAP: 0.0195
 Epoch:74 train steps: 3700 Loc Loss: 0.0797 Cls Loss: 0.0916 Tot Loss: 0.0856 mAP: 0.0195
train
 Epoch:75 train steps: 3710 Loc Loss: 0.0798 Cls Loss: 0.0915 Tot Loss: 0.0856 mAP: 0.0187
 Epoch:75 train steps: 3720 Loc Loss: 0.0813 Cls Loss: 0.0933 Tot Loss: 0.0873 mAP: 0.0204
 Epoch:75 train steps: 3730 Loc Loss: 0.0795 Cls Loss: 0.0909 Tot Loss: 0.0852 mAP: 0.0195
 Epoch:75 train steps: 3740 Loc Loss: 0.0820 Cls Loss: 0.0930 Tot Loss: 0.0875 mAP: 0.0206
 Epoch:75 train steps: 3750 Loc Loss: 0.0812 Cls Loss: 0.0928 Tot Loss: 0.0870 mAP: 0.0207
train
 Epoch:76 train steps: 3760 Loc Loss: 0.0798 Cls Loss: 0.0906 Tot Loss: 0.0852 mAP: 0.0199
 Epoch:76 train steps: 3770 Loc Loss: 0.0813 Cls Loss: 0.0923 Tot Loss: 0.0868 mAP: 0.0196
 Epoch:76 train steps: 3780 Loc Loss: 0.0810 Cls Loss: 0.0927 Tot Loss: 0.0869 mAP: 0.0213
 Epoch:76 train steps: 3790 Loc Loss: 0.0814 Cls Loss: 0.0932 Tot Loss: 0.0873 mAP: 0.0219
 Epoch:76 train steps: 3800 Loc Loss: 0.0819 Cls Loss: 0.0933 Tot Loss: 0.0876 mAP: 0.0202
train
 Epoch:77 train steps: 3810 Loc Loss: 0.0800 Cls Loss: 0.0908 Tot Loss: 0.0854 mAP: 0.0218
 Epoch:77 train steps: 3820 Loc Loss: 0.0812 Cls Loss: 0.0921 Tot Loss: 0.0867 mAP: 0.0210
 Epoch:77 train steps: 3830 Loc Loss: 0.0807 Cls Loss: 0.0922 Tot Loss: 0.0864 mAP: 0.0207
 Epoch:77 train steps: 3840 Loc Loss: 0.0816 Cls Loss: 0.0934 Tot Loss: 0.0875 mAP: 0.0198
 Epoch:77 train steps: 3850 Loc Loss: 0.0814 Cls Loss: 0.0933 Tot Loss: 0.0873 mAP: 0.0199
train
 Epoch:78 train steps: 3860 Loc Loss: 0.0822 Cls Loss: 0.0933 Tot Loss: 0.0878 mAP: 0.0215
 Epoch:78 train steps: 3870 Loc Loss: 0.0811 Cls Loss: 0.0930 Tot Loss: 0.0871 mAP: 0.0200
 Epoch:78 train steps: 3880 Loc Loss: 0.0824 Cls Loss: 0.0948 Tot Loss: 0.0886 mAP: 0.0203
 Epoch:78 train steps: 3890 Loc Loss: 0.0804 Cls Loss: 0.0909 Tot Loss: 0.0857 mAP: 0.0204
 Epoch:78 train steps: 3900 Loc Loss: 0.0812 Cls Loss: 0.0926 Tot Loss: 0.0869 mAP: 0.0210
train
 Epoch:79 train steps: 3910 Loc Loss: 0.0805 Cls Loss: 0.0916 Tot Loss: 0.0861 mAP: 0.0200
 Epoch:79 train steps: 3920 Loc Loss: 0.0808 Cls Loss: 0.0930 Tot Loss: 0.0869 mAP: 0.0195
 Epoch:79 train steps: 3930 Loc Loss: 0.0812 Cls Loss: 0.0930 Tot Loss: 0.0871 mAP: 0.0197
 Epoch:79 train steps: 3940 Loc Loss: 0.0812 Cls Loss: 0.0930 Tot Loss: 0.0871 mAP: 0.0203
 Epoch:79 train steps: 3950 Loc Loss: 0.0808 Cls Loss: 0.0921 Tot Loss: 0.0865 mAP: 0.0193
train
 Epoch:80 train steps: 3960 Loc Loss: 0.0799 Cls Loss: 0.0921 Tot Loss: 0.0860 mAP: 0.0209
 Epoch:80 train steps: 3970 Loc Loss: 0.0809 Cls Loss: 0.0915 Tot Loss: 0.0862 mAP: 0.0206
 Epoch:80 train steps: 3980 Loc Loss: 0.0813 Cls Loss: 0.0931 Tot Loss: 0.0872 mAP: 0.0203
 Epoch:80 train steps: 3990 Loc Loss: 0.0808 Cls Loss: 0.0920 Tot Loss: 0.0864 mAP: 0.0200
 Epoch:80 train steps: 4000 Loc Loss: 0.0787 Cls Loss: 0.0900 Tot Loss: 0.0843 mAP: 0.0198
val
 Epoch:80 val Loc Loss: 0.1071 Cls Loss: 0.1242 Tot Loss: 0.1156 mAP: 0.0278
Step 4000/64000.0
----------
train
 Epoch:81 train steps: 4010 Loc Loss: 0.0797 Cls Loss: 0.0907 Tot Loss: 0.0852 mAP: 0.0200
 Epoch:81 train steps: 4020 Loc Loss: 0.0811 Cls Loss: 0.0928 Tot Loss: 0.0869 mAP: 0.0208
 Epoch:81 train steps: 4030 Loc Loss: 0.0790 Cls Loss: 0.0902 Tot Loss: 0.0846 mAP: 0.0213
 Epoch:81 train steps: 4040 Loc Loss: 0.0815 Cls Loss: 0.0933 Tot Loss: 0.0874 mAP: 0.0230
 Epoch:81 train steps: 4050 Loc Loss: 0.0813 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0209
train
 Epoch:82 train steps: 4060 Loc Loss: 0.0818 Cls Loss: 0.0928 Tot Loss: 0.0873 mAP: 0.0202
 Epoch:82 train steps: 4070 Loc Loss: 0.0796 Cls Loss: 0.0900 Tot Loss: 0.0848 mAP: 0.0198
 Epoch:82 train steps: 4080 Loc Loss: 0.0813 Cls Loss: 0.0925 Tot Loss: 0.0869 mAP: 0.0199
 Epoch:82 train steps: 4090 Loc Loss: 0.0804 Cls Loss: 0.0915 Tot Loss: 0.0860 mAP: 0.0204
 Epoch:82 train steps: 4100 Loc Loss: 0.0811 Cls Loss: 0.0929 Tot Loss: 0.0870 mAP: 0.0198
train
 Epoch:83 train steps: 4110 Loc Loss: 0.0799 Cls Loss: 0.0913 Tot Loss: 0.0856 mAP: 0.0213
 Epoch:83 train steps: 4120 Loc Loss: 0.0826 Cls Loss: 0.0938 Tot Loss: 0.0882 mAP: 0.0202
 Epoch:83 train steps: 4130 Loc Loss: 0.0799 Cls Loss: 0.0906 Tot Loss: 0.0852 mAP: 0.0207
 Epoch:83 train steps: 4140 Loc Loss: 0.0796 Cls Loss: 0.0912 Tot Loss: 0.0854 mAP: 0.0205
 Epoch:83 train steps: 4150 Loc Loss: 0.0820 Cls Loss: 0.0937 Tot Loss: 0.0879 mAP: 0.0197
train
 Epoch:84 train steps: 4160 Loc Loss: 0.0815 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0199
 Epoch:84 train steps: 4170 Loc Loss: 0.0818 Cls Loss: 0.0942 Tot Loss: 0.0880 mAP: 0.0196
 Epoch:84 train steps: 4180 Loc Loss: 0.0811 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0201
 Epoch:84 train steps: 4190 Loc Loss: 0.0808 Cls Loss: 0.0912 Tot Loss: 0.0860 mAP: 0.0200
 Epoch:84 train steps: 4200 Loc Loss: 0.0799 Cls Loss: 0.0914 Tot Loss: 0.0856 mAP: 0.0204
train
 Epoch:85 train steps: 4210 Loc Loss: 0.0810 Cls Loss: 0.0920 Tot Loss: 0.0865 mAP: 0.0202
 Epoch:85 train steps: 4220 Loc Loss: 0.0800 Cls Loss: 0.0916 Tot Loss: 0.0858 mAP: 0.0200
 Epoch:85 train steps: 4230 Loc Loss: 0.0823 Cls Loss: 0.0930 Tot Loss: 0.0877 mAP: 0.0198
 Epoch:85 train steps: 4240 Loc Loss: 0.0811 Cls Loss: 0.0924 Tot Loss: 0.0867 mAP: 0.0209
 Epoch:85 train steps: 4250 Loc Loss: 0.0807 Cls Loss: 0.0923 Tot Loss: 0.0865 mAP: 0.0208
train
 Epoch:86 train steps: 4260 Loc Loss: 0.0788 Cls Loss: 0.0899 Tot Loss: 0.0844 mAP: 0.0196
 Epoch:86 train steps: 4270 Loc Loss: 0.0809 Cls Loss: 0.0914 Tot Loss: 0.0862 mAP: 0.0200
 Epoch:86 train steps: 4280 Loc Loss: 0.0819 Cls Loss: 0.0930 Tot Loss: 0.0875 mAP: 0.0210
 Epoch:86 train steps: 4290 Loc Loss: 0.0811 Cls Loss: 0.0928 Tot Loss: 0.0870 mAP: 0.0218
 Epoch:86 train steps: 4300 Loc Loss: 0.0801 Cls Loss: 0.0914 Tot Loss: 0.0857 mAP: 0.0196
train
 Epoch:87 train steps: 4310 Loc Loss: 0.0804 Cls Loss: 0.0911 Tot Loss: 0.0858 mAP: 0.0196
 Epoch:87 train steps: 4320 Loc Loss: 0.0815 Cls Loss: 0.0927 Tot Loss: 0.0871 mAP: 0.0201
 Epoch:87 train steps: 4330 Loc Loss: 0.0818 Cls Loss: 0.0931 Tot Loss: 0.0875 mAP: 0.0198
 Epoch:87 train steps: 4340 Loc Loss: 0.0806 Cls Loss: 0.0920 Tot Loss: 0.0863 mAP: 0.0201
 Epoch:87 train steps: 4350 Loc Loss: 0.0798 Cls Loss: 0.0902 Tot Loss: 0.0850 mAP: 0.0194
train
 Epoch:88 train steps: 4360 Loc Loss: 0.0793 Cls Loss: 0.0902 Tot Loss: 0.0847 mAP: 0.0209
 Epoch:88 train steps: 4370 Loc Loss: 0.0810 Cls Loss: 0.0928 Tot Loss: 0.0869 mAP: 0.0194
 Epoch:88 train steps: 4380 Loc Loss: 0.0814 Cls Loss: 0.0927 Tot Loss: 0.0871 mAP: 0.0219
 Epoch:88 train steps: 4390 Loc Loss: 0.0816 Cls Loss: 0.0925 Tot Loss: 0.0870 mAP: 0.0195
 Epoch:88 train steps: 4400 Loc Loss: 0.0804 Cls Loss: 0.0922 Tot Loss: 0.0863 mAP: 0.0189
train
 Epoch:89 train steps: 4410 Loc Loss: 0.0812 Cls Loss: 0.0926 Tot Loss: 0.0869 mAP: 0.0195
 Epoch:89 train steps: 4420 Loc Loss: 0.0802 Cls Loss: 0.0920 Tot Loss: 0.0861 mAP: 0.0198
 Epoch:89 train steps: 4430 Loc Loss: 0.0823 Cls Loss: 0.0944 Tot Loss: 0.0883 mAP: 0.0210
 Epoch:89 train steps: 4440 Loc Loss: 0.0790 Cls Loss: 0.0901 Tot Loss: 0.0846 mAP: 0.0189
 Epoch:89 train steps: 4450 Loc Loss: 0.0803 Cls Loss: 0.0921 Tot Loss: 0.0862 mAP: 0.0212
train
 Epoch:90 train steps: 4460 Loc Loss: 0.0815 Cls Loss: 0.0931 Tot Loss: 0.0873 mAP: 0.0214
 Epoch:90 train steps: 4470 Loc Loss: 0.0798 Cls Loss: 0.0917 Tot Loss: 0.0858 mAP: 0.0206
 Epoch:90 train steps: 4480 Loc Loss: 0.0795 Cls Loss: 0.0900 Tot Loss: 0.0847 mAP: 0.0206
 Epoch:90 train steps: 4490 Loc Loss: 0.0816 Cls Loss: 0.0924 Tot Loss: 0.0870 mAP: 0.0213
 Epoch:90 train steps: 4500 Loc Loss: 0.0813 Cls Loss: 0.0928 Tot Loss: 0.0870 mAP: 0.0194
train
 Epoch:91 train steps: 4510 Loc Loss: 0.0807 Cls Loss: 0.0924 Tot Loss: 0.0865 mAP: 0.0196
 Epoch:91 train steps: 4520 Loc Loss: 0.0808 Cls Loss: 0.0924 Tot Loss: 0.0866 mAP: 0.0209
 Epoch:91 train steps: 4530 Loc Loss: 0.0803 Cls Loss: 0.0919 Tot Loss: 0.0861 mAP: 0.0194
 Epoch:91 train steps: 4540 Loc Loss: 0.0822 Cls Loss: 0.0932 Tot Loss: 0.0877 mAP: 0.0198
 Epoch:91 train steps: 4550 Loc Loss: 0.0806 Cls Loss: 0.0916 Tot Loss: 0.0861 mAP: 0.0200
train
 Epoch:92 train steps: 4560 Loc Loss: 0.0803 Cls Loss: 0.0914 Tot Loss: 0.0859 mAP: 0.0207
 Epoch:92 train steps: 4570 Loc Loss: 0.0813 Cls Loss: 0.0931 Tot Loss: 0.0872 mAP: 0.0203
 Epoch:92 train steps: 4580 Loc Loss: 0.0797 Cls Loss: 0.0908 Tot Loss: 0.0852 mAP: 0.0199
 Epoch:92 train steps: 4590 Loc Loss: 0.0807 Cls Loss: 0.0918 Tot Loss: 0.0863 mAP: 0.0207
 Epoch:92 train steps: 4600 Loc Loss: 0.0811 Cls Loss: 0.0928 Tot Loss: 0.0869 mAP: 0.0194
train
 Epoch:93 train steps: 4610 Loc Loss: 0.0812 Cls Loss: 0.0922 Tot Loss: 0.0867 mAP: 0.0211
 Epoch:93 train steps: 4620 Loc Loss: 0.0796 Cls Loss: 0.0909 Tot Loss: 0.0853 mAP: 0.0193
 Epoch:93 train steps: 4630 Loc Loss: 0.0800 Cls Loss: 0.0910 Tot Loss: 0.0855 mAP: 0.0197
 Epoch:93 train steps: 4640 Loc Loss: 0.0809 Cls Loss: 0.0922 Tot Loss: 0.0865 mAP: 0.0210
 Epoch:93 train steps: 4650 Loc Loss: 0.0812 Cls Loss: 0.0921 Tot Loss: 0.0866 mAP: 0.0204
train
 Epoch:94 train steps: 4660 Loc Loss: 0.0803 Cls Loss: 0.0917 Tot Loss: 0.0860 mAP: 0.0232
 Epoch:94 train steps: 4670 Loc Loss: 0.0818 Cls Loss: 0.0937 Tot Loss: 0.0877 mAP: 0.0223
 Epoch:94 train steps: 4680 Loc Loss: 0.0799 Cls Loss: 0.0906 Tot Loss: 0.0852 mAP: 0.0194
 Epoch:94 train steps: 4690 Loc Loss: 0.0811 Cls Loss: 0.0921 Tot Loss: 0.0866 mAP: 0.0199
 Epoch:94 train steps: 4700 Loc Loss: 0.0817 Cls Loss: 0.0930 Tot Loss: 0.0874 mAP: 0.0204
train
 Epoch:95 train steps: 4710 Loc Loss: 0.0798 Cls Loss: 0.0911 Tot Loss: 0.0855 mAP: 0.0205
 Epoch:95 train steps: 4720 Loc Loss: 0.0817 Cls Loss: 0.0933 Tot Loss: 0.0875 mAP: 0.0208
 Epoch:95 train steps: 4730 Loc Loss: 0.0807 Cls Loss: 0.0916 Tot Loss: 0.0862 mAP: 0.0204
 Epoch:95 train steps: 4740 Loc Loss: 0.0806 Cls Loss: 0.0922 Tot Loss: 0.0864 mAP: 0.0200
 Epoch:95 train steps: 4750 Loc Loss: 0.0817 Cls Loss: 0.0940 Tot Loss: 0.0879 mAP: 0.0202
train
 Epoch:96 train steps: 4760 Loc Loss: 0.0816 Cls Loss: 0.0927 Tot Loss: 0.0871 mAP: 0.0212
 Epoch:96 train steps: 4770 Loc Loss: 0.0823 Cls Loss: 0.0933 Tot Loss: 0.0878 mAP: 0.0219
 Epoch:96 train steps: 4780 Loc Loss: 0.0801 Cls Loss: 0.0919 Tot Loss: 0.0860 mAP: 0.0202
 Epoch:96 train steps: 4790 Loc Loss: 0.0800 Cls Loss: 0.0917 Tot Loss: 0.0859 mAP: 0.0193
 Epoch:96 train steps: 4800 Loc Loss: 0.0786 Cls Loss: 0.0899 Tot Loss: 0.0843 mAP: 0.0200
train
 Epoch:97 train steps: 4810 Loc Loss: 0.0801 Cls Loss: 0.0918 Tot Loss: 0.0860 mAP: 0.0209
 Epoch:97 train steps: 4820 Loc Loss: 0.0808 Cls Loss: 0.0920 Tot Loss: 0.0864 mAP: 0.0199
 Epoch:97 train steps: 4830 Loc Loss: 0.0816 Cls Loss: 0.0929 Tot Loss: 0.0872 mAP: 0.0199
 Epoch:97 train steps: 4840 Loc Loss: 0.0814 Cls Loss: 0.0921 Tot Loss: 0.0868 mAP: 0.0195
 Epoch:97 train steps: 4850 Loc Loss: 0.0819 Cls Loss: 0.0933 Tot Loss: 0.0876 mAP: 0.0206
train
 Epoch:98 train steps: 4860 Loc Loss: 0.0812 Cls Loss: 0.0920 Tot Loss: 0.0866 mAP: 0.0208
 Epoch:98 train steps: 4870 Loc Loss: 0.0807 Cls Loss: 0.0920 Tot Loss: 0.0863 mAP: 0.0208
 Epoch:98 train steps: 4880 Loc Loss: 0.0801 Cls Loss: 0.0914 Tot Loss: 0.0858 mAP: 0.0206
 Epoch:98 train steps: 4890 Loc Loss: 0.0808 Cls Loss: 0.0926 Tot Loss: 0.0867 mAP: 0.0200
 Epoch:98 train steps: 4900 Loc Loss: 0.0827 Cls Loss: 0.0940 Tot Loss: 0.0884 mAP: 0.0202
train
 Epoch:99 train steps: 4910 Loc Loss: 0.0821 Cls Loss: 0.0935 Tot Loss: 0.0878 mAP: 0.0209
 Epoch:99 train steps: 4920 Loc Loss: 0.0804 Cls Loss: 0.0911 Tot Loss: 0.0857 mAP: 0.0200
 Epoch:99 train steps: 4930 Loc Loss: 0.0811 Cls Loss: 0.0924 Tot Loss: 0.0868 mAP: 0.0208
 Epoch:99 train steps: 4940 Loc Loss: 0.0807 Cls Loss: 0.0925 Tot Loss: 0.0866 mAP: 0.0203
 Epoch:99 train steps: 4950 Loc Loss: 0.0808 Cls Loss: 0.0925 Tot Loss: 0.0866 mAP: 0.0200
train
 Epoch:100 train steps: 4960 Loc Loss: 0.0807 Cls Loss: 0.0917 Tot Loss: 0.0862 mAP: 0.0195
 Epoch:100 train steps: 4970 Loc Loss: 0.0809 Cls Loss: 0.0921 Tot Loss: 0.0865 mAP: 0.0191
 Epoch:100 train steps: 4980 Loc Loss: 0.0801 Cls Loss: 0.0914 Tot Loss: 0.0857 mAP: 0.0215
 Epoch:100 train steps: 4990 Loc Loss: 0.0816 Cls Loss: 0.0931 Tot Loss: 0.0874 mAP: 0.0200
 Epoch:100 train steps: 5000 Loc Loss: 0.0805 Cls Loss: 0.0926 Tot Loss: 0.0865 mAP: 0.0199
val
 Epoch:100 val Loc Loss: 0.1071 Cls Loss: 0.1241 Tot Loss: 0.1156 mAP: 0.0270
Step 5000/64000.0
----------
train
 Epoch:101 train steps: 5010 Loc Loss: 0.0809 Cls Loss: 0.0917 Tot Loss: 0.0863 mAP: 0.0211
 Epoch:101 train steps: 5020 Loc Loss: 0.0809 Cls Loss: 0.0933 Tot Loss: 0.0871 mAP: 0.0200
 Epoch:101 train steps: 5030 Loc Loss: 0.0808 Cls Loss: 0.0931 Tot Loss: 0.0869 mAP: 0.0197
 Epoch:101 train steps: 5040 Loc Loss: 0.0811 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0193
 Epoch:101 train steps: 5050 Loc Loss: 0.0817 Cls Loss: 0.0936 Tot Loss: 0.0877 mAP: 0.0209
train
 Epoch:102 train steps: 5060 Loc Loss: 0.0814 Cls Loss: 0.0926 Tot Loss: 0.0870 mAP: 0.0204
 Epoch:102 train steps: 5070 Loc Loss: 0.0810 Cls Loss: 0.0924 Tot Loss: 0.0867 mAP: 0.0194
 Epoch:102 train steps: 5080 Loc Loss: 0.0793 Cls Loss: 0.0906 Tot Loss: 0.0850 mAP: 0.0188
 Epoch:102 train steps: 5090 Loc Loss: 0.0806 Cls Loss: 0.0909 Tot Loss: 0.0858 mAP: 0.0205
 Epoch:102 train steps: 5100 Loc Loss: 0.0807 Cls Loss: 0.0927 Tot Loss: 0.0867 mAP: 0.0202
train
 Epoch:103 train steps: 5110 Loc Loss: 0.0818 Cls Loss: 0.0926 Tot Loss: 0.0872 mAP: 0.0200
 Epoch:103 train steps: 5120 Loc Loss: 0.0796 Cls Loss: 0.0906 Tot Loss: 0.0851 mAP: 0.0207
 Epoch:103 train steps: 5130 Loc Loss: 0.0803 Cls Loss: 0.0916 Tot Loss: 0.0860 mAP: 0.0203
 Epoch:103 train steps: 5140 Loc Loss: 0.0813 Cls Loss: 0.0923 Tot Loss: 0.0868 mAP: 0.0198
 Epoch:103 train steps: 5150 Loc Loss: 0.0800 Cls Loss: 0.0914 Tot Loss: 0.0857 mAP: 0.0201
train
 Epoch:104 train steps: 5160 Loc Loss: 0.0803 Cls Loss: 0.0914 Tot Loss: 0.0859 mAP: 0.0199
 Epoch:104 train steps: 5170 Loc Loss: 0.0806 Cls Loss: 0.0916 Tot Loss: 0.0861 mAP: 0.0196
 Epoch:104 train steps: 5180 Loc Loss: 0.0824 Cls Loss: 0.0938 Tot Loss: 0.0881 mAP: 0.0211
 Epoch:104 train steps: 5190 Loc Loss: 0.0805 Cls Loss: 0.0923 Tot Loss: 0.0864 mAP: 0.0222
 Epoch:104 train steps: 5200 Loc Loss: 0.0798 Cls Loss: 0.0912 Tot Loss: 0.0855 mAP: 0.0194
train
 Epoch:105 train steps: 5210 Loc Loss: 0.0813 Cls Loss: 0.0923 Tot Loss: 0.0868 mAP: 0.0206
 Epoch:105 train steps: 5220 Loc Loss: 0.0810 Cls Loss: 0.0925 Tot Loss: 0.0868 mAP: 0.0208
 Epoch:105 train steps: 5230 Loc Loss: 0.0817 Cls Loss: 0.0935 Tot Loss: 0.0876 mAP: 0.0204
 Epoch:105 train steps: 5240 Loc Loss: 0.0802 Cls Loss: 0.0917 Tot Loss: 0.0859 mAP: 0.0203
 Epoch:105 train steps: 5250 Loc Loss: 0.0816 Cls Loss: 0.0926 Tot Loss: 0.0871 mAP: 0.0198
train
 Epoch:106 train steps: 5260 Loc Loss: 0.0811 Cls Loss: 0.0925 Tot Loss: 0.0868 mAP: 0.0201
 Epoch:106 train steps: 5270 Loc Loss: 0.0797 Cls Loss: 0.0909 Tot Loss: 0.0853 mAP: 0.0198
 Epoch:106 train steps: 5280 Loc Loss: 0.0802 Cls Loss: 0.0921 Tot Loss: 0.0861 mAP: 0.0196
 Epoch:106 train steps: 5290 Loc Loss: 0.0819 Cls Loss: 0.0931 Tot Loss: 0.0875 mAP: 0.0211
 Epoch:106 train steps: 5300 Loc Loss: 0.0823 Cls Loss: 0.0928 Tot Loss: 0.0875 mAP: 0.0214
train
 Epoch:107 train steps: 5310 Loc Loss: 0.0821 Cls Loss: 0.0936 Tot Loss: 0.0879 mAP: 0.0212
 Epoch:107 train steps: 5320 Loc Loss: 0.0793 Cls Loss: 0.0907 Tot Loss: 0.0850 mAP: 0.0194
 Epoch:107 train steps: 5330 Loc Loss: 0.0814 Cls Loss: 0.0931 Tot Loss: 0.0872 mAP: 0.0203
 Epoch:107 train steps: 5340 Loc Loss: 0.0815 Cls Loss: 0.0933 Tot Loss: 0.0874 mAP: 0.0193
 Epoch:107 train steps: 5350 Loc Loss: 0.0790 Cls Loss: 0.0902 Tot Loss: 0.0846 mAP: 0.0205
train
 Epoch:108 train steps: 5360 Loc Loss: 0.0794 Cls Loss: 0.0906 Tot Loss: 0.0850 mAP: 0.0201
 Epoch:108 train steps: 5370 Loc Loss: 0.0813 Cls Loss: 0.0918 Tot Loss: 0.0866 mAP: 0.0239
 Epoch:108 train steps: 5380 Loc Loss: 0.0812 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0207
 Epoch:108 train steps: 5390 Loc Loss: 0.0809 Cls Loss: 0.0922 Tot Loss: 0.0866 mAP: 0.0192
 Epoch:108 train steps: 5400 Loc Loss: 0.0815 Cls Loss: 0.0928 Tot Loss: 0.0872 mAP: 0.0198
train
 Epoch:109 train steps: 5410 Loc Loss: 0.0814 Cls Loss: 0.0927 Tot Loss: 0.0870 mAP: 0.0196
 Epoch:109 train steps: 5420 Loc Loss: 0.0815 Cls Loss: 0.0927 Tot Loss: 0.0871 mAP: 0.0215
 Epoch:109 train steps: 5430 Loc Loss: 0.0816 Cls Loss: 0.0925 Tot Loss: 0.0871 mAP: 0.0205
 Epoch:109 train steps: 5440 Loc Loss: 0.0800 Cls Loss: 0.0914 Tot Loss: 0.0857 mAP: 0.0204
 Epoch:109 train steps: 5450 Loc Loss: 0.0793 Cls Loss: 0.0905 Tot Loss: 0.0849 mAP: 0.0208
train
 Epoch:110 train steps: 5460 Loc Loss: 0.0787 Cls Loss: 0.0895 Tot Loss: 0.0841 mAP: 0.0220
 Epoch:110 train steps: 5470 Loc Loss: 0.0818 Cls Loss: 0.0936 Tot Loss: 0.0877 mAP: 0.0209
 Epoch:110 train steps: 5480 Loc Loss: 0.0804 Cls Loss: 0.0919 Tot Loss: 0.0861 mAP: 0.0200
 Epoch:110 train steps: 5490 Loc Loss: 0.0804 Cls Loss: 0.0925 Tot Loss: 0.0864 mAP: 0.0203
 Epoch:110 train steps: 5500 Loc Loss: 0.0812 Cls Loss: 0.0919 Tot Loss: 0.0865 mAP: 0.0198
train
 Epoch:111 train steps: 5510 Loc Loss: 0.0814 Cls Loss: 0.0926 Tot Loss: 0.0870 mAP: 0.0203
 Epoch:111 train steps: 5520 Loc Loss: 0.0802 Cls Loss: 0.0910 Tot Loss: 0.0856 mAP: 0.0196
 Epoch:111 train steps: 5530 Loc Loss: 0.0813 Cls Loss: 0.0931 Tot Loss: 0.0872 mAP: 0.0208
 Epoch:111 train steps: 5540 Loc Loss: 0.0808 Cls Loss: 0.0926 Tot Loss: 0.0867 mAP: 0.0197
 Epoch:111 train steps: 5550 Loc Loss: 0.0806 Cls Loss: 0.0922 Tot Loss: 0.0864 mAP: 0.0198
train
 Epoch:112 train steps: 5560 Loc Loss: 0.0793 Cls Loss: 0.0898 Tot Loss: 0.0846 mAP: 0.0197
 Epoch:112 train steps: 5570 Loc Loss: 0.0803 Cls Loss: 0.0923 Tot Loss: 0.0863 mAP: 0.0201
 Epoch:112 train steps: 5580 Loc Loss: 0.0795 Cls Loss: 0.0906 Tot Loss: 0.0851 mAP: 0.0208
 Epoch:112 train steps: 5590 Loc Loss: 0.0811 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0209
 Epoch:112 train steps: 5600 Loc Loss: 0.0819 Cls Loss: 0.0922 Tot Loss: 0.0871 mAP: 0.0204
train
 Epoch:113 train steps: 5610 Loc Loss: 0.0815 Cls Loss: 0.0933 Tot Loss: 0.0874 mAP: 0.0202
 Epoch:113 train steps: 5620 Loc Loss: 0.0811 Cls Loss: 0.0932 Tot Loss: 0.0872 mAP: 0.0197
 Epoch:113 train steps: 5630 Loc Loss: 0.0816 Cls Loss: 0.0929 Tot Loss: 0.0873 mAP: 0.0202
 Epoch:113 train steps: 5640 Loc Loss: 0.0811 Cls Loss: 0.0921 Tot Loss: 0.0866 mAP: 0.0195
 Epoch:113 train steps: 5650 Loc Loss: 0.0789 Cls Loss: 0.0902 Tot Loss: 0.0846 mAP: 0.0203
train
 Epoch:114 train steps: 5660 Loc Loss: 0.0806 Cls Loss: 0.0921 Tot Loss: 0.0864 mAP: 0.0200
 Epoch:114 train steps: 5670 Loc Loss: 0.0807 Cls Loss: 0.0922 Tot Loss: 0.0865 mAP: 0.0191
 Epoch:114 train steps: 5680 Loc Loss: 0.0812 Cls Loss: 0.0926 Tot Loss: 0.0869 mAP: 0.0205
 Epoch:114 train steps: 5690 Loc Loss: 0.0798 Cls Loss: 0.0913 Tot Loss: 0.0855 mAP: 0.0199
 Epoch:114 train steps: 5700 Loc Loss: 0.0806 Cls Loss: 0.0919 Tot Loss: 0.0862 mAP: 0.0210
train
 Epoch:115 train steps: 5710 Loc Loss: 0.0828 Cls Loss: 0.0945 Tot Loss: 0.0886 mAP: 0.0200
 Epoch:115 train steps: 5720 Loc Loss: 0.0794 Cls Loss: 0.0906 Tot Loss: 0.0850 mAP: 0.0199
 Epoch:115 train steps: 5730 Loc Loss: 0.0794 Cls Loss: 0.0902 Tot Loss: 0.0848 mAP: 0.0201
 Epoch:115 train steps: 5740 Loc Loss: 0.0820 Cls Loss: 0.0936 Tot Loss: 0.0878 mAP: 0.0202
 Epoch:115 train steps: 5750 Loc Loss: 0.0805 Cls Loss: 0.0918 Tot Loss: 0.0861 mAP: 0.0208
train
 Epoch:116 train steps: 5760 Loc Loss: 0.0801 Cls Loss: 0.0916 Tot Loss: 0.0858 mAP: 0.0199
 Epoch:116 train steps: 5770 Loc Loss: 0.0800 Cls Loss: 0.0914 Tot Loss: 0.0857 mAP: 0.0195
 Epoch:116 train steps: 5780 Loc Loss: 0.0822 Cls Loss: 0.0939 Tot Loss: 0.0880 mAP: 0.0205
 Epoch:116 train steps: 5790 Loc Loss: 0.0806 Cls Loss: 0.0916 Tot Loss: 0.0861 mAP: 0.0201
 Epoch:116 train steps: 5800 Loc Loss: 0.0806 Cls Loss: 0.0921 Tot Loss: 0.0864 mAP: 0.0192
train
 Epoch:117 train steps: 5810 Loc Loss: 0.0812 Cls Loss: 0.0933 Tot Loss: 0.0873 mAP: 0.0200
 Epoch:117 train steps: 5820 Loc Loss: 0.0805 Cls Loss: 0.0921 Tot Loss: 0.0863 mAP: 0.0201
 Epoch:117 train steps: 5830 Loc Loss: 0.0798 Cls Loss: 0.0914 Tot Loss: 0.0856 mAP: 0.0188
 Epoch:117 train steps: 5840 Loc Loss: 0.0804 Cls Loss: 0.0912 Tot Loss: 0.0858 mAP: 0.0201
 Epoch:117 train steps: 5850 Loc Loss: 0.0808 Cls Loss: 0.0918 Tot Loss: 0.0863 mAP: 0.0198
train
 Epoch:118 train steps: 5860 Loc Loss: 0.0794 Cls Loss: 0.0904 Tot Loss: 0.0849 mAP: 0.0193
 Epoch:118 train steps: 5870 Loc Loss: 0.0796 Cls Loss: 0.0914 Tot Loss: 0.0855 mAP: 0.0196
 Epoch:118 train steps: 5880 Loc Loss: 0.0813 Cls Loss: 0.0932 Tot Loss: 0.0873 mAP: 0.0194
 Epoch:118 train steps: 5890 Loc Loss: 0.0799 Cls Loss: 0.0913 Tot Loss: 0.0856 mAP: 0.0195
 Epoch:118 train steps: 5900 Loc Loss: 0.0813 Cls Loss: 0.0929 Tot Loss: 0.0871 mAP: 0.0209
train
 Epoch:119 train steps: 5910 Loc Loss: 0.0803 Cls Loss: 0.0917 Tot Loss: 0.0860 mAP: 0.0197
 Epoch:119 train steps: 5920 Loc Loss: 0.0796 Cls Loss: 0.0901 Tot Loss: 0.0849 mAP: 0.0191
 Epoch:119 train steps: 5930 Loc Loss: 0.0812 Cls Loss: 0.0922 Tot Loss: 0.0867 mAP: 0.0198
 Epoch:119 train steps: 5940 Loc Loss: 0.0801 Cls Loss: 0.0911 Tot Loss: 0.0856 mAP: 0.0197
 Epoch:119 train steps: 5950 Loc Loss: 0.0828 Cls Loss: 0.0938 Tot Loss: 0.0883 mAP: 0.0208
train
 Epoch:120 train steps: 5960 Loc Loss: 0.0809 Cls Loss: 0.0925 Tot Loss: 0.0867 mAP: 0.0206
 Epoch:120 train steps: 5970 Loc Loss: 0.0816 Cls Loss: 0.0933 Tot Loss: 0.0875 mAP: 0.0202
 Epoch:120 train steps: 5980 Loc Loss: 0.0813 Cls Loss: 0.0923 Tot Loss: 0.0868 mAP: 0.0195
 Epoch:120 train steps: 5990 Loc Loss: 0.0807 Cls Loss: 0.0920 Tot Loss: 0.0864 mAP: 0.0196
 Epoch:120 train steps: 6000 Loc Loss: 0.0803 Cls Loss: 0.0922 Tot Loss: 0.0862 mAP: 0.0197
val
 Epoch:120 val Loc Loss: 0.1071 Cls Loss: 0.1241 Tot Loss: 0.1156 mAP: 0.0280
Step 6000/64000.0
----------
train
 Epoch:121 train steps: 6010 Loc Loss: 0.0811 Cls Loss: 0.0927 Tot Loss: 0.0869 mAP: 0.0205
 Epoch:121 train steps: 6020 Loc Loss: 0.0807 Cls Loss: 0.0919 Tot Loss: 0.0863 mAP: 0.0201
 Epoch:121 train steps: 6030 Loc Loss: 0.0813 Cls Loss: 0.0931 Tot Loss: 0.0872 mAP: 0.0202
 Epoch:121 train steps: 6040 Loc Loss: 0.0801 Cls Loss: 0.0912 Tot Loss: 0.0857 mAP: 0.0193
 Epoch:121 train steps: 6050 Loc Loss: 0.0805 Cls Loss: 0.0911 Tot Loss: 0.0858 mAP: 0.0206
train
 Epoch:122 train steps: 6060 Loc Loss: 0.0803 Cls Loss: 0.0918 Tot Loss: 0.0860 mAP: 0.0201
 Epoch:122 train steps: 6070 Loc Loss: 0.0816 Cls Loss: 0.0930 Tot Loss: 0.0873 mAP: 0.0207
 Epoch:122 train steps: 6080 Loc Loss: 0.0821 Cls Loss: 0.0937 Tot Loss: 0.0879 mAP: 0.0197
 Epoch:122 train steps: 6090 Loc Loss: 0.0795 Cls Loss: 0.0909 Tot Loss: 0.0852 mAP: 0.0220
 Epoch:122 train steps: 6100 Loc Loss: 0.0803 Cls Loss: 0.0913 Tot Loss: 0.0858 mAP: 0.0202
train
 Epoch:123 train steps: 6110 Loc Loss: 0.0809 Cls Loss: 0.0926 Tot Loss: 0.0868 mAP: 0.0197
 Epoch:123 train steps: 6120 Loc Loss: 0.0810 Cls Loss: 0.0924 Tot Loss: 0.0867 mAP: 0.0202
 Epoch:123 train steps: 6130 Loc Loss: 0.0811 Cls Loss: 0.0919 Tot Loss: 0.0865 mAP: 0.0212
 Epoch:123 train steps: 6140 Loc Loss: 0.0808 Cls Loss: 0.0925 Tot Loss: 0.0866 mAP: 0.0206
 Epoch:123 train steps: 6150 Loc Loss: 0.0795 Cls Loss: 0.0905 Tot Loss: 0.0850 mAP: 0.0197
train
 Epoch:124 train steps: 6160 Loc Loss: 0.0810 Cls Loss: 0.0920 Tot Loss: 0.0865 mAP: 0.0209
 Epoch:124 train steps: 6170 Loc Loss: 0.0789 Cls Loss: 0.0899 Tot Loss: 0.0844 mAP: 0.0212
 Epoch:124 train steps: 6180 Loc Loss: 0.0826 Cls Loss: 0.0936 Tot Loss: 0.0881 mAP: 0.0206
 Epoch:124 train steps: 6190 Loc Loss: 0.0813 Cls Loss: 0.0927 Tot Loss: 0.0870 mAP: 0.0209
 Epoch:124 train steps: 6200 Loc Loss: 0.0796 Cls Loss: 0.0898 Tot Loss: 0.0847 mAP: 0.0199
train
 Epoch:125 train steps: 6210 Loc Loss: 0.0798 Cls Loss: 0.0904 Tot Loss: 0.0851 mAP: 0.0193
 Epoch:125 train steps: 6220 Loc Loss: 0.0808 Cls Loss: 0.0921 Tot Loss: 0.0865 mAP: 0.0201
 Epoch:125 train steps: 6230 Loc Loss: 0.0816 Cls Loss: 0.0929 Tot Loss: 0.0872 mAP: 0.0198
 Epoch:125 train steps: 6240 Loc Loss: 0.0816 Cls Loss: 0.0928 Tot Loss: 0.0872 mAP: 0.0213
 Epoch:125 train steps: 6250 Loc Loss: 0.0817 Cls Loss: 0.0936 Tot Loss: 0.0876 mAP: 0.0198
train
 Epoch:126 train steps: 6260 Loc Loss: 0.0814 Cls Loss: 0.0931 Tot Loss: 0.0873 mAP: 0.0201
 Epoch:126 train steps: 6270 Loc Loss: 0.0810 Cls Loss: 0.0922 Tot Loss: 0.0866 mAP: 0.0223
 Epoch:126 train steps: 6280 Loc Loss: 0.0796 Cls Loss: 0.0907 Tot Loss: 0.0851 mAP: 0.0204
 Epoch:126 train steps: 6290 Loc Loss: 0.0814 Cls Loss: 0.0927 Tot Loss: 0.0870 mAP: 0.0211
 Epoch:126 train steps: 6300 Loc Loss: 0.0811 Cls Loss: 0.0925 Tot Loss: 0.0868 mAP: 0.0201
train
 Epoch:127 train steps: 6310 Loc Loss: 0.0811 Cls Loss: 0.0927 Tot Loss: 0.0869 mAP: 0.0197
 Epoch:127 train steps: 6320 Loc Loss: 0.0821 Cls Loss: 0.0935 Tot Loss: 0.0878 mAP: 0.0213
 Epoch:127 train steps: 6330 Loc Loss: 0.0808 Cls Loss: 0.0914 Tot Loss: 0.0861 mAP: 0.0203
 Epoch:127 train steps: 6340 Loc Loss: 0.0802 Cls Loss: 0.0912 Tot Loss: 0.0857 mAP: 0.0199
 Epoch:127 train steps: 6350 Loc Loss: 0.0785 Cls Loss: 0.0899 Tot Loss: 0.0842 mAP: 0.0215
train
 Epoch:128 train steps: 6360 Loc Loss: 0.0797 Cls Loss: 0.0911 Tot Loss: 0.0854 mAP: 0.0195
 Epoch:128 train steps: 6370 Loc Loss: 0.0804 Cls Loss: 0.0913 Tot Loss: 0.0859 mAP: 0.0197
 Epoch:128 train steps: 6380 Loc Loss: 0.0827 Cls Loss: 0.0946 Tot Loss: 0.0886 mAP: 0.0200
 Epoch:128 train steps: 6390 Loc Loss: 0.0814 Cls Loss: 0.0930 Tot Loss: 0.0872 mAP: 0.0197
 Epoch:128 train steps: 6400 Loc Loss: 0.0807 Cls Loss: 0.0919 Tot Loss: 0.0863 mAP: 0.0205
train
 Epoch:129 train steps: 6410 Loc Loss: 0.0805 Cls Loss: 0.0909 Tot Loss: 0.0857 mAP: 0.0201
 Epoch:129 train steps: 6420 Loc Loss: 0.0812 Cls Loss: 0.0927 Tot Loss: 0.0869 mAP: 0.0203
 Epoch:129 train steps: 6430 Loc Loss: 0.0809 Cls Loss: 0.0923 Tot Loss: 0.0866 mAP: 0.0205
 Epoch:129 train steps: 6440 Loc Loss: 0.0800 Cls Loss: 0.0908 Tot Loss: 0.0854 mAP: 0.0200
 Epoch:129 train steps: 6450 Loc Loss: 0.0804 Cls Loss: 0.0921 Tot Loss: 0.0862 mAP: 0.0215
train
 Epoch:130 train steps: 6460 Loc Loss: 0.0812 Cls Loss: 0.0923 Tot Loss: 0.0868 mAP: 0.0200
 Epoch:130 train steps: 6470 Loc Loss: 0.0808 Cls Loss: 0.0922 Tot Loss: 0.0865 mAP: 0.0200
 Epoch:130 train steps: 6480 Loc Loss: 0.0825 Cls Loss: 0.0937 Tot Loss: 0.0881 mAP: 0.0199
 Epoch:130 train steps: 6490 Loc Loss: 0.0795 Cls Loss: 0.0904 Tot Loss: 0.0849 mAP: 0.0198
 Epoch:130 train steps: 6500 Loc Loss: 0.0804 Cls Loss: 0.0918 Tot Loss: 0.0861 mAP: 0.0210
train
 Epoch:131 train steps: 6510 Loc Loss: 0.0800 Cls Loss: 0.0908 Tot Loss: 0.0854 mAP: 0.0218
 Epoch:131 train steps: 6520 Loc Loss: 0.0809 Cls Loss: 0.0927 Tot Loss: 0.0868 mAP: 0.0210
 Epoch:131 train steps: 6530 Loc Loss: 0.0813 Cls Loss: 0.0934 Tot Loss: 0.0873 mAP: 0.0202
 Epoch:131 train steps: 6540 Loc Loss: 0.0804 Cls Loss: 0.0917 Tot Loss: 0.0860 mAP: 0.0202
 Epoch:131 train steps: 6550 Loc Loss: 0.0812 Cls Loss: 0.0923 Tot Loss: 0.0868 mAP: 0.0202
train
 Epoch:132 train steps: 6560 Loc Loss: 0.0797 Cls Loss: 0.0906 Tot Loss: 0.0851 mAP: 0.0205
 Epoch:132 train steps: 6570 Loc Loss: 0.0801 Cls Loss: 0.0917 Tot Loss: 0.0859 mAP: 0.0205
 Epoch:132 train steps: 6580 Loc Loss: 0.0803 Cls Loss: 0.0914 Tot Loss: 0.0859 mAP: 0.0199
 Epoch:132 train steps: 6590 Loc Loss: 0.0824 Cls Loss: 0.0933 Tot Loss: 0.0878 mAP: 0.0202
 Epoch:132 train steps: 6600 Loc Loss: 0.0804 Cls Loss: 0.0915 Tot Loss: 0.0859 mAP: 0.0205
train
 Epoch:133 train steps: 6610 Loc Loss: 0.0807 Cls Loss: 0.0921 Tot Loss: 0.0864 mAP: 0.0193
 Epoch:133 train steps: 6620 Loc Loss: 0.0803 Cls Loss: 0.0918 Tot Loss: 0.0861 mAP: 0.0199
 Epoch:133 train steps: 6630 Loc Loss: 0.0812 Cls Loss: 0.0919 Tot Loss: 0.0866 mAP: 0.0205
 Epoch:133 train steps: 6640 Loc Loss: 0.0816 Cls Loss: 0.0930 Tot Loss: 0.0873 mAP: 0.0198
 Epoch:133 train steps: 6650 Loc Loss: 0.0801 Cls Loss: 0.0913 Tot Loss: 0.0857 mAP: 0.0201
train
 Epoch:134 train steps: 6660 Loc Loss: 0.0815 Cls Loss: 0.0926 Tot Loss: 0.0870 mAP: 0.0193
 Epoch:134 train steps: 6670 Loc Loss: 0.0788 Cls Loss: 0.0901 Tot Loss: 0.0845 mAP: 0.0202
 Epoch:134 train steps: 6680 Loc Loss: 0.0807 Cls Loss: 0.0915 Tot Loss: 0.0861 mAP: 0.0198
 Epoch:134 train steps: 6690 Loc Loss: 0.0819 Cls Loss: 0.0939 Tot Loss: 0.0879 mAP: 0.0210
 Epoch:134 train steps: 6700 Loc Loss: 0.0814 Cls Loss: 0.0929 Tot Loss: 0.0871 mAP: 0.0209
train
 Epoch:135 train steps: 6710 Loc Loss: 0.0821 Cls Loss: 0.0933 Tot Loss: 0.0877 mAP: 0.0207
 Epoch:135 train steps: 6720 Loc Loss: 0.0822 Cls Loss: 0.0938 Tot Loss: 0.0880 mAP: 0.0213
 Epoch:135 train steps: 6730 Loc Loss: 0.0794 Cls Loss: 0.0904 Tot Loss: 0.0849 mAP: 0.0206
 Epoch:135 train steps: 6740 Loc Loss: 0.0796 Cls Loss: 0.0900 Tot Loss: 0.0848 mAP: 0.0192
 Epoch:135 train steps: 6750 Loc Loss: 0.0816 Cls Loss: 0.0929 Tot Loss: 0.0873 mAP: 0.0199
train
 Epoch:136 train steps: 6760 Loc Loss: 0.0798 Cls Loss: 0.0903 Tot Loss: 0.0851 mAP: 0.0201
 Epoch:136 train steps: 6770 Loc Loss: 0.0808 Cls Loss: 0.0920 Tot Loss: 0.0864 mAP: 0.0204
 Epoch:136 train steps: 6780 Loc Loss: 0.0819 Cls Loss: 0.0937 Tot Loss: 0.0878 mAP: 0.0197
 Epoch:136 train steps: 6790 Loc Loss: 0.0803 Cls Loss: 0.0914 Tot Loss: 0.0858 mAP: 0.0203
 Epoch:136 train steps: 6800 Loc Loss: 0.0817 Cls Loss: 0.0932 Tot Loss: 0.0874 mAP: 0.0201
train
 Epoch:137 train steps: 6810 Loc Loss: 0.0807 Cls Loss: 0.0921 Tot Loss: 0.0864 mAP: 0.0208
 Epoch:137 train steps: 6820 Loc Loss: 0.0809 Cls Loss: 0.0920 Tot Loss: 0.0865 mAP: 0.0207
 Epoch:137 train steps: 6830 Loc Loss: 0.0798 Cls Loss: 0.0909 Tot Loss: 0.0854 mAP: 0.0197
 Epoch:137 train steps: 6840 Loc Loss: 0.0810 Cls Loss: 0.0917 Tot Loss: 0.0863 mAP: 0.0203
 Epoch:137 train steps: 6850 Loc Loss: 0.0801 Cls Loss: 0.0917 Tot Loss: 0.0859 mAP: 0.0199
train
 Epoch:138 train steps: 6860 Loc Loss: 0.0817 Cls Loss: 0.0932 Tot Loss: 0.0875 mAP: 0.0191
 Epoch:138 train steps: 6870 Loc Loss: 0.0800 Cls Loss: 0.0924 Tot Loss: 0.0862 mAP: 0.0200
 Epoch:138 train steps: 6880 Loc Loss: 0.0796 Cls Loss: 0.0906 Tot Loss: 0.0851 mAP: 0.0199
 Epoch:138 train steps: 6890 Loc Loss: 0.0802 Cls Loss: 0.0913 Tot Loss: 0.0858 mAP: 0.0201
 Epoch:138 train steps: 6900 Loc Loss: 0.0825 Cls Loss: 0.0940 Tot Loss: 0.0882 mAP: 0.0200
train
 Epoch:139 train steps: 6910 Loc Loss: 0.0810 Cls Loss: 0.0918 Tot Loss: 0.0864 mAP: 0.0198
 Epoch:139 train steps: 6920 Loc Loss: 0.0813 Cls Loss: 0.0939 Tot Loss: 0.0876 mAP: 0.0199
 Epoch:139 train steps: 6930 Loc Loss: 0.0818 Cls Loss: 0.0933 Tot Loss: 0.0876 mAP: 0.0201
 Epoch:139 train steps: 6940 Loc Loss: 0.0820 Cls Loss: 0.0939 Tot Loss: 0.0880 mAP: 0.0197
 Epoch:139 train steps: 6950 Loc Loss: 0.0797 Cls Loss: 0.0907 Tot Loss: 0.0852 mAP: 0.0195
train
 Epoch:140 train steps: 6960 Loc Loss: 0.0818 Cls Loss: 0.0933 Tot Loss: 0.0876 mAP: 0.0202
 Epoch:140 train steps: 6970 Loc Loss: 0.0800 Cls Loss: 0.0915 Tot Loss: 0.0857 mAP: 0.0210
 Epoch:140 train steps: 6980 Loc Loss: 0.0803 Cls Loss: 0.0924 Tot Loss: 0.0864 mAP: 0.0201
 Epoch:140 train steps: 6990 Loc Loss: 0.0806 Cls Loss: 0.0914 Tot Loss: 0.0860 mAP: 0.0210
 Epoch:140 train steps: 7000 Loc Loss: 0.0813 Cls Loss: 0.0920 Tot Loss: 0.0867 mAP: 0.0220
val
 Epoch:140 val Loc Loss: 0.1083 Cls Loss: 0.1258 Tot Loss: 0.1170 mAP: 0.0276
Step 7000/64000.0
----------
train
 Epoch:141 train steps: 7010 Loc Loss: 0.0814 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0208
 Epoch:141 train steps: 7020 Loc Loss: 0.0793 Cls Loss: 0.0904 Tot Loss: 0.0849 mAP: 0.0206
 Epoch:141 train steps: 7030 Loc Loss: 0.0796 Cls Loss: 0.0910 Tot Loss: 0.0853 mAP: 0.0204
 Epoch:141 train steps: 7040 Loc Loss: 0.0817 Cls Loss: 0.0932 Tot Loss: 0.0874 mAP: 0.0209
 Epoch:141 train steps: 7050 Loc Loss: 0.0814 Cls Loss: 0.0930 Tot Loss: 0.0872 mAP: 0.0203
train
 Epoch:142 train steps: 7060 Loc Loss: 0.0811 Cls Loss: 0.0926 Tot Loss: 0.0869 mAP: 0.0202
 Epoch:142 train steps: 7070 Loc Loss: 0.0811 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0197
 Epoch:142 train steps: 7080 Loc Loss: 0.0784 Cls Loss: 0.0898 Tot Loss: 0.0841 mAP: 0.0197
 Epoch:142 train steps: 7090 Loc Loss: 0.0805 Cls Loss: 0.0918 Tot Loss: 0.0862 mAP: 0.0206
 Epoch:142 train steps: 7100 Loc Loss: 0.0802 Cls Loss: 0.0918 Tot Loss: 0.0860 mAP: 0.0201
train
 Epoch:143 train steps: 7110 Loc Loss: 0.0805 Cls Loss: 0.0917 Tot Loss: 0.0861 mAP: 0.0204
 Epoch:143 train steps: 7120 Loc Loss: 0.0811 Cls Loss: 0.0924 Tot Loss: 0.0867 mAP: 0.0216
 Epoch:143 train steps: 7130 Loc Loss: 0.0802 Cls Loss: 0.0912 Tot Loss: 0.0857 mAP: 0.0211
 Epoch:143 train steps: 7140 Loc Loss: 0.0812 Cls Loss: 0.0924 Tot Loss: 0.0868 mAP: 0.0201
 Epoch:143 train steps: 7150 Loc Loss: 0.0793 Cls Loss: 0.0904 Tot Loss: 0.0848 mAP: 0.0191
train
 Epoch:144 train steps: 7160 Loc Loss: 0.0810 Cls Loss: 0.0924 Tot Loss: 0.0867 mAP: 0.0207
 Epoch:144 train steps: 7170 Loc Loss: 0.0797 Cls Loss: 0.0906 Tot Loss: 0.0852 mAP: 0.0196
 Epoch:144 train steps: 7180 Loc Loss: 0.0803 Cls Loss: 0.0910 Tot Loss: 0.0857 mAP: 0.0201
 Epoch:144 train steps: 7190 Loc Loss: 0.0814 Cls Loss: 0.0927 Tot Loss: 0.0870 mAP: 0.0196
 Epoch:144 train steps: 7200 Loc Loss: 0.0821 Cls Loss: 0.0934 Tot Loss: 0.0877 mAP: 0.0222
train
 Epoch:145 train steps: 7210 Loc Loss: 0.0799 Cls Loss: 0.0908 Tot Loss: 0.0853 mAP: 0.0205
 Epoch:145 train steps: 7220 Loc Loss: 0.0817 Cls Loss: 0.0934 Tot Loss: 0.0876 mAP: 0.0202
 Epoch:145 train steps: 7230 Loc Loss: 0.0802 Cls Loss: 0.0912 Tot Loss: 0.0857 mAP: 0.0190
 Epoch:145 train steps: 7240 Loc Loss: 0.0826 Cls Loss: 0.0941 Tot Loss: 0.0884 mAP: 0.0217
 Epoch:145 train steps: 7250 Loc Loss: 0.0819 Cls Loss: 0.0926 Tot Loss: 0.0873 mAP: 0.0221
train
 Epoch:146 train steps: 7260 Loc Loss: 0.0831 Cls Loss: 0.0946 Tot Loss: 0.0888 mAP: 0.0202
 Epoch:146 train steps: 7270 Loc Loss: 0.0792 Cls Loss: 0.0907 Tot Loss: 0.0850 mAP: 0.0214
 Epoch:146 train steps: 7280 Loc Loss: 0.0809 Cls Loss: 0.0918 Tot Loss: 0.0864 mAP: 0.0202
 Epoch:146 train steps: 7290 Loc Loss: 0.0816 Cls Loss: 0.0923 Tot Loss: 0.0869 mAP: 0.0201
 Epoch:146 train steps: 7300 Loc Loss: 0.0807 Cls Loss: 0.0923 Tot Loss: 0.0865 mAP: 0.0203
train
 Epoch:147 train steps: 7310 Loc Loss: 0.0829 Cls Loss: 0.0947 Tot Loss: 0.0888 mAP: 0.0208
 Epoch:147 train steps: 7320 Loc Loss: 0.0813 Cls Loss: 0.0925 Tot Loss: 0.0869 mAP: 0.0201
 Epoch:147 train steps: 7330 Loc Loss: 0.0792 Cls Loss: 0.0902 Tot Loss: 0.0847 mAP: 0.0245
 Epoch:147 train steps: 7340 Loc Loss: 0.0800 Cls Loss: 0.0917 Tot Loss: 0.0859 mAP: 0.0206
 Epoch:147 train steps: 7350 Loc Loss: 0.0805 Cls Loss: 0.0918 Tot Loss: 0.0861 mAP: 0.0204
train
 Epoch:148 train steps: 7360 Loc Loss: 0.0814 Cls Loss: 0.0930 Tot Loss: 0.0872 mAP: 0.0212
 Epoch:148 train steps: 7370 Loc Loss: 0.0816 Cls Loss: 0.0935 Tot Loss: 0.0875 mAP: 0.0222
 Epoch:148 train steps: 7380 Loc Loss: 0.0800 Cls Loss: 0.0907 Tot Loss: 0.0853 mAP: 0.0195
 Epoch:148 train steps: 7390 Loc Loss: 0.0803 Cls Loss: 0.0910 Tot Loss: 0.0856 mAP: 0.0226
 Epoch:148 train steps: 7400 Loc Loss: 0.0790 Cls Loss: 0.0899 Tot Loss: 0.0844 mAP: 0.0193
train
 Epoch:149 train steps: 7410 Loc Loss: 0.0812 Cls Loss: 0.0929 Tot Loss: 0.0871 mAP: 0.0209
 Epoch:149 train steps: 7420 Loc Loss: 0.0799 Cls Loss: 0.0919 Tot Loss: 0.0859 mAP: 0.0200
 Epoch:149 train steps: 7430 Loc Loss: 0.0817 Cls Loss: 0.0934 Tot Loss: 0.0875 mAP: 0.0206
 Epoch:149 train steps: 7440 Loc Loss: 0.0805 Cls Loss: 0.0920 Tot Loss: 0.0862 mAP: 0.0201
 Epoch:149 train steps: 7450 Loc Loss: 0.0810 Cls Loss: 0.0920 Tot Loss: 0.0865 mAP: 0.0196
train
 Epoch:150 train steps: 7460 Loc Loss: 0.0817 Cls Loss: 0.0930 Tot Loss: 0.0873 mAP: 0.0203
 Epoch:150 train steps: 7470 Loc Loss: 0.0804 Cls Loss: 0.0917 Tot Loss: 0.0861 mAP: 0.0200
 Epoch:150 train steps: 7480 Loc Loss: 0.0803 Cls Loss: 0.0916 Tot Loss: 0.0860 mAP: 0.0194
 Epoch:150 train steps: 7490 Loc Loss: 0.0794 Cls Loss: 0.0915 Tot Loss: 0.0855 mAP: 0.0208
 Epoch:150 train steps: 7500 Loc Loss: 0.0806 Cls Loss: 0.0921 Tot Loss: 0.0864 mAP: 0.0226
train
 Epoch:151 train steps: 7510 Loc Loss: 0.0821 Cls Loss: 0.0934 Tot Loss: 0.0878 mAP: 0.0201
 Epoch:151 train steps: 7520 Loc Loss: 0.0797 Cls Loss: 0.0911 Tot Loss: 0.0854 mAP: 0.0195
 Epoch:151 train steps: 7530 Loc Loss: 0.0806 Cls Loss: 0.0918 Tot Loss: 0.0862 mAP: 0.0205
 Epoch:151 train steps: 7540 Loc Loss: 0.0787 Cls Loss: 0.0893 Tot Loss: 0.0840 mAP: 0.0196
 Epoch:151 train steps: 7550 Loc Loss: 0.0807 Cls Loss: 0.0914 Tot Loss: 0.0861 mAP: 0.0203
train
 Epoch:152 train steps: 7560 Loc Loss: 0.0817 Cls Loss: 0.0930 Tot Loss: 0.0873 mAP: 0.0205
 Epoch:152 train steps: 7570 Loc Loss: 0.0816 Cls Loss: 0.0924 Tot Loss: 0.0870 mAP: 0.0205
 Epoch:152 train steps: 7580 Loc Loss: 0.0796 Cls Loss: 0.0904 Tot Loss: 0.0850 mAP: 0.0204
 Epoch:152 train steps: 7590 Loc Loss: 0.0816 Cls Loss: 0.0938 Tot Loss: 0.0877 mAP: 0.0200
 Epoch:152 train steps: 7600 Loc Loss: 0.0798 Cls Loss: 0.0910 Tot Loss: 0.0854 mAP: 0.0204
train
 Epoch:153 train steps: 7610 Loc Loss: 0.0806 Cls Loss: 0.0911 Tot Loss: 0.0859 mAP: 0.0211
 Epoch:153 train steps: 7620 Loc Loss: 0.0797 Cls Loss: 0.0907 Tot Loss: 0.0852 mAP: 0.0198
 Epoch:153 train steps: 7630 Loc Loss: 0.0808 Cls Loss: 0.0915 Tot Loss: 0.0861 mAP: 0.0204
 Epoch:153 train steps: 7640 Loc Loss: 0.0815 Cls Loss: 0.0927 Tot Loss: 0.0871 mAP: 0.0205
 Epoch:153 train steps: 7650 Loc Loss: 0.0809 Cls Loss: 0.0933 Tot Loss: 0.0871 mAP: 0.0190
train
 Epoch:154 train steps: 7660 Loc Loss: 0.0802 Cls Loss: 0.0922 Tot Loss: 0.0862 mAP: 0.0199
 Epoch:154 train steps: 7670 Loc Loss: 0.0800 Cls Loss: 0.0907 Tot Loss: 0.0853 mAP: 0.0188
 Epoch:154 train steps: 7680 Loc Loss: 0.0805 Cls Loss: 0.0916 Tot Loss: 0.0860 mAP: 0.0201
 Epoch:154 train steps: 7690 Loc Loss: 0.0804 Cls Loss: 0.0909 Tot Loss: 0.0857 mAP: 0.0205
 Epoch:154 train steps: 7700 Loc Loss: 0.0808 Cls Loss: 0.0922 Tot Loss: 0.0865 mAP: 0.0208
train
 Epoch:155 train steps: 7710 Loc Loss: 0.0829 Cls Loss: 0.0943 Tot Loss: 0.0886 mAP: 0.0204
 Epoch:155 train steps: 7720 Loc Loss: 0.0795 Cls Loss: 0.0911 Tot Loss: 0.0853 mAP: 0.0205
 Epoch:155 train steps: 7730 Loc Loss: 0.0804 Cls Loss: 0.0917 Tot Loss: 0.0860 mAP: 0.0209
 Epoch:155 train steps: 7740 Loc Loss: 0.0796 Cls Loss: 0.0904 Tot Loss: 0.0850 mAP: 0.0203
 Epoch:155 train steps: 7750 Loc Loss: 0.0804 Cls Loss: 0.0919 Tot Loss: 0.0861 mAP: 0.0204
train
 Epoch:156 train steps: 7760 Loc Loss: 0.0821 Cls Loss: 0.0936 Tot Loss: 0.0879 mAP: 0.0201
 Epoch:156 train steps: 7770 Loc Loss: 0.0820 Cls Loss: 0.0935 Tot Loss: 0.0878 mAP: 0.0203
 Epoch:156 train steps: 7780 Loc Loss: 0.0793 Cls Loss: 0.0907 Tot Loss: 0.0850 mAP: 0.0198
 Epoch:156 train steps: 7790 Loc Loss: 0.0818 Cls Loss: 0.0937 Tot Loss: 0.0877 mAP: 0.0198
 Epoch:156 train steps: 7800 Loc Loss: 0.0807 Cls Loss: 0.0927 Tot Loss: 0.0867 mAP: 0.0196
train
 Epoch:157 train steps: 7810 Loc Loss: 0.0821 Cls Loss: 0.0940 Tot Loss: 0.0881 mAP: 0.0202
 Epoch:157 train steps: 7820 Loc Loss: 0.0814 Cls Loss: 0.0929 Tot Loss: 0.0871 mAP: 0.0199
 Epoch:157 train steps: 7830 Loc Loss: 0.0817 Cls Loss: 0.0926 Tot Loss: 0.0871 mAP: 0.0203
 Epoch:157 train steps: 7840 Loc Loss: 0.0799 Cls Loss: 0.0917 Tot Loss: 0.0858 mAP: 0.0197
 Epoch:157 train steps: 7850 Loc Loss: 0.0807 Cls Loss: 0.0925 Tot Loss: 0.0866 mAP: 0.0193
train
 Epoch:158 train steps: 7860 Loc Loss: 0.0813 Cls Loss: 0.0922 Tot Loss: 0.0867 mAP: 0.0202
 Epoch:158 train steps: 7870 Loc Loss: 0.0814 Cls Loss: 0.0921 Tot Loss: 0.0868 mAP: 0.0206
 Epoch:158 train steps: 7880 Loc Loss: 0.0809 Cls Loss: 0.0921 Tot Loss: 0.0865 mAP: 0.0197
 Epoch:158 train steps: 7890 Loc Loss: 0.0805 Cls Loss: 0.0920 Tot Loss: 0.0863 mAP: 0.0204
 Epoch:158 train steps: 7900 Loc Loss: 0.0806 Cls Loss: 0.0915 Tot Loss: 0.0861 mAP: 0.0205
train
 Epoch:159 train steps: 7910 Loc Loss: 0.0816 Cls Loss: 0.0931 Tot Loss: 0.0873 mAP: 0.0195
 Epoch:159 train steps: 7920 Loc Loss: 0.0799 Cls Loss: 0.0909 Tot Loss: 0.0854 mAP: 0.0193
 Epoch:159 train steps: 7930 Loc Loss: 0.0807 Cls Loss: 0.0923 Tot Loss: 0.0865 mAP: 0.0204
 Epoch:159 train steps: 7940 Loc Loss: 0.0820 Cls Loss: 0.0941 Tot Loss: 0.0881 mAP: 0.0203
 Epoch:159 train steps: 7950 Loc Loss: 0.0803 Cls Loss: 0.0917 Tot Loss: 0.0860 mAP: 0.0202
train
 Epoch:160 train steps: 7960 Loc Loss: 0.0821 Cls Loss: 0.0935 Tot Loss: 0.0878 mAP: 0.0206
 Epoch:160 train steps: 7970 Loc Loss: 0.0805 Cls Loss: 0.0915 Tot Loss: 0.0860 mAP: 0.0201
 Epoch:160 train steps: 7980 Loc Loss: 0.0801 Cls Loss: 0.0911 Tot Loss: 0.0856 mAP: 0.0215
 Epoch:160 train steps: 7990 Loc Loss: 0.0802 Cls Loss: 0.0913 Tot Loss: 0.0857 mAP: 0.0207
 Epoch:160 train steps: 8000 Loc Loss: 0.0803 Cls Loss: 0.0920 Tot Loss: 0.0862 mAP: 0.0197
val
 Epoch:160 val Loc Loss: 0.1078 Cls Loss: 0.1250 Tot Loss: 0.1164 mAP: 0.0274
Step 8000/64000.0
----------
train
 Epoch:161 train steps: 8010 Loc Loss: 0.0808 Cls Loss: 0.0922 Tot Loss: 0.0865 mAP: 0.0199
 Epoch:161 train steps: 8020 Loc Loss: 0.0802 Cls Loss: 0.0906 Tot Loss: 0.0854 mAP: 0.0206
 Epoch:161 train steps: 8030 Loc Loss: 0.0801 Cls Loss: 0.0913 Tot Loss: 0.0857 mAP: 0.0224
 Epoch:161 train steps: 8040 Loc Loss: 0.0803 Cls Loss: 0.0918 Tot Loss: 0.0860 mAP: 0.0211
 Epoch:161 train steps: 8050 Loc Loss: 0.0819 Cls Loss: 0.0944 Tot Loss: 0.0882 mAP: 0.0211
train
 Epoch:162 train steps: 8060 Loc Loss: 0.0813 Cls Loss: 0.0929 Tot Loss: 0.0871 mAP: 0.0197
 Epoch:162 train steps: 8070 Loc Loss: 0.0800 Cls Loss: 0.0922 Tot Loss: 0.0861 mAP: 0.0203
 Epoch:162 train steps: 8080 Loc Loss: 0.0785 Cls Loss: 0.0894 Tot Loss: 0.0839 mAP: 0.0189
 Epoch:162 train steps: 8090 Loc Loss: 0.0834 Cls Loss: 0.0955 Tot Loss: 0.0895 mAP: 0.0207
 Epoch:162 train steps: 8100 Loc Loss: 0.0810 Cls Loss: 0.0922 Tot Loss: 0.0866 mAP: 0.0197
train
 Epoch:163 train steps: 8110 Loc Loss: 0.0804 Cls Loss: 0.0923 Tot Loss: 0.0864 mAP: 0.0210
 Epoch:163 train steps: 8120 Loc Loss: 0.0816 Cls Loss: 0.0933 Tot Loss: 0.0875 mAP: 0.0203
 Epoch:163 train steps: 8130 Loc Loss: 0.0805 Cls Loss: 0.0922 Tot Loss: 0.0864 mAP: 0.0209
 Epoch:163 train steps: 8140 Loc Loss: 0.0796 Cls Loss: 0.0909 Tot Loss: 0.0852 mAP: 0.0196
 Epoch:163 train steps: 8150 Loc Loss: 0.0804 Cls Loss: 0.0917 Tot Loss: 0.0861 mAP: 0.0210
train
 Epoch:164 train steps: 8160 Loc Loss: 0.0814 Cls Loss: 0.0930 Tot Loss: 0.0872 mAP: 0.0210
 Epoch:164 train steps: 8170 Loc Loss: 0.0806 Cls Loss: 0.0916 Tot Loss: 0.0861 mAP: 0.0205
 Epoch:164 train steps: 8180 Loc Loss: 0.0816 Cls Loss: 0.0929 Tot Loss: 0.0872 mAP: 0.0200
 Epoch:164 train steps: 8190 Loc Loss: 0.0802 Cls Loss: 0.0917 Tot Loss: 0.0859 mAP: 0.0215
 Epoch:164 train steps: 8200 Loc Loss: 0.0796 Cls Loss: 0.0903 Tot Loss: 0.0850 mAP: 0.0188
train
 Epoch:165 train steps: 8210 Loc Loss: 0.0804 Cls Loss: 0.0917 Tot Loss: 0.0860 mAP: 0.0224
 Epoch:165 train steps: 8220 Loc Loss: 0.0809 Cls Loss: 0.0924 Tot Loss: 0.0867 mAP: 0.0197
 Epoch:165 train steps: 8230 Loc Loss: 0.0809 Cls Loss: 0.0931 Tot Loss: 0.0870 mAP: 0.0204
 Epoch:165 train steps: 8240 Loc Loss: 0.0804 Cls Loss: 0.0915 Tot Loss: 0.0859 mAP: 0.0204
 Epoch:165 train steps: 8250 Loc Loss: 0.0799 Cls Loss: 0.0917 Tot Loss: 0.0858 mAP: 0.0202
train
 Epoch:166 train steps: 8260 Loc Loss: 0.0802 Cls Loss: 0.0918 Tot Loss: 0.0860 mAP: 0.0199
 Epoch:166 train steps: 8270 Loc Loss: 0.0807 Cls Loss: 0.0911 Tot Loss: 0.0859 mAP: 0.0210
 Epoch:166 train steps: 8280 Loc Loss: 0.0817 Cls Loss: 0.0934 Tot Loss: 0.0876 mAP: 0.0200
 Epoch:166 train steps: 8290 Loc Loss: 0.0807 Cls Loss: 0.0925 Tot Loss: 0.0866 mAP: 0.0198
 Epoch:166 train steps: 8300 Loc Loss: 0.0784 Cls Loss: 0.0901 Tot Loss: 0.0842 mAP: 0.0193
train
 Epoch:167 train steps: 8310 Loc Loss: 0.0809 Cls Loss: 0.0929 Tot Loss: 0.0869 mAP: 0.0203
 Epoch:167 train steps: 8320 Loc Loss: 0.0819 Cls Loss: 0.0929 Tot Loss: 0.0874 mAP: 0.0212
 Epoch:167 train steps: 8330 Loc Loss: 0.0813 Cls Loss: 0.0928 Tot Loss: 0.0870 mAP: 0.0213
 Epoch:167 train steps: 8340 Loc Loss: 0.0808 Cls Loss: 0.0923 Tot Loss: 0.0866 mAP: 0.0194
 Epoch:167 train steps: 8350 Loc Loss: 0.0792 Cls Loss: 0.0904 Tot Loss: 0.0848 mAP: 0.0194
train
 Epoch:168 train steps: 8360 Loc Loss: 0.0805 Cls Loss: 0.0914 Tot Loss: 0.0859 mAP: 0.0201
 Epoch:168 train steps: 8370 Loc Loss: 0.0819 Cls Loss: 0.0939 Tot Loss: 0.0879 mAP: 0.0203
 Epoch:168 train steps: 8380 Loc Loss: 0.0814 Cls Loss: 0.0923 Tot Loss: 0.0868 mAP: 0.0210
 Epoch:168 train steps: 8390 Loc Loss: 0.0805 Cls Loss: 0.0926 Tot Loss: 0.0866 mAP: 0.0205
 Epoch:168 train steps: 8400 Loc Loss: 0.0801 Cls Loss: 0.0911 Tot Loss: 0.0856 mAP: 0.0217
train
 Epoch:169 train steps: 8410 Loc Loss: 0.0817 Cls Loss: 0.0925 Tot Loss: 0.0871 mAP: 0.0207
 Epoch:169 train steps: 8420 Loc Loss: 0.0800 Cls Loss: 0.0909 Tot Loss: 0.0854 mAP: 0.0195
 Epoch:169 train steps: 8430 Loc Loss: 0.0800 Cls Loss: 0.0910 Tot Loss: 0.0855 mAP: 0.0217
 Epoch:169 train steps: 8440 Loc Loss: 0.0812 Cls Loss: 0.0926 Tot Loss: 0.0869 mAP: 0.0204
 Epoch:169 train steps: 8450 Loc Loss: 0.0813 Cls Loss: 0.0934 Tot Loss: 0.0874 mAP: 0.0195
train
 Epoch:170 train steps: 8460 Loc Loss: 0.0785 Cls Loss: 0.0897 Tot Loss: 0.0841 mAP: 0.0205
 Epoch:170 train steps: 8470 Loc Loss: 0.0804 Cls Loss: 0.0920 Tot Loss: 0.0862 mAP: 0.0201
 Epoch:170 train steps: 8480 Loc Loss: 0.0806 Cls Loss: 0.0924 Tot Loss: 0.0865 mAP: 0.0202
 Epoch:170 train steps: 8490 Loc Loss: 0.0814 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0217
 Epoch:170 train steps: 8500 Loc Loss: 0.0818 Cls Loss: 0.0938 Tot Loss: 0.0878 mAP: 0.0232
train
 Epoch:171 train steps: 8510 Loc Loss: 0.0810 Cls Loss: 0.0927 Tot Loss: 0.0869 mAP: 0.0224
 Epoch:171 train steps: 8520 Loc Loss: 0.0828 Cls Loss: 0.0941 Tot Loss: 0.0884 mAP: 0.0202
 Epoch:171 train steps: 8530 Loc Loss: 0.0793 Cls Loss: 0.0908 Tot Loss: 0.0851 mAP: 0.0205
 Epoch:171 train steps: 8540 Loc Loss: 0.0802 Cls Loss: 0.0911 Tot Loss: 0.0856 mAP: 0.0204
 Epoch:171 train steps: 8550 Loc Loss: 0.0803 Cls Loss: 0.0918 Tot Loss: 0.0860 mAP: 0.0198
train
 Epoch:172 train steps: 8560 Loc Loss: 0.0790 Cls Loss: 0.0903 Tot Loss: 0.0846 mAP: 0.0208
 Epoch:172 train steps: 8570 Loc Loss: 0.0806 Cls Loss: 0.0922 Tot Loss: 0.0864 mAP: 0.0187
 Epoch:172 train steps: 8580 Loc Loss: 0.0805 Cls Loss: 0.0913 Tot Loss: 0.0859 mAP: 0.0192
 Epoch:172 train steps: 8590 Loc Loss: 0.0821 Cls Loss: 0.0932 Tot Loss: 0.0876 mAP: 0.0216
 Epoch:172 train steps: 8600 Loc Loss: 0.0804 Cls Loss: 0.0923 Tot Loss: 0.0863 mAP: 0.0193
train
 Epoch:173 train steps: 8610 Loc Loss: 0.0813 Cls Loss: 0.0933 Tot Loss: 0.0873 mAP: 0.0197
 Epoch:173 train steps: 8620 Loc Loss: 0.0797 Cls Loss: 0.0912 Tot Loss: 0.0855 mAP: 0.0198
 Epoch:173 train steps: 8630 Loc Loss: 0.0811 Cls Loss: 0.0929 Tot Loss: 0.0870 mAP: 0.0223
 Epoch:173 train steps: 8640 Loc Loss: 0.0794 Cls Loss: 0.0909 Tot Loss: 0.0852 mAP: 0.0197
 Epoch:173 train steps: 8650 Loc Loss: 0.0808 Cls Loss: 0.0923 Tot Loss: 0.0866 mAP: 0.0195
train
 Epoch:174 train steps: 8660 Loc Loss: 0.0798 Cls Loss: 0.0909 Tot Loss: 0.0853 mAP: 0.0216
 Epoch:174 train steps: 8670 Loc Loss: 0.0800 Cls Loss: 0.0916 Tot Loss: 0.0858 mAP: 0.0203
 Epoch:174 train steps: 8680 Loc Loss: 0.0819 Cls Loss: 0.0933 Tot Loss: 0.0876 mAP: 0.0211
 Epoch:174 train steps: 8690 Loc Loss: 0.0801 Cls Loss: 0.0917 Tot Loss: 0.0859 mAP: 0.0202
 Epoch:174 train steps: 8700 Loc Loss: 0.0803 Cls Loss: 0.0920 Tot Loss: 0.0862 mAP: 0.0193
train
 Epoch:175 train steps: 8710 Loc Loss: 0.0803 Cls Loss: 0.0916 Tot Loss: 0.0860 mAP: 0.0218
 Epoch:175 train steps: 8720 Loc Loss: 0.0819 Cls Loss: 0.0935 Tot Loss: 0.0877 mAP: 0.0207
 Epoch:175 train steps: 8730 Loc Loss: 0.0808 Cls Loss: 0.0925 Tot Loss: 0.0866 mAP: 0.0204
 Epoch:175 train steps: 8740 Loc Loss: 0.0793 Cls Loss: 0.0902 Tot Loss: 0.0848 mAP: 0.0198
 Epoch:175 train steps: 8750 Loc Loss: 0.0789 Cls Loss: 0.0904 Tot Loss: 0.0847 mAP: 0.0195
train
 Epoch:176 train steps: 8760 Loc Loss: 0.0814 Cls Loss: 0.0928 Tot Loss: 0.0871 mAP: 0.0203
 Epoch:176 train steps: 8770 Loc Loss: 0.0805 Cls Loss: 0.0928 Tot Loss: 0.0867 mAP: 0.0193
 Epoch:176 train steps: 8780 Loc Loss: 0.0798 Cls Loss: 0.0918 Tot Loss: 0.0858 mAP: 0.0199
 Epoch:176 train steps: 8790 Loc Loss: 0.0809 Cls Loss: 0.0914 Tot Loss: 0.0861 mAP: 0.0204
 Epoch:176 train steps: 8800 Loc Loss: 0.0796 Cls Loss: 0.0911 Tot Loss: 0.0854 mAP: 0.0192
train
 Epoch:177 train steps: 8810 Loc Loss: 0.0812 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0209
 Epoch:177 train steps: 8820 Loc Loss: 0.0795 Cls Loss: 0.0907 Tot Loss: 0.0851 mAP: 0.0195
 Epoch:177 train steps: 8830 Loc Loss: 0.0804 Cls Loss: 0.0915 Tot Loss: 0.0860 mAP: 0.0196
 Epoch:177 train steps: 8840 Loc Loss: 0.0796 Cls Loss: 0.0903 Tot Loss: 0.0850 mAP: 0.0202
 Epoch:177 train steps: 8850 Loc Loss: 0.0805 Cls Loss: 0.0919 Tot Loss: 0.0862 mAP: 0.0202
train
 Epoch:178 train steps: 8860 Loc Loss: 0.0809 Cls Loss: 0.0916 Tot Loss: 0.0862 mAP: 0.0201
 Epoch:178 train steps: 8870 Loc Loss: 0.0824 Cls Loss: 0.0944 Tot Loss: 0.0884 mAP: 0.0209
 Epoch:178 train steps: 8880 Loc Loss: 0.0802 Cls Loss: 0.0915 Tot Loss: 0.0858 mAP: 0.0203
 Epoch:178 train steps: 8890 Loc Loss: 0.0801 Cls Loss: 0.0912 Tot Loss: 0.0857 mAP: 0.0209
 Epoch:178 train steps: 8900 Loc Loss: 0.0807 Cls Loss: 0.0924 Tot Loss: 0.0865 mAP: 0.0213
train
 Epoch:179 train steps: 8910 Loc Loss: 0.0810 Cls Loss: 0.0926 Tot Loss: 0.0868 mAP: 0.0198
 Epoch:179 train steps: 8920 Loc Loss: 0.0800 Cls Loss: 0.0910 Tot Loss: 0.0855 mAP: 0.0213
 Epoch:179 train steps: 8930 Loc Loss: 0.0812 Cls Loss: 0.0923 Tot Loss: 0.0867 mAP: 0.0208
 Epoch:179 train steps: 8940 Loc Loss: 0.0804 Cls Loss: 0.0911 Tot Loss: 0.0858 mAP: 0.0205
 Epoch:179 train steps: 8950 Loc Loss: 0.0813 Cls Loss: 0.0932 Tot Loss: 0.0873 mAP: 0.0200
train
 Epoch:180 train steps: 8960 Loc Loss: 0.0822 Cls Loss: 0.0939 Tot Loss: 0.0880 mAP: 0.0208
 Epoch:180 train steps: 8970 Loc Loss: 0.0792 Cls Loss: 0.0906 Tot Loss: 0.0849 mAP: 0.0200
 Epoch:180 train steps: 8980 Loc Loss: 0.0816 Cls Loss: 0.0936 Tot Loss: 0.0876 mAP: 0.0205
 Epoch:180 train steps: 8990 Loc Loss: 0.0812 Cls Loss: 0.0930 Tot Loss: 0.0871 mAP: 0.0214
 Epoch:180 train steps: 9000 Loc Loss: 0.0804 Cls Loss: 0.0918 Tot Loss: 0.0861 mAP: 0.0202
val
 Epoch:180 val Loc Loss: 0.1068 Cls Loss: 0.1245 Tot Loss: 0.1157 mAP: 0.0282
Step 9000/64000.0
----------
train
 Epoch:181 train steps: 9010 Loc Loss: 0.0809 Cls Loss: 0.0923 Tot Loss: 0.0866 mAP: 0.0198
 Epoch:181 train steps: 9020 Loc Loss: 0.0798 Cls Loss: 0.0911 Tot Loss: 0.0855 mAP: 0.0203
 Epoch:181 train steps: 9030 Loc Loss: 0.0809 Cls Loss: 0.0913 Tot Loss: 0.0861 mAP: 0.0203
 Epoch:181 train steps: 9040 Loc Loss: 0.0806 Cls Loss: 0.0923 Tot Loss: 0.0865 mAP: 0.0206
 Epoch:181 train steps: 9050 Loc Loss: 0.0813 Cls Loss: 0.0926 Tot Loss: 0.0869 mAP: 0.0209
train
 Epoch:182 train steps: 9060 Loc Loss: 0.0795 Cls Loss: 0.0907 Tot Loss: 0.0851 mAP: 0.0198
 Epoch:182 train steps: 9070 Loc Loss: 0.0824 Cls Loss: 0.0936 Tot Loss: 0.0880 mAP: 0.0210
 Epoch:182 train steps: 9080 Loc Loss: 0.0805 Cls Loss: 0.0912 Tot Loss: 0.0858 mAP: 0.0201
 Epoch:182 train steps: 9090 Loc Loss: 0.0815 Cls Loss: 0.0932 Tot Loss: 0.0873 mAP: 0.0202
 Epoch:182 train steps: 9100 Loc Loss: 0.0806 Cls Loss: 0.0931 Tot Loss: 0.0869 mAP: 0.0194
train
