./data/charades_traininglabeldata.npy exists
dataset size:7974
./data/charades_testinglabeldata.npy exists
dataset size:1861
Total steps: 50000
datasets created
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv3d-1     [-1, 24, 16, 224, 224]             648
            Conv3d-2     [-1, 24, 16, 224, 224]              72
       BatchNorm3d-3     [-1, 24, 16, 224, 224]              48
         Hardswish-4     [-1, 24, 16, 224, 224]               0
         MaxPool3d-5      [-1, 24, 8, 112, 112]               0
            Conv3d-6      [-1, 54, 8, 112, 112]           1,296
       BatchNorm3d-7      [-1, 54, 8, 112, 112]             108
         Hardswish-8      [-1, 54, 8, 112, 112]               0
            Conv3d-9        [-1, 54, 4, 56, 56]           1,458
      BatchNorm3d-10        [-1, 54, 4, 56, 56]             108
        Hardswish-11        [-1, 54, 4, 56, 56]               0
           Conv3d-12        [-1, 24, 4, 56, 56]           1,296
      BatchNorm3d-13        [-1, 24, 4, 56, 56]              48
AdaptiveAvgPool3d-14          [-1, 24, 1, 1, 1]               0
           Linear-15                    [-1, 6]             150
        Hardswish-16                    [-1, 6]               0
           Linear-17                   [-1, 24]             168
          Sigmoid-18                   [-1, 24]               0
           Conv3d-19        [-1, 24, 4, 56, 56]             576
      BatchNorm3d-20        [-1, 24, 4, 56, 56]              48
        Hardswish-21        [-1, 24, 4, 56, 56]               0
       Bottleneck-22        [-1, 24, 4, 56, 56]               0
           Conv3d-23        [-1, 54, 4, 56, 56]           1,296
      BatchNorm3d-24        [-1, 54, 4, 56, 56]             108
        Hardswish-25        [-1, 54, 4, 56, 56]               0
           Conv3d-26        [-1, 54, 4, 56, 56]           1,458
      BatchNorm3d-27        [-1, 54, 4, 56, 56]             108
        Hardswish-28        [-1, 54, 4, 56, 56]               0
           Conv3d-29        [-1, 24, 4, 56, 56]           1,296
      BatchNorm3d-30        [-1, 24, 4, 56, 56]              48
AdaptiveAvgPool3d-31          [-1, 24, 1, 1, 1]               0
           Linear-32                    [-1, 6]             150
        Hardswish-33                    [-1, 6]               0
           Linear-34                   [-1, 24]             168
          Sigmoid-35                   [-1, 24]               0
        Hardswish-36        [-1, 24, 4, 56, 56]               0
       Bottleneck-37        [-1, 24, 4, 56, 56]               0
           Conv3d-38        [-1, 54, 4, 56, 56]           1,296
      BatchNorm3d-39        [-1, 54, 4, 56, 56]             108
        Hardswish-40        [-1, 54, 4, 56, 56]               0
           Conv3d-41        [-1, 54, 4, 56, 56]           1,458
      BatchNorm3d-42        [-1, 54, 4, 56, 56]             108
        Hardswish-43        [-1, 54, 4, 56, 56]               0
           Conv3d-44        [-1, 24, 4, 56, 56]           1,296
      BatchNorm3d-45        [-1, 24, 4, 56, 56]              48
AdaptiveAvgPool3d-46          [-1, 24, 1, 1, 1]               0
           Linear-47                    [-1, 6]             150
        Hardswish-48                    [-1, 6]               0
           Linear-49                   [-1, 24]             168
          Sigmoid-50                   [-1, 24]               0
        Hardswish-51        [-1, 24, 4, 56, 56]               0
       Bottleneck-52        [-1, 24, 4, 56, 56]               0
           Conv3d-53       [-1, 108, 4, 56, 56]           2,592
      BatchNorm3d-54       [-1, 108, 4, 56, 56]             216
        Hardswish-55       [-1, 108, 4, 56, 56]               0
           Conv3d-56       [-1, 108, 2, 28, 28]           2,916
      BatchNorm3d-57       [-1, 108, 2, 28, 28]             216
        Hardswish-58       [-1, 108, 2, 28, 28]               0
           Conv3d-59        [-1, 48, 2, 28, 28]           5,184
      BatchNorm3d-60        [-1, 48, 2, 28, 28]              96
AdaptiveAvgPool3d-61          [-1, 48, 1, 1, 1]               0
           Linear-62                   [-1, 12]             588
        Hardswish-63                   [-1, 12]               0
           Linear-64                   [-1, 48]             624
          Sigmoid-65                   [-1, 48]               0
           Conv3d-66        [-1, 48, 2, 28, 28]           1,152
      BatchNorm3d-67        [-1, 48, 2, 28, 28]              96
        Hardswish-68        [-1, 48, 2, 28, 28]               0
       Bottleneck-69        [-1, 48, 2, 28, 28]               0
           Conv3d-70       [-1, 108, 2, 28, 28]           5,184
      BatchNorm3d-71       [-1, 108, 2, 28, 28]             216
        Hardswish-72       [-1, 108, 2, 28, 28]               0
           Conv3d-73       [-1, 108, 2, 28, 28]           2,916
      BatchNorm3d-74       [-1, 108, 2, 28, 28]             216
        Hardswish-75       [-1, 108, 2, 28, 28]               0
           Conv3d-76        [-1, 48, 2, 28, 28]           5,184
      BatchNorm3d-77        [-1, 48, 2, 28, 28]              96
AdaptiveAvgPool3d-78          [-1, 48, 1, 1, 1]               0
           Linear-79                   [-1, 12]             588
        Hardswish-80                   [-1, 12]               0
           Linear-81                   [-1, 48]             624
          Sigmoid-82                   [-1, 48]               0
        Hardswish-83        [-1, 48, 2, 28, 28]               0
       Bottleneck-84        [-1, 48, 2, 28, 28]               0
           Conv3d-85       [-1, 108, 2, 28, 28]           5,184
      BatchNorm3d-86       [-1, 108, 2, 28, 28]             216
        Hardswish-87       [-1, 108, 2, 28, 28]               0
           Conv3d-88       [-1, 108, 2, 28, 28]           2,916
      BatchNorm3d-89       [-1, 108, 2, 28, 28]             216
        Hardswish-90       [-1, 108, 2, 28, 28]               0
           Conv3d-91        [-1, 48, 2, 28, 28]           5,184
      BatchNorm3d-92        [-1, 48, 2, 28, 28]              96
AdaptiveAvgPool3d-93          [-1, 48, 1, 1, 1]               0
           Linear-94                   [-1, 12]             588
        Hardswish-95                   [-1, 12]               0
           Linear-96                   [-1, 48]             624
          Sigmoid-97                   [-1, 48]               0
        Hardswish-98        [-1, 48, 2, 28, 28]               0
       Bottleneck-99        [-1, 48, 2, 28, 28]               0
          Conv3d-100       [-1, 108, 2, 28, 28]           5,184
     BatchNorm3d-101       [-1, 108, 2, 28, 28]             216
       Hardswish-102       [-1, 108, 2, 28, 28]               0
          Conv3d-103       [-1, 108, 2, 28, 28]           2,916
     BatchNorm3d-104       [-1, 108, 2, 28, 28]             216
       Hardswish-105       [-1, 108, 2, 28, 28]               0
          Conv3d-106        [-1, 48, 2, 28, 28]           5,184
     BatchNorm3d-107        [-1, 48, 2, 28, 28]              96
AdaptiveAvgPool3d-108          [-1, 48, 1, 1, 1]               0
          Linear-109                   [-1, 12]             588
       Hardswish-110                   [-1, 12]               0
          Linear-111                   [-1, 48]             624
         Sigmoid-112                   [-1, 48]               0
       Hardswish-113        [-1, 48, 2, 28, 28]               0
      Bottleneck-114        [-1, 48, 2, 28, 28]               0
          Conv3d-115       [-1, 108, 2, 28, 28]           5,184
     BatchNorm3d-116       [-1, 108, 2, 28, 28]             216
       Hardswish-117       [-1, 108, 2, 28, 28]               0
          Conv3d-118       [-1, 108, 2, 28, 28]           2,916
     BatchNorm3d-119       [-1, 108, 2, 28, 28]             216
       Hardswish-120       [-1, 108, 2, 28, 28]               0
          Conv3d-121        [-1, 48, 2, 28, 28]           5,184
     BatchNorm3d-122        [-1, 48, 2, 28, 28]              96
AdaptiveAvgPool3d-123          [-1, 48, 1, 1, 1]               0
          Linear-124                   [-1, 12]             588
       Hardswish-125                   [-1, 12]               0
          Linear-126                   [-1, 48]             624
         Sigmoid-127                   [-1, 48]               0
       Hardswish-128        [-1, 48, 2, 28, 28]               0
      Bottleneck-129        [-1, 48, 2, 28, 28]               0
          Conv3d-130       [-1, 216, 2, 28, 28]          10,368
     BatchNorm3d-131       [-1, 216, 2, 28, 28]             432
       Hardswish-132       [-1, 216, 2, 28, 28]               0
          Conv3d-133       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-134       [-1, 216, 1, 14, 14]             432
       Hardswish-135       [-1, 216, 1, 14, 14]               0
          Conv3d-136        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-137        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-138          [-1, 96, 1, 1, 1]               0
          Linear-139                   [-1, 24]           2,328
       Hardswish-140                   [-1, 24]               0
          Linear-141                   [-1, 96]           2,400
         Sigmoid-142                   [-1, 96]               0
          Conv3d-143        [-1, 96, 1, 14, 14]           4,608
     BatchNorm3d-144        [-1, 96, 1, 14, 14]             192
       Hardswish-145        [-1, 96, 1, 14, 14]               0
      Bottleneck-146        [-1, 96, 1, 14, 14]               0
          Conv3d-147       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-148       [-1, 216, 1, 14, 14]             432
       Hardswish-149       [-1, 216, 1, 14, 14]               0
          Conv3d-150       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-151       [-1, 216, 1, 14, 14]             432
       Hardswish-152       [-1, 216, 1, 14, 14]               0
          Conv3d-153        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-154        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-155          [-1, 96, 1, 1, 1]               0
          Linear-156                   [-1, 24]           2,328
       Hardswish-157                   [-1, 24]               0
          Linear-158                   [-1, 96]           2,400
         Sigmoid-159                   [-1, 96]               0
       Hardswish-160        [-1, 96, 1, 14, 14]               0
      Bottleneck-161        [-1, 96, 1, 14, 14]               0
          Conv3d-162       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-163       [-1, 216, 1, 14, 14]             432
       Hardswish-164       [-1, 216, 1, 14, 14]               0
          Conv3d-165       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-166       [-1, 216, 1, 14, 14]             432
       Hardswish-167       [-1, 216, 1, 14, 14]               0
          Conv3d-168        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-169        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-170          [-1, 96, 1, 1, 1]               0
          Linear-171                   [-1, 24]           2,328
       Hardswish-172                   [-1, 24]               0
          Linear-173                   [-1, 96]           2,400
         Sigmoid-174                   [-1, 96]               0
       Hardswish-175        [-1, 96, 1, 14, 14]               0
      Bottleneck-176        [-1, 96, 1, 14, 14]               0
          Conv3d-177       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-178       [-1, 216, 1, 14, 14]             432
       Hardswish-179       [-1, 216, 1, 14, 14]               0
          Conv3d-180       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-181       [-1, 216, 1, 14, 14]             432
       Hardswish-182       [-1, 216, 1, 14, 14]               0
          Conv3d-183        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-184        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-185          [-1, 96, 1, 1, 1]               0
          Linear-186                   [-1, 24]           2,328
       Hardswish-187                   [-1, 24]               0
          Linear-188                   [-1, 96]           2,400
         Sigmoid-189                   [-1, 96]               0
       Hardswish-190        [-1, 96, 1, 14, 14]               0
      Bottleneck-191        [-1, 96, 1, 14, 14]               0
          Conv3d-192       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-193       [-1, 216, 1, 14, 14]             432
       Hardswish-194       [-1, 216, 1, 14, 14]               0
          Conv3d-195       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-196       [-1, 216, 1, 14, 14]             432
       Hardswish-197       [-1, 216, 1, 14, 14]               0
          Conv3d-198        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-199        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-200          [-1, 96, 1, 1, 1]               0
          Linear-201                   [-1, 24]           2,328
       Hardswish-202                   [-1, 24]               0
          Linear-203                   [-1, 96]           2,400
         Sigmoid-204                   [-1, 96]               0
       Hardswish-205        [-1, 96, 1, 14, 14]               0
      Bottleneck-206        [-1, 96, 1, 14, 14]               0
          Conv3d-207       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-208       [-1, 216, 1, 14, 14]             432
       Hardswish-209       [-1, 216, 1, 14, 14]               0
          Conv3d-210       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-211       [-1, 216, 1, 14, 14]             432
       Hardswish-212       [-1, 216, 1, 14, 14]               0
          Conv3d-213        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-214        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-215          [-1, 96, 1, 1, 1]               0
          Linear-216                   [-1, 24]           2,328
       Hardswish-217                   [-1, 24]               0
          Linear-218                   [-1, 96]           2,400
         Sigmoid-219                   [-1, 96]               0
       Hardswish-220        [-1, 96, 1, 14, 14]               0
      Bottleneck-221        [-1, 96, 1, 14, 14]               0
          Conv3d-222       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-223       [-1, 216, 1, 14, 14]             432
       Hardswish-224       [-1, 216, 1, 14, 14]               0
          Conv3d-225       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-226       [-1, 216, 1, 14, 14]             432
       Hardswish-227       [-1, 216, 1, 14, 14]               0
          Conv3d-228        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-229        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-230          [-1, 96, 1, 1, 1]               0
          Linear-231                   [-1, 24]           2,328
       Hardswish-232                   [-1, 24]               0
          Linear-233                   [-1, 96]           2,400
         Sigmoid-234                   [-1, 96]               0
       Hardswish-235        [-1, 96, 1, 14, 14]               0
      Bottleneck-236        [-1, 96, 1, 14, 14]               0
          Conv3d-237       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-238       [-1, 216, 1, 14, 14]             432
       Hardswish-239       [-1, 216, 1, 14, 14]               0
          Conv3d-240       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-241       [-1, 216, 1, 14, 14]             432
       Hardswish-242       [-1, 216, 1, 14, 14]               0
          Conv3d-243        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-244        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-245          [-1, 96, 1, 1, 1]               0
          Linear-246                   [-1, 24]           2,328
       Hardswish-247                   [-1, 24]               0
          Linear-248                   [-1, 96]           2,400
         Sigmoid-249                   [-1, 96]               0
       Hardswish-250        [-1, 96, 1, 14, 14]               0
      Bottleneck-251        [-1, 96, 1, 14, 14]               0
          Conv3d-252       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-253       [-1, 216, 1, 14, 14]             432
       Hardswish-254       [-1, 216, 1, 14, 14]               0
          Conv3d-255       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-256       [-1, 216, 1, 14, 14]             432
       Hardswish-257       [-1, 216, 1, 14, 14]               0
          Conv3d-258        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-259        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-260          [-1, 96, 1, 1, 1]               0
          Linear-261                   [-1, 24]           2,328
       Hardswish-262                   [-1, 24]               0
          Linear-263                   [-1, 96]           2,400
         Sigmoid-264                   [-1, 96]               0
       Hardswish-265        [-1, 96, 1, 14, 14]               0
      Bottleneck-266        [-1, 96, 1, 14, 14]               0
          Conv3d-267       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-268       [-1, 216, 1, 14, 14]             432
       Hardswish-269       [-1, 216, 1, 14, 14]               0
          Conv3d-270       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-271       [-1, 216, 1, 14, 14]             432
       Hardswish-272       [-1, 216, 1, 14, 14]               0
          Conv3d-273        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-274        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-275          [-1, 96, 1, 1, 1]               0
          Linear-276                   [-1, 24]           2,328
       Hardswish-277                   [-1, 24]               0
          Linear-278                   [-1, 96]           2,400
         Sigmoid-279                   [-1, 96]               0
       Hardswish-280        [-1, 96, 1, 14, 14]               0
      Bottleneck-281        [-1, 96, 1, 14, 14]               0
          Conv3d-282       [-1, 216, 1, 14, 14]          20,736
     BatchNorm3d-283       [-1, 216, 1, 14, 14]             432
       Hardswish-284       [-1, 216, 1, 14, 14]               0
          Conv3d-285       [-1, 216, 1, 14, 14]           5,832
     BatchNorm3d-286       [-1, 216, 1, 14, 14]             432
       Hardswish-287       [-1, 216, 1, 14, 14]               0
          Conv3d-288        [-1, 96, 1, 14, 14]          20,736
     BatchNorm3d-289        [-1, 96, 1, 14, 14]             192
AdaptiveAvgPool3d-290          [-1, 96, 1, 1, 1]               0
          Linear-291                   [-1, 24]           2,328
       Hardswish-292                   [-1, 24]               0
          Linear-293                   [-1, 96]           2,400
         Sigmoid-294                   [-1, 96]               0
       Hardswish-295        [-1, 96, 1, 14, 14]               0
      Bottleneck-296        [-1, 96, 1, 14, 14]               0
          Conv3d-297       [-1, 432, 1, 14, 14]          41,472
     BatchNorm3d-298       [-1, 432, 1, 14, 14]             864
       Hardswish-299       [-1, 432, 1, 14, 14]               0
          Conv3d-300         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-301         [-1, 432, 1, 7, 7]             864
       Hardswish-302         [-1, 432, 1, 7, 7]               0
          Conv3d-303         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-304         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-305         [-1, 192, 1, 1, 1]               0
          Linear-306                   [-1, 48]           9,264
       Hardswish-307                   [-1, 48]               0
          Linear-308                  [-1, 192]           9,408
         Sigmoid-309                  [-1, 192]               0
          Conv3d-310         [-1, 192, 1, 7, 7]          18,432
     BatchNorm3d-311         [-1, 192, 1, 7, 7]             384
       Hardswish-312         [-1, 192, 1, 7, 7]               0
      Bottleneck-313         [-1, 192, 1, 7, 7]               0
          Conv3d-314         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-315         [-1, 432, 1, 7, 7]             864
       Hardswish-316         [-1, 432, 1, 7, 7]               0
          Conv3d-317         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-318         [-1, 432, 1, 7, 7]             864
       Hardswish-319         [-1, 432, 1, 7, 7]               0
          Conv3d-320         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-321         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-322         [-1, 192, 1, 1, 1]               0
          Linear-323                   [-1, 48]           9,264
       Hardswish-324                   [-1, 48]               0
          Linear-325                  [-1, 192]           9,408
         Sigmoid-326                  [-1, 192]               0
       Hardswish-327         [-1, 192, 1, 7, 7]               0
      Bottleneck-328         [-1, 192, 1, 7, 7]               0
          Conv3d-329         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-330         [-1, 432, 1, 7, 7]             864
       Hardswish-331         [-1, 432, 1, 7, 7]               0
          Conv3d-332         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-333         [-1, 432, 1, 7, 7]             864
       Hardswish-334         [-1, 432, 1, 7, 7]               0
          Conv3d-335         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-336         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-337         [-1, 192, 1, 1, 1]               0
          Linear-338                   [-1, 48]           9,264
       Hardswish-339                   [-1, 48]               0
          Linear-340                  [-1, 192]           9,408
         Sigmoid-341                  [-1, 192]               0
       Hardswish-342         [-1, 192, 1, 7, 7]               0
      Bottleneck-343         [-1, 192, 1, 7, 7]               0
          Conv3d-344         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-345         [-1, 432, 1, 7, 7]             864
       Hardswish-346         [-1, 432, 1, 7, 7]               0
          Conv3d-347         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-348         [-1, 432, 1, 7, 7]             864
       Hardswish-349         [-1, 432, 1, 7, 7]               0
          Conv3d-350         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-351         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-352         [-1, 192, 1, 1, 1]               0
          Linear-353                   [-1, 48]           9,264
       Hardswish-354                   [-1, 48]               0
          Linear-355                  [-1, 192]           9,408
         Sigmoid-356                  [-1, 192]               0
       Hardswish-357         [-1, 192, 1, 7, 7]               0
      Bottleneck-358         [-1, 192, 1, 7, 7]               0
          Conv3d-359         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-360         [-1, 432, 1, 7, 7]             864
       Hardswish-361         [-1, 432, 1, 7, 7]               0
          Conv3d-362         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-363         [-1, 432, 1, 7, 7]             864
       Hardswish-364         [-1, 432, 1, 7, 7]               0
          Conv3d-365         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-366         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-367         [-1, 192, 1, 1, 1]               0
          Linear-368                   [-1, 48]           9,264
       Hardswish-369                   [-1, 48]               0
          Linear-370                  [-1, 192]           9,408
         Sigmoid-371                  [-1, 192]               0
       Hardswish-372         [-1, 192, 1, 7, 7]               0
      Bottleneck-373         [-1, 192, 1, 7, 7]               0
          Conv3d-374         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-375         [-1, 432, 1, 7, 7]             864
       Hardswish-376         [-1, 432, 1, 7, 7]               0
          Conv3d-377         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-378         [-1, 432, 1, 7, 7]             864
       Hardswish-379         [-1, 432, 1, 7, 7]               0
          Conv3d-380         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-381         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-382         [-1, 192, 1, 1, 1]               0
          Linear-383                   [-1, 48]           9,264
       Hardswish-384                   [-1, 48]               0
          Linear-385                  [-1, 192]           9,408
         Sigmoid-386                  [-1, 192]               0
       Hardswish-387         [-1, 192, 1, 7, 7]               0
      Bottleneck-388         [-1, 192, 1, 7, 7]               0
          Conv3d-389         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-390         [-1, 432, 1, 7, 7]             864
       Hardswish-391         [-1, 432, 1, 7, 7]               0
          Conv3d-392         [-1, 432, 1, 7, 7]          11,664
     BatchNorm3d-393         [-1, 432, 1, 7, 7]             864
       Hardswish-394         [-1, 432, 1, 7, 7]               0
          Conv3d-395         [-1, 192, 1, 7, 7]          82,944
     BatchNorm3d-396         [-1, 192, 1, 7, 7]             384
AdaptiveAvgPool3d-397         [-1, 192, 1, 1, 1]               0
          Linear-398                   [-1, 48]           9,264
       Hardswish-399                   [-1, 48]               0
          Linear-400                  [-1, 192]           9,408
         Sigmoid-401                  [-1, 192]               0
       Hardswish-402         [-1, 192, 1, 7, 7]               0
      Bottleneck-403         [-1, 192, 1, 7, 7]               0
          Conv3d-404         [-1, 432, 1, 7, 7]          82,944
     BatchNorm3d-405         [-1, 432, 1, 7, 7]             864
       Hardswish-406         [-1, 432, 1, 7, 7]               0
AdaptiveAvgPool3d-407        [-1, 432, 16, 1, 1]               0
          Linear-408             [-1, 16, 2048]         886,784
          Linear-409              [-1, 16, 157]         321,693
================================================================
Total params: 3,325,445
Trainable params: 3,325,445
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 9.19
Forward/backward pass size (MB): 964.58
Params size (MB): 12.69
Estimated Total Size (MB): 986.45
----------------------------------------------------------------
model loaded
Step 0/0
----------
train
LR:  0.04
 Epoch:1 train steps: 1 Loc Loss: 0.0696 Cls Loss: 0.0696 Tot Loss: 0.0696 mAP: 0.0228
 Epoch:1 train steps: 10 Loc Loss: 0.6145 Cls Loss: 0.6146 Tot Loss: 0.6145 mAP: 0.0179
 Epoch:1 train steps: 20 Loc Loss: 0.6347 Cls Loss: 0.6349 Tot Loss: 0.6348 mAP: 0.0177
 Epoch:1 train steps: 30 Loc Loss: 0.5665 Cls Loss: 0.5669 Tot Loss: 0.5667 mAP: 0.0177
 Epoch:1 train steps: 40 Loc Loss: 0.4745 Cls Loss: 0.4754 Tot Loss: 0.4750 mAP: 0.0175
 Epoch:1 train steps: 50 Loc Loss: 0.3547 Cls Loss: 0.3561 Tot Loss: 0.3554 mAP: 0.0178
 Epoch:1 train steps: 60 Loc Loss: 0.2337 Cls Loss: 0.2360 Tot Loss: 0.2348 mAP: 0.0177
 Epoch:1 train steps: 70 Loc Loss: 0.1505 Cls Loss: 0.1536 Tot Loss: 0.1521 mAP: 0.0178
 Epoch:1 train steps: 80 Loc Loss: 0.1112 Cls Loss: 0.1153 Tot Loss: 0.1132 mAP: 0.0179
 Epoch:1 train steps: 90 Loc Loss: 0.0952 Cls Loss: 0.0998 Tot Loss: 0.0975 mAP: 0.0181
 Epoch:1 train steps: 100 Loc Loss: 0.0885 Cls Loss: 0.0934 Tot Loss: 0.0910 mAP: 0.0177
 Epoch:1 train steps: 110 Loc Loss: 0.0856 Cls Loss: 0.0909 Tot Loss: 0.0882 mAP: 0.0177
 Epoch:1 train steps: 120 Loc Loss: 0.0846 Cls Loss: 0.0901 Tot Loss: 0.0874 mAP: 0.0179
 Epoch:1 train steps: 130 Loc Loss: 0.0829 Cls Loss: 0.0885 Tot Loss: 0.0857 mAP: 0.0178
 Epoch:1 train steps: 140 Loc Loss: 0.0825 Cls Loss: 0.0881 Tot Loss: 0.0853 mAP: 0.0180
 Epoch:1 train steps: 150 Loc Loss: 0.0826 Cls Loss: 0.0881 Tot Loss: 0.0853 mAP: 0.0178
 Epoch:1 train steps: 160 Loc Loss: 0.0816 Cls Loss: 0.0873 Tot Loss: 0.0845 mAP: 0.0177
 Epoch:1 train steps: 170 Loc Loss: 0.0825 Cls Loss: 0.0882 Tot Loss: 0.0854 mAP: 0.0178
 Epoch:1 train steps: 180 Loc Loss: 0.0814 Cls Loss: 0.0871 Tot Loss: 0.0843 mAP: 0.0177
 Epoch:1 train steps: 190 Loc Loss: 0.0808 Cls Loss: 0.0866 Tot Loss: 0.0837 mAP: 0.0177
 Epoch:1 train steps: 200 Loc Loss: 0.0816 Cls Loss: 0.0874 Tot Loss: 0.0845 mAP: 0.0178
 Epoch:1 train steps: 210 Loc Loss: 0.0810 Cls Loss: 0.0868 Tot Loss: 0.0839 mAP: 0.0176
 Epoch:1 train steps: 220 Loc Loss: 0.0813 Cls Loss: 0.0872 Tot Loss: 0.0842 mAP: 0.0176
 Epoch:1 train steps: 230 Loc Loss: 0.0806 Cls Loss: 0.0864 Tot Loss: 0.0835 mAP: 0.0177
 Epoch:1 train steps: 240 Loc Loss: 0.0809 Cls Loss: 0.0868 Tot Loss: 0.0838 mAP: 0.0178
 Epoch:1 train steps: 250 Loc Loss: 0.0810 Cls Loss: 0.0868 Tot Loss: 0.0839 mAP: 0.0177
 Epoch:1 train steps: 260 Loc Loss: 0.0810 Cls Loss: 0.0870 Tot Loss: 0.0840 mAP: 0.0176
 Epoch:1 train steps: 270 Loc Loss: 0.0809 Cls Loss: 0.0866 Tot Loss: 0.0838 mAP: 0.0178
 Epoch:1 train steps: 280 Loc Loss: 0.0810 Cls Loss: 0.0869 Tot Loss: 0.0839 mAP: 0.0180
 Epoch:1 train steps: 290 Loc Loss: 0.0807 Cls Loss: 0.0865 Tot Loss: 0.0836 mAP: 0.0184
 Epoch:1 train steps: 300 Loc Loss: 0.0807 Cls Loss: 0.0864 Tot Loss: 0.0836 mAP: 0.0181
 Epoch:1 train steps: 310 Loc Loss: 0.0809 Cls Loss: 0.0867 Tot Loss: 0.0838 mAP: 0.0181
 Epoch:1 train steps: 320 Loc Loss: 0.0806 Cls Loss: 0.0862 Tot Loss: 0.0834 mAP: 0.0178
 Epoch:1 train steps: 330 Loc Loss: 0.0807 Cls Loss: 0.0865 Tot Loss: 0.0836 mAP: 0.0178
 Epoch:1 train steps: 340 Loc Loss: 0.0809 Cls Loss: 0.0869 Tot Loss: 0.0839 mAP: 0.0181
 Epoch:1 train steps: 350 Loc Loss: 0.0809 Cls Loss: 0.0868 Tot Loss: 0.0839 mAP: 0.0180
 Epoch:1 train steps: 360 Loc Loss: 0.0799 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0177
 Epoch:1 train steps: 370 Loc Loss: 0.0805 Cls Loss: 0.0863 Tot Loss: 0.0834 mAP: 0.0180
 Epoch:1 train steps: 380 Loc Loss: 0.0805 Cls Loss: 0.0864 Tot Loss: 0.0834 mAP: 0.0181
 Epoch:1 train steps: 390 Loc Loss: 0.0807 Cls Loss: 0.0863 Tot Loss: 0.0835 mAP: 0.0180
 Epoch:1 train steps: 400 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0831 mAP: 0.0183
 Epoch:1 train steps: 410 Loc Loss: 0.0805 Cls Loss: 0.0862 Tot Loss: 0.0834 mAP: 0.0178
 Epoch:1 train steps: 420 Loc Loss: 0.0802 Cls Loss: 0.0859 Tot Loss: 0.0831 mAP: 0.0181
 Epoch:1 train steps: 430 Loc Loss: 0.0806 Cls Loss: 0.0864 Tot Loss: 0.0835 mAP: 0.0179
 Epoch:1 train steps: 440 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0179
 Epoch:1 train steps: 450 Loc Loss: 0.0801 Cls Loss: 0.0862 Tot Loss: 0.0831 mAP: 0.0177
 Epoch:1 train steps: 460 Loc Loss: 0.0804 Cls Loss: 0.0862 Tot Loss: 0.0833 mAP: 0.0179
 Epoch:1 train steps: 470 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0178
 Epoch:1 train steps: 480 Loc Loss: 0.0799 Cls Loss: 0.0860 Tot Loss: 0.0829 mAP: 0.0180
 Epoch:1 train steps: 490 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0180
 Epoch:1 train steps: 500 Loc Loss: 0.0803 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0178
 Epoch:1 train steps: 510 Loc Loss: 0.0800 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0178
 Epoch:1 train steps: 520 Loc Loss: 0.0812 Cls Loss: 0.0872 Tot Loss: 0.0842 mAP: 0.0179
 Epoch:1 train steps: 530 Loc Loss: 0.0803 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0178
 Epoch:1 train steps: 540 Loc Loss: 0.0801 Cls Loss: 0.0861 Tot Loss: 0.0831 mAP: 0.0179
 Epoch:1 train steps: 550 Loc Loss: 0.0804 Cls Loss: 0.0862 Tot Loss: 0.0833 mAP: 0.0179
 Epoch:1 train steps: 560 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0180
 Epoch:1 train steps: 570 Loc Loss: 0.0804 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0181
 Epoch:1 train steps: 580 Loc Loss: 0.0799 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0179
 Epoch:1 train steps: 590 Loc Loss: 0.0803 Cls Loss: 0.0862 Tot Loss: 0.0833 mAP: 0.0183
 Epoch:1 train steps: 600 Loc Loss: 0.0807 Cls Loss: 0.0865 Tot Loss: 0.0836 mAP: 0.0181
 Epoch:1 train steps: 610 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0181
 Epoch:1 train steps: 620 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0179
 Epoch:1 train steps: 630 Loc Loss: 0.0801 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0179
 Epoch:1 train steps: 640 Loc Loss: 0.0799 Cls Loss: 0.0858 Tot Loss: 0.0828 mAP: 0.0180
 Epoch:1 train steps: 650 Loc Loss: 0.0800 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0182
 Epoch:1 train steps: 660 Loc Loss: 0.0804 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0181
train
 Epoch:2 train steps: 670 Loc Loss: 0.0240 Cls Loss: 0.0257 Tot Loss: 0.0249 mAP: 0.0180
 Epoch:2 train steps: 680 Loc Loss: 0.0801 Cls Loss: 0.0858 Tot Loss: 0.0830 mAP: 0.0179
 Epoch:2 train steps: 690 Loc Loss: 0.0804 Cls Loss: 0.0865 Tot Loss: 0.0834 mAP: 0.0180
 Epoch:2 train steps: 700 Loc Loss: 0.0800 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0181
 Epoch:2 train steps: 710 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0179
 Epoch:2 train steps: 720 Loc Loss: 0.0805 Cls Loss: 0.0866 Tot Loss: 0.0836 mAP: 0.0181
 Epoch:2 train steps: 730 Loc Loss: 0.0801 Cls Loss: 0.0861 Tot Loss: 0.0831 mAP: 0.0182
 Epoch:2 train steps: 740 Loc Loss: 0.0795 Cls Loss: 0.0854 Tot Loss: 0.0825 mAP: 0.0180
 Epoch:2 train steps: 750 Loc Loss: 0.0799 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0184
 Epoch:2 train steps: 760 Loc Loss: 0.0806 Cls Loss: 0.0864 Tot Loss: 0.0835 mAP: 0.0179
 Epoch:2 train steps: 770 Loc Loss: 0.0800 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0183
 Epoch:2 train steps: 780 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0181
 Epoch:2 train steps: 790 Loc Loss: 0.0798 Cls Loss: 0.0855 Tot Loss: 0.0827 mAP: 0.0179
 Epoch:2 train steps: 800 Loc Loss: 0.0801 Cls Loss: 0.0856 Tot Loss: 0.0829 mAP: 0.0185
 Epoch:2 train steps: 810 Loc Loss: 0.0797 Cls Loss: 0.0857 Tot Loss: 0.0827 mAP: 0.0181
 Epoch:2 train steps: 820 Loc Loss: 0.0797 Cls Loss: 0.0854 Tot Loss: 0.0826 mAP: 0.0181
 Epoch:2 train steps: 830 Loc Loss: 0.0795 Cls Loss: 0.0851 Tot Loss: 0.0823 mAP: 0.0175
 Epoch:2 train steps: 840 Loc Loss: 0.0802 Cls Loss: 0.0862 Tot Loss: 0.0832 mAP: 0.0182
 Epoch:2 train steps: 850 Loc Loss: 0.0797 Cls Loss: 0.0856 Tot Loss: 0.0826 mAP: 0.0179
 Epoch:2 train steps: 860 Loc Loss: 0.0809 Cls Loss: 0.0868 Tot Loss: 0.0839 mAP: 0.0181
 Epoch:2 train steps: 870 Loc Loss: 0.0804 Cls Loss: 0.0862 Tot Loss: 0.0833 mAP: 0.0181
 Epoch:2 train steps: 880 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0179
 Epoch:2 train steps: 890 Loc Loss: 0.0804 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0179
 Epoch:2 train steps: 900 Loc Loss: 0.0799 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0179
 Epoch:2 train steps: 910 Loc Loss: 0.0803 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0182
 Epoch:2 train steps: 920 Loc Loss: 0.0803 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0179
 Epoch:2 train steps: 930 Loc Loss: 0.0803 Cls Loss: 0.0859 Tot Loss: 0.0831 mAP: 0.0183
 Epoch:2 train steps: 940 Loc Loss: 0.0803 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0182
 Epoch:2 train steps: 950 Loc Loss: 0.0799 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0179
 Epoch:2 train steps: 960 Loc Loss: 0.0798 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0181
 Epoch:2 train steps: 970 Loc Loss: 0.0799 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0182
 Epoch:2 train steps: 980 Loc Loss: 0.0802 Cls Loss: 0.0858 Tot Loss: 0.0830 mAP: 0.0182
 Epoch:2 train steps: 990 Loc Loss: 0.0803 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0183
 Epoch:2 train steps: 1000 Loc Loss: 0.0800 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0179
 Epoch:2 train steps: 1010 Loc Loss: 0.0798 Cls Loss: 0.0859 Tot Loss: 0.0828 mAP: 0.0180
 Epoch:2 train steps: 1020 Loc Loss: 0.0805 Cls Loss: 0.0864 Tot Loss: 0.0835 mAP: 0.0181
 Epoch:2 train steps: 1030 Loc Loss: 0.0803 Cls Loss: 0.0862 Tot Loss: 0.0832 mAP: 0.0179
 Epoch:2 train steps: 1040 Loc Loss: 0.0804 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0182
 Epoch:2 train steps: 1050 Loc Loss: 0.0795 Cls Loss: 0.0855 Tot Loss: 0.0825 mAP: 0.0183
 Epoch:2 train steps: 1060 Loc Loss: 0.0800 Cls Loss: 0.0857 Tot Loss: 0.0829 mAP: 0.0181
 Epoch:2 train steps: 1070 Loc Loss: 0.0806 Cls Loss: 0.0867 Tot Loss: 0.0837 mAP: 0.0185
 Epoch:2 train steps: 1080 Loc Loss: 0.0803 Cls Loss: 0.0862 Tot Loss: 0.0832 mAP: 0.0181
 Epoch:2 train steps: 1090 Loc Loss: 0.0798 Cls Loss: 0.0859 Tot Loss: 0.0829 mAP: 0.0180
 Epoch:2 train steps: 1100 Loc Loss: 0.0801 Cls Loss: 0.0861 Tot Loss: 0.0831 mAP: 0.0183
 Epoch:2 train steps: 1110 Loc Loss: 0.0800 Cls Loss: 0.0859 Tot Loss: 0.0829 mAP: 0.0183
 Epoch:2 train steps: 1120 Loc Loss: 0.0804 Cls Loss: 0.0864 Tot Loss: 0.0834 mAP: 0.0183
 Epoch:2 train steps: 1130 Loc Loss: 0.0798 Cls Loss: 0.0852 Tot Loss: 0.0825 mAP: 0.0182
 Epoch:2 train steps: 1140 Loc Loss: 0.0798 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0180
 Epoch:2 train steps: 1150 Loc Loss: 0.0805 Cls Loss: 0.0864 Tot Loss: 0.0835 mAP: 0.0185
 Epoch:2 train steps: 1160 Loc Loss: 0.0802 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0183
 Epoch:2 train steps: 1170 Loc Loss: 0.0800 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0180
 Epoch:2 train steps: 1180 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0831 mAP: 0.0180
 Epoch:2 train steps: 1190 Loc Loss: 0.0798 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0180
 Epoch:2 train steps: 1200 Loc Loss: 0.0800 Cls Loss: 0.0859 Tot Loss: 0.0829 mAP: 0.0181
 Epoch:2 train steps: 1210 Loc Loss: 0.0798 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0180
 Epoch:2 train steps: 1220 Loc Loss: 0.0801 Cls Loss: 0.0858 Tot Loss: 0.0830 mAP: 0.0182
 Epoch:2 train steps: 1230 Loc Loss: 0.0797 Cls Loss: 0.0857 Tot Loss: 0.0827 mAP: 0.0181
 Epoch:2 train steps: 1240 Loc Loss: 0.0797 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0179
 Epoch:2 train steps: 1250 Loc Loss: 0.0798 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0183
 Epoch:2 train steps: 1260 Loc Loss: 0.0798 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0181
 Epoch:2 train steps: 1270 Loc Loss: 0.0803 Cls Loss: 0.0862 Tot Loss: 0.0832 mAP: 0.0181
 Epoch:2 train steps: 1280 Loc Loss: 0.0799 Cls Loss: 0.0859 Tot Loss: 0.0829 mAP: 0.0182
 Epoch:2 train steps: 1290 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0831 mAP: 0.0180
 Epoch:2 train steps: 1300 Loc Loss: 0.0800 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0181
 Epoch:2 train steps: 1310 Loc Loss: 0.0792 Cls Loss: 0.0851 Tot Loss: 0.0821 mAP: 0.0182
 Epoch:2 train steps: 1320 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0831 mAP: 0.0183
 Epoch:2 train steps: 1330 Loc Loss: 0.0805 Cls Loss: 0.0862 Tot Loss: 0.0833 mAP: 0.0182
train
 Epoch:3 train steps: 1340 Loc Loss: 0.0478 Cls Loss: 0.0513 Tot Loss: 0.0495 mAP: 0.0182
 Epoch:3 train steps: 1350 Loc Loss: 0.0799 Cls Loss: 0.0858 Tot Loss: 0.0828 mAP: 0.0181
 Epoch:3 train steps: 1360 Loc Loss: 0.0800 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0184
 Epoch:3 train steps: 1370 Loc Loss: 0.0808 Cls Loss: 0.0868 Tot Loss: 0.0838 mAP: 0.0184
 Epoch:3 train steps: 1380 Loc Loss: 0.0805 Cls Loss: 0.0864 Tot Loss: 0.0834 mAP: 0.0183
 Epoch:3 train steps: 1390 Loc Loss: 0.0800 Cls Loss: 0.0856 Tot Loss: 0.0828 mAP: 0.0183
 Epoch:3 train steps: 1400 Loc Loss: 0.0796 Cls Loss: 0.0854 Tot Loss: 0.0825 mAP: 0.0182
 Epoch:3 train steps: 1410 Loc Loss: 0.0801 Cls Loss: 0.0858 Tot Loss: 0.0830 mAP: 0.0183
 Epoch:3 train steps: 1420 Loc Loss: 0.0800 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0181
 Epoch:3 train steps: 1430 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0181
 Epoch:3 train steps: 1440 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0181
 Epoch:3 train steps: 1450 Loc Loss: 0.0797 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0181
 Epoch:3 train steps: 1460 Loc Loss: 0.0794 Cls Loss: 0.0853 Tot Loss: 0.0823 mAP: 0.0183
 Epoch:3 train steps: 1470 Loc Loss: 0.0800 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0182
 Epoch:3 train steps: 1480 Loc Loss: 0.0806 Cls Loss: 0.0864 Tot Loss: 0.0835 mAP: 0.0182
 Epoch:3 train steps: 1490 Loc Loss: 0.0798 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0184
 Epoch:3 train steps: 1500 Loc Loss: 0.0797 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0182
 Epoch:3 train steps: 1510 Loc Loss: 0.0793 Cls Loss: 0.0850 Tot Loss: 0.0822 mAP: 0.0182
 Epoch:3 train steps: 1520 Loc Loss: 0.0805 Cls Loss: 0.0863 Tot Loss: 0.0834 mAP: 0.0183
 Epoch:3 train steps: 1530 Loc Loss: 0.0804 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0185
 Epoch:3 train steps: 1540 Loc Loss: 0.0798 Cls Loss: 0.0857 Tot Loss: 0.0827 mAP: 0.0181
 Epoch:3 train steps: 1550 Loc Loss: 0.0801 Cls Loss: 0.0862 Tot Loss: 0.0832 mAP: 0.0185
 Epoch:3 train steps: 1560 Loc Loss: 0.0803 Cls Loss: 0.0865 Tot Loss: 0.0834 mAP: 0.0187
 Epoch:3 train steps: 1570 Loc Loss: 0.0803 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0181
 Epoch:3 train steps: 1580 Loc Loss: 0.0800 Cls Loss: 0.0856 Tot Loss: 0.0828 mAP: 0.0183
 Epoch:3 train steps: 1590 Loc Loss: 0.0792 Cls Loss: 0.0851 Tot Loss: 0.0822 mAP: 0.0181
 Epoch:3 train steps: 1600 Loc Loss: 0.0800 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0183
 Epoch:3 train steps: 1610 Loc Loss: 0.0799 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0183
 Epoch:3 train steps: 1620 Loc Loss: 0.0803 Cls Loss: 0.0860 Tot Loss: 0.0832 mAP: 0.0186
 Epoch:3 train steps: 1630 Loc Loss: 0.0798 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0185
 Epoch:3 train steps: 1640 Loc Loss: 0.0805 Cls Loss: 0.0865 Tot Loss: 0.0835 mAP: 0.0184
 Epoch:3 train steps: 1650 Loc Loss: 0.0797 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0182
 Epoch:3 train steps: 1660 Loc Loss: 0.0807 Cls Loss: 0.0865 Tot Loss: 0.0836 mAP: 0.0184
 Epoch:3 train steps: 1670 Loc Loss: 0.0800 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0185
 Epoch:3 train steps: 1680 Loc Loss: 0.0800 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0185
 Epoch:3 train steps: 1690 Loc Loss: 0.0801 Cls Loss: 0.0857 Tot Loss: 0.0829 mAP: 0.0185
 Epoch:3 train steps: 1700 Loc Loss: 0.0804 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0186
 Epoch:3 train steps: 1710 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0831 mAP: 0.0185
 Epoch:3 train steps: 1720 Loc Loss: 0.0804 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0183
 Epoch:3 train steps: 1730 Loc Loss: 0.0804 Cls Loss: 0.0864 Tot Loss: 0.0834 mAP: 0.0183
 Epoch:3 train steps: 1740 Loc Loss: 0.0793 Cls Loss: 0.0851 Tot Loss: 0.0822 mAP: 0.0182
 Epoch:3 train steps: 1750 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0187
 Epoch:3 train steps: 1760 Loc Loss: 0.0795 Cls Loss: 0.0853 Tot Loss: 0.0824 mAP: 0.0184
 Epoch:3 train steps: 1770 Loc Loss: 0.0800 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0181
 Epoch:3 train steps: 1780 Loc Loss: 0.0803 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0183
 Epoch:3 train steps: 1790 Loc Loss: 0.0793 Cls Loss: 0.0851 Tot Loss: 0.0822 mAP: 0.0187
 Epoch:3 train steps: 1800 Loc Loss: 0.0803 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0186
 Epoch:3 train steps: 1810 Loc Loss: 0.0795 Cls Loss: 0.0854 Tot Loss: 0.0825 mAP: 0.0186
 Epoch:3 train steps: 1820 Loc Loss: 0.0803 Cls Loss: 0.0862 Tot Loss: 0.0833 mAP: 0.0187
 Epoch:3 train steps: 1830 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0184
 Epoch:3 train steps: 1840 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0185
 Epoch:3 train steps: 1850 Loc Loss: 0.0796 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0183
 Epoch:3 train steps: 1860 Loc Loss: 0.0804 Cls Loss: 0.0865 Tot Loss: 0.0834 mAP: 0.0187
 Epoch:3 train steps: 1870 Loc Loss: 0.0798 Cls Loss: 0.0859 Tot Loss: 0.0828 mAP: 0.0183
 Epoch:3 train steps: 1880 Loc Loss: 0.0797 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0184
 Epoch:3 train steps: 1890 Loc Loss: 0.0794 Cls Loss: 0.0853 Tot Loss: 0.0824 mAP: 0.0181
 Epoch:3 train steps: 1900 Loc Loss: 0.0798 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0183
 Epoch:3 train steps: 1910 Loc Loss: 0.0794 Cls Loss: 0.0851 Tot Loss: 0.0822 mAP: 0.0182
 Epoch:3 train steps: 1920 Loc Loss: 0.0800 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0185
 Epoch:3 train steps: 1930 Loc Loss: 0.0803 Cls Loss: 0.0860 Tot Loss: 0.0832 mAP: 0.0183
 Epoch:3 train steps: 1940 Loc Loss: 0.0800 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0184
 Epoch:3 train steps: 1950 Loc Loss: 0.0798 Cls Loss: 0.0859 Tot Loss: 0.0828 mAP: 0.0182
 Epoch:3 train steps: 1960 Loc Loss: 0.0807 Cls Loss: 0.0865 Tot Loss: 0.0836 mAP: 0.0188
 Epoch:3 train steps: 1970 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0181
 Epoch:3 train steps: 1980 Loc Loss: 0.0799 Cls Loss: 0.0859 Tot Loss: 0.0829 mAP: 0.0185
 Epoch:3 train steps: 1990 Loc Loss: 0.0798 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0187
 Epoch:3 train steps: 2000 Loc Loss: 0.0795 Cls Loss: 0.0854 Tot Loss: 0.0825 mAP: 0.0183
train
 Epoch:4 train steps: 2010 Loc Loss: 0.0715 Cls Loss: 0.0768 Tot Loss: 0.0742 mAP: 0.0182
 Epoch:4 train steps: 2020 Loc Loss: 0.0798 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0188
 Epoch:4 train steps: 2030 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0185
 Epoch:4 train steps: 2040 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0181
 Epoch:4 train steps: 2050 Loc Loss: 0.0803 Cls Loss: 0.0862 Tot Loss: 0.0833 mAP: 0.0187
 Epoch:4 train steps: 2060 Loc Loss: 0.0794 Cls Loss: 0.0851 Tot Loss: 0.0823 mAP: 0.0183
 Epoch:4 train steps: 2070 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0185
 Epoch:4 train steps: 2080 Loc Loss: 0.0812 Cls Loss: 0.0870 Tot Loss: 0.0841 mAP: 0.0186
 Epoch:4 train steps: 2090 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0185
 Epoch:4 train steps: 2100 Loc Loss: 0.0799 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0183
 Epoch:4 train steps: 2110 Loc Loss: 0.0799 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0184
 Epoch:4 train steps: 2120 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0185
 Epoch:4 train steps: 2130 Loc Loss: 0.0799 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0185
 Epoch:4 train steps: 2140 Loc Loss: 0.0801 Cls Loss: 0.0858 Tot Loss: 0.0830 mAP: 0.0183
 Epoch:4 train steps: 2150 Loc Loss: 0.0799 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0186
 Epoch:4 train steps: 2160 Loc Loss: 0.0797 Cls Loss: 0.0856 Tot Loss: 0.0826 mAP: 0.0186
 Epoch:4 train steps: 2170 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0186
 Epoch:4 train steps: 2180 Loc Loss: 0.0799 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0185
 Epoch:4 train steps: 2190 Loc Loss: 0.0803 Cls Loss: 0.0864 Tot Loss: 0.0833 mAP: 0.0186
 Epoch:4 train steps: 2200 Loc Loss: 0.0797 Cls Loss: 0.0856 Tot Loss: 0.0826 mAP: 0.0185
 Epoch:4 train steps: 2210 Loc Loss: 0.0796 Cls Loss: 0.0853 Tot Loss: 0.0824 mAP: 0.0185
 Epoch:4 train steps: 2220 Loc Loss: 0.0805 Cls Loss: 0.0863 Tot Loss: 0.0834 mAP: 0.0188
 Epoch:4 train steps: 2230 Loc Loss: 0.0801 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0185
 Epoch:4 train steps: 2240 Loc Loss: 0.0798 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0184
 Epoch:4 train steps: 2250 Loc Loss: 0.0797 Cls Loss: 0.0853 Tot Loss: 0.0825 mAP: 0.0184
 Epoch:4 train steps: 2260 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0186
 Epoch:4 train steps: 2270 Loc Loss: 0.0808 Cls Loss: 0.0866 Tot Loss: 0.0837 mAP: 0.0186
 Epoch:4 train steps: 2280 Loc Loss: 0.0796 Cls Loss: 0.0853 Tot Loss: 0.0824 mAP: 0.0184
 Epoch:4 train steps: 2290 Loc Loss: 0.0801 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0187
 Epoch:4 train steps: 2300 Loc Loss: 0.0794 Cls Loss: 0.0852 Tot Loss: 0.0823 mAP: 0.0185
 Epoch:4 train steps: 2310 Loc Loss: 0.0804 Cls Loss: 0.0865 Tot Loss: 0.0834 mAP: 0.0186
 Epoch:4 train steps: 2320 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0189
 Epoch:4 train steps: 2330 Loc Loss: 0.0802 Cls Loss: 0.0864 Tot Loss: 0.0833 mAP: 0.0187
 Epoch:4 train steps: 2340 Loc Loss: 0.0796 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0186
 Epoch:4 train steps: 2350 Loc Loss: 0.0798 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0187
 Epoch:4 train steps: 2360 Loc Loss: 0.0799 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0185
 Epoch:4 train steps: 2370 Loc Loss: 0.0800 Cls Loss: 0.0856 Tot Loss: 0.0828 mAP: 0.0184
 Epoch:4 train steps: 2380 Loc Loss: 0.0798 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0186
 Epoch:4 train steps: 2390 Loc Loss: 0.0794 Cls Loss: 0.0853 Tot Loss: 0.0824 mAP: 0.0186
 Epoch:4 train steps: 2400 Loc Loss: 0.0798 Cls Loss: 0.0858 Tot Loss: 0.0828 mAP: 0.0185
 Epoch:4 train steps: 2410 Loc Loss: 0.0805 Cls Loss: 0.0862 Tot Loss: 0.0833 mAP: 0.0186
 Epoch:4 train steps: 2420 Loc Loss: 0.0801 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0187
 Epoch:4 train steps: 2430 Loc Loss: 0.0797 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0184
 Epoch:4 train steps: 2440 Loc Loss: 0.0800 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0187
 Epoch:4 train steps: 2450 Loc Loss: 0.0792 Cls Loss: 0.0850 Tot Loss: 0.0821 mAP: 0.0184
 Epoch:4 train steps: 2460 Loc Loss: 0.0807 Cls Loss: 0.0867 Tot Loss: 0.0837 mAP: 0.0190
 Epoch:4 train steps: 2470 Loc Loss: 0.0790 Cls Loss: 0.0850 Tot Loss: 0.0820 mAP: 0.0187
 Epoch:4 train steps: 2480 Loc Loss: 0.0791 Cls Loss: 0.0851 Tot Loss: 0.0821 mAP: 0.0187
 Epoch:4 train steps: 2490 Loc Loss: 0.0804 Cls Loss: 0.0864 Tot Loss: 0.0834 mAP: 0.0184
 Epoch:4 train steps: 2500 Loc Loss: 0.0796 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0186
 Epoch:4 train steps: 2510 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0186
 Epoch:4 train steps: 2520 Loc Loss: 0.0799 Cls Loss: 0.0859 Tot Loss: 0.0829 mAP: 0.0185
 Epoch:4 train steps: 2530 Loc Loss: 0.0799 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0186
 Epoch:4 train steps: 2540 Loc Loss: 0.0799 Cls Loss: 0.0856 Tot Loss: 0.0828 mAP: 0.0186
 Epoch:4 train steps: 2550 Loc Loss: 0.0798 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0188
 Epoch:4 train steps: 2560 Loc Loss: 0.0792 Cls Loss: 0.0850 Tot Loss: 0.0821 mAP: 0.0188
 Epoch:4 train steps: 2570 Loc Loss: 0.0800 Cls Loss: 0.0861 Tot Loss: 0.0830 mAP: 0.0189
 Epoch:4 train steps: 2580 Loc Loss: 0.0804 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0188
 Epoch:4 train steps: 2590 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0188
 Epoch:4 train steps: 2600 Loc Loss: 0.0802 Cls Loss: 0.0858 Tot Loss: 0.0830 mAP: 0.0186
 Epoch:4 train steps: 2610 Loc Loss: 0.0800 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0184
 Epoch:4 train steps: 2620 Loc Loss: 0.0799 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0188
 Epoch:4 train steps: 2630 Loc Loss: 0.0802 Cls Loss: 0.0859 Tot Loss: 0.0831 mAP: 0.0189
 Epoch:4 train steps: 2640 Loc Loss: 0.0797 Cls Loss: 0.0854 Tot Loss: 0.0826 mAP: 0.0186
 Epoch:4 train steps: 2650 Loc Loss: 0.0795 Cls Loss: 0.0853 Tot Loss: 0.0824 mAP: 0.0185
 Epoch:4 train steps: 2660 Loc Loss: 0.0800 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0187
train
 Epoch:5 train steps: 2670 Loc Loss: 0.0160 Cls Loss: 0.0172 Tot Loss: 0.0166 mAP: 0.0186
 Epoch:5 train steps: 2680 Loc Loss: 0.0797 Cls Loss: 0.0856 Tot Loss: 0.0826 mAP: 0.0186
 Epoch:5 train steps: 2690 Loc Loss: 0.0797 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0186
 Epoch:5 train steps: 2700 Loc Loss: 0.0796 Cls Loss: 0.0854 Tot Loss: 0.0825 mAP: 0.0188
 Epoch:5 train steps: 2710 Loc Loss: 0.0804 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0190
 Epoch:5 train steps: 2720 Loc Loss: 0.0800 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0186
 Epoch:5 train steps: 2730 Loc Loss: 0.0795 Cls Loss: 0.0852 Tot Loss: 0.0823 mAP: 0.0185
 Epoch:5 train steps: 2740 Loc Loss: 0.0809 Cls Loss: 0.0865 Tot Loss: 0.0837 mAP: 0.0189
 Epoch:5 train steps: 2750 Loc Loss: 0.0799 Cls Loss: 0.0858 Tot Loss: 0.0828 mAP: 0.0183
 Epoch:5 train steps: 2760 Loc Loss: 0.0803 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0186
 Epoch:5 train steps: 2770 Loc Loss: 0.0792 Cls Loss: 0.0850 Tot Loss: 0.0821 mAP: 0.0187
 Epoch:5 train steps: 2780 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0832 mAP: 0.0190
 Epoch:5 train steps: 2790 Loc Loss: 0.0790 Cls Loss: 0.0848 Tot Loss: 0.0819 mAP: 0.0184
 Epoch:5 train steps: 2800 Loc Loss: 0.0797 Cls Loss: 0.0857 Tot Loss: 0.0827 mAP: 0.0188
 Epoch:5 train steps: 2810 Loc Loss: 0.0797 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0184
 Epoch:5 train steps: 2820 Loc Loss: 0.0795 Cls Loss: 0.0851 Tot Loss: 0.0823 mAP: 0.0190
 Epoch:5 train steps: 2830 Loc Loss: 0.0796 Cls Loss: 0.0854 Tot Loss: 0.0825 mAP: 0.0187
 Epoch:5 train steps: 2840 Loc Loss: 0.0799 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0186
 Epoch:5 train steps: 2850 Loc Loss: 0.0802 Cls Loss: 0.0862 Tot Loss: 0.0832 mAP: 0.0186
 Epoch:5 train steps: 2860 Loc Loss: 0.0803 Cls Loss: 0.0864 Tot Loss: 0.0833 mAP: 0.0187
 Epoch:5 train steps: 2870 Loc Loss: 0.0799 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0188
 Epoch:5 train steps: 2880 Loc Loss: 0.0798 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0187
 Epoch:5 train steps: 2890 Loc Loss: 0.0800 Cls Loss: 0.0857 Tot Loss: 0.0829 mAP: 0.0189
 Epoch:5 train steps: 2900 Loc Loss: 0.0803 Cls Loss: 0.0864 Tot Loss: 0.0834 mAP: 0.0187
 Epoch:5 train steps: 2910 Loc Loss: 0.0797 Cls Loss: 0.0853 Tot Loss: 0.0825 mAP: 0.0187
 Epoch:5 train steps: 2920 Loc Loss: 0.0788 Cls Loss: 0.0845 Tot Loss: 0.0817 mAP: 0.0186
 Epoch:5 train steps: 2930 Loc Loss: 0.0800 Cls Loss: 0.0857 Tot Loss: 0.0829 mAP: 0.0186
 Epoch:5 train steps: 2940 Loc Loss: 0.0798 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0186
 Epoch:5 train steps: 2950 Loc Loss: 0.0807 Cls Loss: 0.0865 Tot Loss: 0.0836 mAP: 0.0192
 Epoch:5 train steps: 2960 Loc Loss: 0.0798 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0184
 Epoch:5 train steps: 2970 Loc Loss: 0.0805 Cls Loss: 0.0864 Tot Loss: 0.0834 mAP: 0.0185
 Epoch:5 train steps: 2980 Loc Loss: 0.0805 Cls Loss: 0.0862 Tot Loss: 0.0834 mAP: 0.0189
 Epoch:5 train steps: 2990 Loc Loss: 0.0800 Cls Loss: 0.0858 Tot Loss: 0.0829 mAP: 0.0186
 Epoch:5 train steps: 3000 Loc Loss: 0.0799 Cls Loss: 0.0856 Tot Loss: 0.0828 mAP: 0.0187
 Epoch:5 train steps: 3010 Loc Loss: 0.0798 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0190
 Epoch:5 train steps: 3020 Loc Loss: 0.0798 Cls Loss: 0.0857 Tot Loss: 0.0828 mAP: 0.0189
 Epoch:5 train steps: 3030 Loc Loss: 0.0797 Cls Loss: 0.0856 Tot Loss: 0.0826 mAP: 0.0186
 Epoch:5 train steps: 3040 Loc Loss: 0.0794 Cls Loss: 0.0856 Tot Loss: 0.0825 mAP: 0.0186
 Epoch:5 train steps: 3050 Loc Loss: 0.0804 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0190
 Epoch:5 train steps: 3060 Loc Loss: 0.0801 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0190
 Epoch:5 train steps: 3070 Loc Loss: 0.0800 Cls Loss: 0.0859 Tot Loss: 0.0829 mAP: 0.0187
 Epoch:5 train steps: 3080 Loc Loss: 0.0801 Cls Loss: 0.0859 Tot Loss: 0.0830 mAP: 0.0189
 Epoch:5 train steps: 3090 Loc Loss: 0.0792 Cls Loss: 0.0851 Tot Loss: 0.0822 mAP: 0.0188
 Epoch:5 train steps: 3100 Loc Loss: 0.0793 Cls Loss: 0.0850 Tot Loss: 0.0822 mAP: 0.0188
 Epoch:5 train steps: 3110 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0189
 Epoch:5 train steps: 3120 Loc Loss: 0.0798 Cls Loss: 0.0856 Tot Loss: 0.0827 mAP: 0.0186
 Epoch:5 train steps: 3130 Loc Loss: 0.0794 Cls Loss: 0.0852 Tot Loss: 0.0823 mAP: 0.0187
 Epoch:5 train steps: 3140 Loc Loss: 0.0808 Cls Loss: 0.0868 Tot Loss: 0.0838 mAP: 0.0188
 Epoch:5 train steps: 3150 Loc Loss: 0.0801 Cls Loss: 0.0860 Tot Loss: 0.0830 mAP: 0.0188
 Epoch:5 train steps: 3160 Loc Loss: 0.0793 Cls Loss: 0.0852 Tot Loss: 0.0822 mAP: 0.0187
 Epoch:5 train steps: 3170 Loc Loss: 0.0803 Cls Loss: 0.0863 Tot Loss: 0.0833 mAP: 0.0189
 Epoch:5 train steps: 3180 Loc Loss: 0.0796 Cls Loss: 0.0854 Tot Loss: 0.0825 mAP: 0.0189
 Epoch:5 train steps: 3190 Loc Loss: 0.0799 Cls Loss: 0.0855 Tot Loss: 0.0827 mAP: 0.0188
 Epoch:5 train steps: 3200 Loc Loss: 0.0797 Cls Loss: 0.0854 Tot Loss: 0.0826 mAP: 0.0188
 Epoch:5 train steps: 3210 Loc Loss: 0.0808 Cls Loss: 0.0869 Tot Loss: 0.0838 mAP: 0.0189
 Epoch:5 train steps: 3220 Loc Loss: 0.0795 Cls Loss: 0.0854 Tot Loss: 0.0824 mAP: 0.0188
 Epoch:5 train steps: 3230 Loc Loss: 0.0802 Cls Loss: 0.0860 Tot Loss: 0.0831 mAP: 0.0191
 Epoch:5 train steps: 3240 Loc Loss: 0.0796 Cls Loss: 0.0854 Tot Loss: 0.0825 mAP: 0.0188
 Epoch:5 train steps: 3250 Loc Loss: 0.0794 Cls Loss: 0.0854 Tot Loss: 0.0824 mAP: 0.0189
 Epoch:5 train steps: 3260 Loc Loss: 0.0802 Cls Loss: 0.0861 Tot Loss: 0.0831 mAP: 0.0191
 Epoch:5 train steps: 3270 Loc Loss: 0.0788 Cls Loss: 0.0846 Tot Loss: 0.0817 mAP: 0.0192
 Epoch:5 train steps: 3280 Loc Loss: 0.0797 Cls Loss: 0.0855 Tot Loss: 0.0826 mAP: 0.0190
 Epoch:5 train steps: 3290 Loc Loss: 0.0799 Cls Loss: 0.0855 Tot Loss: 0.0827 mAP: 0.0188
 Epoch:5 train steps: 3300 Loc Loss: 0.0795 Cls Loss: 0.0854 Tot Loss: 0.0825 mAP: 0.0190
 Epoch:5 train steps: 3310 Loc Loss: 0.0793 Cls Loss: 0.0852 Tot Loss: 0.0823 mAP: 0.0187
 Epoch:5 train steps: 3320 Loc Loss: 0.0800 Cls Loss: 0.0859 Tot Loss: 0.0829 mAP: 0.0187
 Epoch:5 train steps: 3330 Loc Loss: 0.0794 Cls Loss: 0.0850 Tot Loss: 0.0822 mAP: 0.0190
train
